# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_dataset_creation.ipynb.

# %% auto 0
__all__ = ['get_transforms', 'SegmentationDataset', 'create_pytorch_dataloader', 'visualize_batch']

# %% ../nbs/09_dataset_creation.ipynb 4
import torch
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
from typing import List, Callable, Tuple, Dict, Union
from pathlib import Path
from fastcore.all import *
import cv2

import matplotlib.pyplot as plt
import numpy as np

# %% ../nbs/09_dataset_creation.ipynb 5
def get_transforms(*, data):
    if data == 'train':
        return A.Compose([
            #A.Resize(256, 256),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.RandomRotate90(p=0.5),
            A.Transpose(p=0.5),
            A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),
            #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            #A.Normalize(mean=[0, 0, 0], std=[1/255, 1/255, 1/255]),
            A.Normalize(mean=[0], std=[1/255]),
            ToTensorV2(),
        ])

    elif data == 'valid':
        return A.Compose([
            #A.Resize(256, 256),
            A.Normalize(mean=[0], std=[1/255]),
            #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])

# %% ../nbs/09_dataset_creation.ipynb 6
class SegmentationDataset(Dataset):
    def __init__(
                self, 
                image_path:Union[Path, str], 
                mask_path:Union[Path, str], 
                exts:str,
                transform=None
                ):
        self.image_path = image_path
        self.mask_path = mask_path
        # getting images and masks
        self.images = Path(self.image_path).ls(file_exts=exts)
        self.masks = Path(self.mask_path).ls(file_exts=exts)

        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = np.array(Image.open(f"{self.images[idx]}").convert("L")).astype(dtype=np.float32)
        mask = np.array(Image.open(f"{self.masks[idx]}").convert("L"))
        mask = (mask> 0).astype(np.float32)

        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']

        #image = image[None, ...]
        mask = mask[None, ...]

        return image, mask

# %% ../nbs/09_dataset_creation.ipynb 17
def create_pytorch_dataloader(
    split_type:str, # in case of 'random' randomly data will be splitted
    split_per:float, # percentage of training data
    batch_size:int,
    image_path:Union[Path, str],
    mask_path:Union[Path, str],
    transforms:Callable, # a callable function named as [get_transform] with argument data='train' or 'valid'
    exts:str='.png',  # image  and mask data extensions
    num_workers:str=4
   ):

    'Create pytorch dataloader based on the argument'

    dataset = SegmentationDataset(
                                image_path=image_path,
                                mask_path=mask_path,
                                exts=exts
                              )

    print(f' Number of images found = {len(dataset)}')
    if split_type == 'random':
        train_len = int(split_per * len(dataset))
        val_len = len(dataset) - train_len

        train_ds, val_ds = random_split(dataset, lengths=[train_len, val_len])
        print(f' training dataset length = {len(train_ds)} and validation dataset length=  {len(val_ds)}')
        

        if transforms is not None:
            train_ds.dataset.transform = transforms(data='train')
            val_ds.dataset.transform = transforms(data='valid')
        else:
            train_ds.dataset.transform = get_transforms(data='train')
            val_ds.dataset.transform = get_transforms(data='valid')
            print(train_ds[0][0].shape, val_ds[0][0].shape)

        train_dl = DataLoader(
                              train_ds, 
                              batch_size=batch_size,
                              shuffle=True, 
                              num_workers=num_workers, 
                              pin_memory=True
        )
        val_dl = DataLoader(
                              val_ds, 
                              batch_size=batch_size,
                              shuffle=True, 
                              num_workers=num_workers, 
                              pin_memory=True
        )

    else:
        raise NotImplementedError("Only random split is implemented")
    
    return train_dl, val_dl




# %% ../nbs/09_dataset_creation.ipynb 22
def visualize_batch(images, masks, num_images=4):
    fig, axs = plt.subplots(1,num_images, figsize=(5, num_images*5))
    for idx, (image, mask) in enumerate(zip(images, masks)):
        if idx >= num_images:
            break
        axs[idx].imshow(image.squeeze(0).squeeze(0), cmap='gray')
        axs[idx].imshow(mask.squeeze(), cmap='jet', alpha=0.3)  # overlay mask on image
        axs[idx].axis('off')
        axs[idx].set_title('Image with Mask')
    plt.tight_layout()
    plt.show()
