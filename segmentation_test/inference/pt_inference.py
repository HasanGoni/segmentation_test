# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/17_inference.pt_inference.ipynb.

# %% auto 0
__all__ = ['load_model']

# %% ../../nbs/17_inference.pt_inference.ipynb 3
from cv_tools.core import *
from cv_tools.imports import *


# %% ../../nbs/17_inference.pt_inference.ipynb 4
import torch

# %% ../../nbs/17_inference.pt_inference.ipynb 5
from typing import Optional, Type


# %% ../../nbs/17_inference.pt_inference.ipynb 6
from ..pytorch_model_development import UNet


# %% ../../nbs/17_inference.pt_inference.ipynb 7
def load_model(
        model_path: str,
        model_name: str,
        device: str = 'cuda',
        **model_params
):
    """
    Load a PyTorch model from a file.

    Args:
        model_path (str): Path to the saved model file
        model_name (str): Name of the model class to instantiate (e.g. 'UNet')
        device (str): Device to load the model on ('cuda' or 'cpu')
        **model_params: Parameters to initialize the model class
            For UNet example:
            - in_channels (int): Number of input channels
            - out_channels (int): Number of output classes
            - max_pool_only (bool): Whether to use only max pooling
            - near_size (int): Size of input image
            - features (List[int]): List of filter sizes for each level

    Returns:
        torch.nn.Module: Loaded PyTorch model
    
    Example:
        >>> loaded_model = load_model(
        ...     model_path='path/to/model.pth',
        ...     model_name='UNet',
        ...     in_channels=1,
        ...     out_channels=1, 
        ...     near_size=256,
        ...     filters=[64, 128, 256],
        ...     device='cuda'
        ... )
    """
    # Get the model class dynamically from the module
    model_class = globals()[model_name]
    
    # Initialize model with provided parameters directly (not nested in model_params)
    model = model_class(**model_params)
    
    # Load weights
    checkpoint = torch.load(model_path, map_location=device)
    
    # Handle both state dict and full model saves
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
        
    model.to(device)
    model.eval()  # Set the model to evaluation mode
    return model
