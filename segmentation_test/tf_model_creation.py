# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07_tf_model_creation.ipynb.

# %% auto 0
__all__ = ['pooling', 'conv_block', 'double_conv', 'encoder_block', 'decoder_block']

# %% ../nbs/07_tf_model_creation.ipynb 3
import tensorflow as tf
import numpy as np
import pandas as pd
from fastcore.all import *
from fastcore.test import *
from pathlib import Path
from typing import List, Tuple, Dict, Union, Callable, Optional
import os

# %% ../nbs/07_tf_model_creation.ipynb 5
def pooling(
        inputs,
        max_pool_only=False,
        both=True,
        pool_size=2):
    if both:
        p1 = tf.keras.layers.MaxPooling2D(
            (pool_size, pool_size))(inputs)
        p2 = tf.keras.layers.AvgPool2D(
            pool_size=(pool_size, pool_size))(inputs)
        return tf.keras.layers.concatenate([p1, p2])
    elif max_pool_only:
        return tf.keras.layers.MaxPooling2D(
            (pool_size, pool_size))(inputs)
    else:
        return tf.keras.layers.AvgPool2D(
            pool_size=(pool_size, pool_size))(inputs)
            


# %% ../nbs/07_tf_model_creation.ipynb 6
def conv_block(
        inputs, 
        filter_no, 
        kernel_size, 
        batch_nm=True, 
        dropout=True, 
        drp_rt=0.1,
        kernel_initializer='glorot_normal' #option he_normal
        ):
    'Create a conv block with batch norma and dropout'
    c1 = tf.keras.layers.Conv2D(
        filter_no,
        (kernel_size, kernel_size),

        kernel_initializer='glorot_normal',
        padding='same',
        activation=None)(inputs)
    if batch_nm:
        c1 = tf.keras.layers.BatchNormalization()(c1)
    c1 = tf.keras.layers.Activation('relu')(c1)
    if dropout:
        c1 = tf.keras.layers.Dropout(drp_rt)(c1)
    return c1

# %% ../nbs/07_tf_model_creation.ipynb 8
def double_conv(
        inputs, 
        filter_no, 
        kernel_size, 
        batch_nm=True, 
        dropout=True, 
        drp_rt=0.1,
        kernel_initializer='glorot_normal' #option he_normal
        ):
    'Create double conv block with batch norma and dropout'

    c1 = tf.keras.layers.Conv2D(
        filter_no,
        (kernel_size, kernel_size),

        kernel_initializer='glorot_normal',
        padding='same',
        activation=None)(inputs)
    if batch_nm:
        c1 = tf.keras.layers.BatchNormalization()(c1)
    c1 = tf.keras.layers.Activation('relu')(c1)
    if dropout:
        c1 = tf.keras.layers.Dropout(drp_rt)(c1)
    c2 = tf.keras.layers.Conv2D(
        filter_no,
        (kernel_size, kernel_size),

        kernel_initializer='glorot_normal',
        padding='same',
        activation=None)(c1)
    if batch_nm:
        c2 = tf.keras.layers.BatchNormalization()(c2)
    c2 = tf.keras.layers.Activation('relu')(c2)
    if dropout:
        c2 = tf.keras.layers.Dropout(drp_rt)(c2)
    return c2

# %% ../nbs/07_tf_model_creation.ipynb 11
def encoder_block(
        input_size:tf.Tensor,
        filter_size:List[int]=[32,64,128,256,512, 1024],
        kernel_initializer:str='glorot_normal' #option he_normal
                 ):
    'Create Encoder block for Unet'

    inputs = tf.keras.layers.Input(input_size)
    outputs = []
    p = inputs 
    

    for i in filter_size:
        c = double_conv(
            inputs=p, 
            filter_no=i, 
            kernel_size=3, 
            batch_nm=True, 
            dropout=True, 
            drp_rt=0.1,
            kernel_initializer='glorot_normal' #option he_normal
            )
        p = pooling(c, both=True)
        outputs.append(p)
    return inputs, outputs

# %% ../nbs/07_tf_model_creation.ipynb 19
def decoder_block(
    filter_list:List[int],
    encoder_outputs:List[tf.Tensor],
   ):
    'Create a decoder block for unet'

    filter_list = filter_list[::-1]
    decoder_input = encoder_outputs[-1]

    for i in range(1, len(encoder_outputs)):
        filter_no = filter_list[i-1]
        up_ = tf.keras.layers.Conv2DTranspose(
                                            filter_no,
                                            (2,2),
                                            strides=(2,2),
                                            padding='same',
                                            )(decoder_input)
        
        # Handle the shape mismatch (if any)
        if up_.shape[1:3] != encoder_outputs[-(i + 1)].shape[1:3]:
            # Adjust padding, cropping, or use other techniques as needed
            print(f'up_.shape: {up_.shape}')
            pass

        # Concatenate the upsampled feature map with the corresponding encoder feature map
        up_ = tf.keras.layers.concatenate([up_, encoder_outputs[-(i + 1)]])

        c_= double_conv(
            inputs=up_, 
            filter_no=filter_no, 
            kernel_size=3, 
            batch_nm=True, 
            dropout=True, 
            drp_rt=0.1,
            kernel_initializer='glorot_normal' #option he_normal
            )
        decoder_input = c_
    return decoder_input


    


