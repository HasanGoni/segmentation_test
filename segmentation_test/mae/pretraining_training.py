# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/14_mae.pretraining_training.ipynb.

# %% auto 0
__all__ = ['NativeScaler', 'get_grad_norm_', 'NativeScalerWithGradNormCount', 'adjust_learning_rate', 'test_adjust_learning_rate',
           'train_one_epoch', 'parse_args_', 'main']

# %% ../../nbs/14_mae.pretraining_training.ipynb 4
import argparse
import datetime
import json
import numpy as np
from typing import Iterable
from types import SimpleNamespace
import os
import time
from pathlib import Path

import torch
from torch import inf
import torch.backends.cudnn as cudnn
from torch.utils.tensorboard import SummaryWriter
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from argparse import ArgumentParser

import timm

# %% ../../nbs/14_mae.pretraining_training.ipynb 8
import segmentation_test.mae.misc as misc
import segmentation_test.mae.model_development as models_mae

# %% ../../nbs/14_mae.pretraining_training.ipynb 9
import timm.optim.optim_factory as optim_factory
import math
from fastcore.test import *

# %% ../../nbs/14_mae.pretraining_training.ipynb 12
def get_grad_norm_(
        parameters, 
        norm_type: float = 2.0
        ) -> torch.Tensor:
    """
    Calculate the gradient norm of the given parameters.
    """
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = [p for p in parameters if p.grad is not None]
    norm_type = float(norm_type)
    if len(parameters) == 0:
        return torch.tensor(0.)
    device = parameters[0].grad.device
    if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)
    else:
        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
    return total_norm

# %% ../../nbs/14_mae.pretraining_training.ipynb 14
class NativeScalerWithGradNormCount:
    state_dict_key = "amp_scaler"

    def __init__(self):
        self._scaler = torch.cuda.amp.GradScaler()

    def __call__(
		self, 
		loss, 
		optimizer, 
		clip_grad=None, 
		parameters=None, 
		create_graph=False, 
		update_grad=True):
        self._scaler.scale(loss).backward(create_graph=create_graph)
        if update_grad:
            if clip_grad is not None:
                assert parameters is not None
                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)
            else:
                self._scaler.unscale_(optimizer)
                norm = get_grad_norm_(parameters)
            self._scaler.step(optimizer)
            self._scaler.update()
        else:
            norm = None
        return norm

    def state_dict(self):
        return self._scaler.state_dict()

    def load_state_dict(self, state_dict):
        self._scaler.load_state_dict(state_dict)

# %% ../../nbs/14_mae.pretraining_training.ipynb 15
def adjust_learning_rate(
	optimizer, 
	epoch, 
	args
	):
    """Decay the learning rate with half-cycle cosine after warmup"""
    if epoch < args.warmup_epochs:
        lr = args.lr * epoch / args.warmup_epochs 
    else:
        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \
            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))
    for param_group in optimizer.param_groups:
        if "lr_scale" in param_group:
            param_group["lr"] = lr * param_group["lr_scale"]
        else:
            param_group["lr"] = lr
    return lr


# %% ../../nbs/14_mae.pretraining_training.ipynb 16
def test_adjust_learning_rate():
    class DummyOptimizer:
        def __init__(self):
            self.param_groups = [
				{"lr": 0.1}, 
				{"lr": 0.2, "lr_scale": 2}
			]

    class DummyArgs:
        def __init__(self):
            self.warmup_epochs = 5
            self.epochs = 100
            self.lr = 0.1
            self.min_lr = 0.001

    optimizer = DummyOptimizer()
    args = DummyArgs()

    # Test during warmup
    lr = adjust_learning_rate(optimizer, 2, args)
    test_eq(lr, 0.04)
    test_eq(optimizer.param_groups[0]["lr"], 0.04)
    test_eq(optimizer.param_groups[1]["lr"], 0.08)

    # Test after warmup
    lr = adjust_learning_rate(optimizer, 50, args)
    expected_lr = 0.001 + (0.1 - 0.001) * 0.5 * (1 + math.cos(math.pi * 45 / 95))
    test_close(lr, expected_lr, eps=1e-6)
    test_close(optimizer.param_groups[0]["lr"], expected_lr, eps=1e-6)
    test_close(optimizer.param_groups[1]["lr"], expected_lr * 2, eps=1e-6)
    print("All tests passed!")


# %% ../../nbs/14_mae.pretraining_training.ipynb 18
def train_one_epoch(
	            model: torch.nn.Module,
                data_loader: Iterable, 
				optimizer: torch.optim.Optimizer,
                device: torch.device, 
				epoch: int, 
				loss_scaler: NativeScalerWithGradNormCount,
                log_writer=None,
                args=None):
    model.train(True)
    metric_logger = misc.MetricLogger(delimiter="  ")
    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)
    print_freq = 20

    accum_iter = args.accum_iter

    optimizer.zero_grad()

    if log_writer is not None:
        print('log_dir: {}'.format(log_writer.log_dir))

    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):

        # we use a per iteration (instead of per epoch) lr scheduler
        if data_iter_step % accum_iter == 0:
            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)

        samples = samples.to(device, non_blocking=True)

        with torch.cuda.amp.autocast():
            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)

        loss_value = loss.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            sys.exit(1)

        loss /= accum_iter
        loss_scaler(
			loss, 
			optimizer, 
			parameters=model.parameters(),
            update_grad=(data_iter_step + 1) % accum_iter == 0
		)
        if (data_iter_step + 1) % accum_iter == 0:
        	optimizer.zero_grad()
        torch.cuda.synchronize()

        metric_logger.update(loss=loss_value)

        lr = optimizer.param_groups[0]["lr"]
        metric_logger.update(lr=lr)

        loss_value_reduce = misc.all_reduce_mean(loss_value)
        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:
            """ We use epoch_1000x as the x-axis in tensorboard.
			This calibrates different curves when batch size changes.
			"""
            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)
            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)
            log_writer.add_scalar('lr', lr, epoch_1000x)


	# gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}

# %% ../../nbs/14_mae.pretraining_training.ipynb 23
def parse_args_():
    parser = ArgumentParser(description="MAE Pre-training Arguments")
    
    # MAE pre-training arguments
    parser.add_argument("--batch_size", type=int, default=2, help="Batch size per GPU (effective batch size is BATCH_SIZE * ACCUM_ITER * # gpus)")
    parser.add_argument("--epochs", type=int, default=400, help="Number of epochs for training")
    parser.add_argument("--accum_iter", type=int, default=1, help="Accumulate gradient iterations (for increasing the effective batch size under memory constraints)")
    
    # Model parameters
    parser.add_argument("--model", type=str, default='mae_vit_base_patch16', help="Name of model to train")
    parser.add_argument("--input_size", type=int, default=224, help="Images input size")
    parser.add_argument("--mask_ratio", type=float, default=0.75, help="Masking ratio (percentage of removed patches)")
    parser.add_argument("--norm_pix_loss", action='store_true', help="Use (per-patch) normalized pixels as targets for computing loss")
    
    # Optimizer parameters
    parser.add_argument("--weight_decay", type=float, default=0.05, help="Weight decay for optimizer")
    parser.add_argument("--lr", type=float, default=None, help="Learning rate (absolute lr)")
    parser.add_argument("--blr", type=float, default=1e-3, help="Base learning rate: absolute_lr = base_lr * total_batch_size / 256")
    parser.add_argument("--min_lr", type=float, default=0., help="Lower lr bound for cyclic schedulers that hit 0")
    parser.add_argument("--warmup_epochs", type=int, default=40, help="Epochs to warmup LR")
    
    # Dataset parameters
    parser.add_argument("--data_path", type=str, default=" /home/hasan/.fastai/data/imagewang-160", help="Dataset path")
    parser.add_argument("--output_dir", type=str, default='./output_dir', help="Path where to save, empty for no saving")
    parser.add_argument("--log_dir", type=str, default='./output_dir', help="Path where to tensorboard log")
    parser.add_argument("--device", type=str, default='cuda', help="Device to use for training / testing")
    parser.add_argument("--seed", type=int, default=0, help="Seed for reproducibility")
    parser.add_argument("--resume", type=str, default='', help="Resume from checkpoint")
    parser.add_argument("--start_epoch", type=int, default=0, help="Starting epoch")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers for data loading")
    parser.add_argument("--pin_mem", action='store_true', help="Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.")
    
    # Distributed training parameters
    parser.add_argument("--world_size", type=int, default=1, help="Number of distributed processes")
    parser.add_argument("--local_rank", type=int, default=-1, help="Local rank for distributed training")
    parser.add_argument("--dist_on_itp", action='store_true', help="Distributed training on ITP")
    parser.add_argument("--dist_url", type=str, default='env://', help="URL used to set up distributed training")
    
    return parser.parse_args()


# %% ../../nbs/14_mae.pretraining_training.ipynb 51
NativeScaler = NativeScalerWithGradNormCount

# %% ../../nbs/14_mae.pretraining_training.ipynb 55
def main():
    args = parse_args_()
    misc.init_distributed_mode(args)
    print('job dir: {}'.format(os.getcwd()))
    print("{}".format(args).replace(', ', ',\n'))

    device = torch.device(args.device)

    # fix the seed for reproducibility
    seed = args.seed + misc.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)

    cudnn.benchmark = True

    # simple augmentation
    transform_train = transforms.Compose([
            transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
    dataset_train = datasets.ImageFolder(os.path.join(args.data_path, 'train'), transform=transform_train)
    print(dataset_train)

    if True:  # args.distributed:
        num_tasks = misc.get_world_size()
        global_rank = misc.get_rank()
        sampler_train = torch.utils.data.DistributedSampler(
            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True
        )
        print("Sampler_train = %s" % str(sampler_train))
    else:
        sampler_train = torch.utils.data.RandomSampler(dataset_train)

    if global_rank == 0 and args.log_dir is not None:
        os.makedirs(args.log_dir, exist_ok=True)
        log_writer = SummaryWriter(log_dir=args.log_dir)
    else:
        log_writer = None

    data_loader_train = torch.utils.data.DataLoader(
        dataset_train, sampler=sampler_train,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=args.pin_mem,
        drop_last=True,
    )
    # define the model
    model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)

    model.to(device)

    model_without_ddp = model
    print("Model = %s" % str(model_without_ddp))

    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()
    
    if args.lr is None:  # only base_lr is specified
        args.lr = args.blr * eff_batch_size / 256

    print("base lr: %.2e" % (args.lr * 256 / eff_batch_size))
    print("actual lr: %.2e" % args.lr)

    print("accumulate grad iterations: %d" % args.accum_iter)
    print("effective batch size: %d" % eff_batch_size)

    if args.distributed:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)
        model_without_ddp = model.module
    
    # following timm: set wd as 0 for bias and norm layers
    param_groups = optim_factory.param_groups_weight_decay(
        model_without_ddp, 
        args.weight_decay)
    optimizer = torch.optim.AdamW(
        param_groups, 
        lr=args.lr, 
        betas=(0.9, 0.95))
    print(optimizer)
    loss_scaler = NativeScaler()

    misc.load_model(
        args=args, 
        model_without_ddp=model_without_ddp, 
        optimizer=optimizer, 
        loss_scaler=loss_scaler)

    print(f"Start training for {args.epochs} epochs")
    start_time = time.time()
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            data_loader_train.sampler.set_epoch(epoch)
        train_stats = train_one_epoch(
            model, data_loader_train,
            optimizer, device, epoch, loss_scaler,
            log_writer=log_writer, args=args
        )
        if args.output_dir and (epoch % 20 == 0 or epoch + 1 == args.epochs):
            misc.save_model(
                args=args, 
                model=model, 
                model_without_ddp=model_without_ddp, 
                optimizer=optimizer,
                loss_scaler=loss_scaler, 
                epoch=epoch)

        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
                        'epoch': epoch,}

        # This section of code is responsible for logging the training statistics to a file.
        # It checks if an output directory is specified and if the current process is the main process.
        # If both conditions are true, it flushes the log writer (if it exists) to ensure all logs are written.
        # Then, it appends the current training statistics (log_stats) to a file named "log.txt" in the output directory.
        # The statistics are written in JSON format followed by a newline character.
        if args.output_dir and misc.is_main_process():
            if log_writer is not None:
                log_writer.flush()
            with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))

# %% ../../nbs/14_mae.pretraining_training.ipynb 56
if __name__ == '__main__':
	main()
