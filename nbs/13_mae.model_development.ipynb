{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mae.model_development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d961ae",
   "metadata": {},
   "source": [
    "# Masked Autoencoder Model\n",
    "> MAE model development from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ec45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cv_tools.imports import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03127abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d4450",
   "metadata": {},
   "source": [
    "# Create positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9dece7",
   "metadata": {},
   "source": [
    "- 2D sine-cosine position embedding\n",
    "- References:\n",
    "\t- [Transformer](https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py)\n",
    "\t- [MoCo v3](https://github.com/facebookresearch/moco-v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ff0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7712f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_1d_sincos_pos_embed_from_grid(\n",
    "        embed_dim, \n",
    "        pos\n",
    "        ):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d778bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_2d_sincos_pos_embed_from_grid(\n",
    "        embed_dim, \n",
    "        grid\n",
    "        ):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_2d_sincos_pos_embed(\n",
    "        embed_dim, \n",
    "        grid_size, \n",
    "        cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67cf34",
   "metadata": {},
   "source": [
    "# Create MAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc30b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size=224, \n",
    "                 patch_size=16, \n",
    "                 in_chans=3,\n",
    "                 embed_dim=1024,\n",
    "                 depth=24, \n",
    "                 num_heads=16,\n",
    "                 decoder_embed_dim=512, \n",
    "                 decoder_depth=8, \n",
    "                 decoder_num_heads=16,\n",
    "                 mlp_ratio=4., \n",
    "                 norm_layer=nn.LayerNorm, \n",
    "                 norm_pix_loss=False):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\t\t# --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def initialize_weights(MaskedAutoencoderViT):\n",
    "\t# initialization\n",
    "    # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "    pos_embed = get_2d_sincos_pos_embed(\n",
    "        self.pos_embed.shape[-1], \n",
    "        int(self.patch_embed.num_patches**.5), \n",
    "        cls_token=True)\n",
    "\n",
    "    self.pos_embed.data.copy_(\n",
    "        torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "    decoder_pos_embed = get_2d_sincos_pos_embed(\n",
    "        self.decoder_pos_embed.shape[-1], \n",
    "        int(self.patch_embed.num_patches**.5), \n",
    "        cls_token=True)\n",
    "    self.decoder_pos_embed.data.copy_(\n",
    "        torch.from_numpy(\n",
    "            decoder_pos_embed\n",
    "            ).float().unsqueeze(0))\n",
    "\n",
    "    # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "    w = self.patch_embed.proj.weight.data\n",
    "    torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "    # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "    torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "    torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "    # initialize nn.Linear and nn.LayerNorm\n",
    "    self.apply(self._init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb55eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def _init_weights(MaskedAutoencoderViT, m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # we use xavier_uniform following official JAX ViT:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ce384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def patchify(MaskedAutoencoderViT, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d57bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def unpatchify(MaskedAutoencoderViT, x):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = self.patch_embed.patch_size[0]\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "        \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def random_masking(MaskedAutoencoderViT, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ebc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def forward_encoder(\n",
    "            self:MaskedAutoencoderViT, \n",
    "            x, \n",
    "            mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def forward_decoder(\n",
    "            self:MaskedAutoencoderViT, \n",
    "            x, \n",
    "            ids_restore):\n",
    "    # embed tokens\n",
    "    x = self.decoder_embed(x)\n",
    "\n",
    "    # append mask tokens to sequence\n",
    "    mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "    x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "    x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "    x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "    # add pos embed\n",
    "    x = x + self.decoder_pos_embed\n",
    "\n",
    "    # apply Transformer blocks\n",
    "    for blk in self.decoder_blocks:\n",
    "        x = blk(x)\n",
    "    x = self.decoder_norm(x)\n",
    "\n",
    "    # predictor projection\n",
    "    x = self.decoder_pred(x)\n",
    "\n",
    "    # remove cls token\n",
    "    x = x[:, 1:, :]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97263b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def forward_loss(\n",
    "            self:MaskedAutoencoderViT, \n",
    "            imgs, \n",
    "            pred, \n",
    "            mask):\n",
    "    \"\"\"\n",
    "    imgs: [N, 3, H, W]\n",
    "    pred: [N, L, p*p*3]\n",
    "    mask: [N, L], 0 is keep, 1 is remove, \n",
    "    \"\"\"\n",
    "    target = self.patchify(imgs)\n",
    "    if self.norm_pix_loss:\n",
    "        mean = target.mean(dim=-1, keepdim=True)\n",
    "        var = target.var(dim=-1, keepdim=True)\n",
    "        target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "    loss = (pred - target) ** 2\n",
    "    loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "    loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbf30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to\n",
    "def forward(\n",
    "            self:MaskedAutoencoderViT, \n",
    "            imgs, \n",
    "            mask_ratio=0.75):\n",
    "    latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "    pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "    loss = self.forward_loss(imgs, pred, mask)\n",
    "    return loss, pred, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4005c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# set recommended archs\n",
    "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
    "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a01916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('mae.model_development')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
