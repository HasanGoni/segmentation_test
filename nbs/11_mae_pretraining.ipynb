{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Auto Encoder for pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "> Masked autoenocder pre-training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All codes are taken from [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-pretraining/run_mae.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pretraining.mae_pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu121'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, Lambda, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 14:51:28.146053: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-01 14:51:28.146096: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-01 14:51:28.146104: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-01 14:51:28.151943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    ViTImageProcessor,\n",
    "    ViTMAEConfig,\n",
    "    ViTMAEForPreTraining,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.45.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/image-pretraining/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "\n",
    "\tdataset_name: Optional[str] = field(\n",
    "        default=\"cifar10\", metadata={\"help\": \"Name of a dataset from the datasets package\"}\n",
    "    )\n",
    "\tdataset_config_name: Optional[str] = field(\n",
    "        default=None, \n",
    "\t\tmetadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    \t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\ttrust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n",
    "                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n",
    "                \" code, as it will execute code present on the Hub on your local machine.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\timage_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of the images in the files.\"}\n",
    "    )\n",
    "\n",
    "\ttrain_dir: Optional[str] = field(\n",
    "            default=None, \n",
    "            metadata={\"help\": \"A folder containing the training data.\"}\n",
    "                  )\n",
    "\tvalidation_dir: Optional[str] = field(\n",
    "            default=None, metadata={\"help\": \"A folder containing the validation data.\"}\n",
    "                  )\n",
    "\ttrain_val_split: Optional[float] = field(\n",
    "        default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n",
    "    )\n",
    "\tmax_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\tmax_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\tdef __post_init__(self):\n",
    "\t\tdata_files = {}\n",
    "\t\tif self.train_dir is not None:\n",
    "\t\t\tdata_files[\"train\"] = self.train_dir\n",
    "\t\tif self.validation_dir is not None:\n",
    "\t\t\tdata_files[\"val\"] = self.validation_dir\n",
    "\t\tself.data_files = data_files if data_files else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/image processor we are going to pre-train.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name_or_path\"}\n",
    "    )\n",
    "    config_overrides: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n",
    "                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n",
    "    token: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n",
    "                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    mask_ratio: float = field(\n",
    "        default=0.75, metadata={\"help\": \"The ratio of the number of masked tokens in the input sequence.\"}\n",
    "    )\n",
    "    norm_pix_loss: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether or not to train with normalized pixel values as target.\"}\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class CustomTrainingArguments(TrainingArguments):\n",
    "    base_learning_rate: float = field(\n",
    "        default=1e-3, metadata={\"help\": \"Base learning rate: absolute_lr = base_lr * total_batch_size / 256.\"}\n",
    "    )\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main_():\n",
    "\n",
    "    # getting arguments and parser\n",
    "\tparser = HfArgumentParser((\n",
    "\t\tModelArguments, \n",
    "\t\tDataTrainingArguments, \n",
    "\t\tCustomTrainingArguments ))\n",
    "\tif len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "\t\tmodel_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "\telse:\n",
    "\t\tmodel_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "\n",
    "\t# Setup logging\n",
    "\tlogging.basicConfig(\n",
    "\t\tformat=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "\t\tdatefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "\t\thandlers=[logging.StreamHandler(sys.stdout)],\n",
    "\t)\n",
    "\n",
    "\tif training_args.should_log:\n",
    "\t\ttransformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "\tlog_level = training_args.get_process_log_level()\n",
    "\tlogger.setLevel(log_level)\n",
    "\ttransformers.utils.logging.set_verbosity(log_level)\n",
    "\ttransformers.utils.logging.enable_default_handler()\n",
    "\ttransformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "\t  # Log on each process the small summary:\n",
    "\tlogger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
    "        + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "\tlogger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "\t# Detecting last checkpoint.\n",
    "\tlast_checkpoint = None\n",
    "\tif os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "\t\tlast_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\t\tif last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "\t\t\traise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "\t\telif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "\t\t\tlogger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "\t# Initialize our dataset.\n",
    "\tds = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        data_files=data_args.data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        token=model_args.token,\n",
    "        trust_remote_code=data_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "\t#If we don't have a validation split, split off a percentage of train as validation.\n",
    "\tdata_args.train_val_split = None if \"validation\" in ds.keys() else data_args.train_val_split\n",
    "\tif isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n",
    "\t\tsplit = ds[\"train\"].train_test_split(data_args.train_val_split)\n",
    "\t\tds[\"train\"] = split[\"train\"]\n",
    "\t\tds[\"validation\"] = split[\"test\"]\n",
    "\n",
    "\n",
    "\t# Config updateing\n",
    "\tconfig_kwargs = {\n",
    "\n",
    "        \"cache_dir\": model_args.cache_dir,\n",
    "        \"revision\": model_args.model_revision,\n",
    "        \"token\": model_args.token,\n",
    "    }\n",
    "\n",
    "\tif model_args.config_name:\n",
    "\t\tconfig = ViTMAEConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "\telif model_args.model_name_or_path:\n",
    "\t\tconfig = ViTMAEConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "\telse:\n",
    "\t\tconfig = ViTMAEConfig()\n",
    "\t\tlogger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\t\tif model_args.config_overrides is not None:\n",
    "\t\t\tlogger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "\t\t\tconfig.update_from_string(model_args.config_overrides)\n",
    "\t\t\tlogger.info(f\"New config: {config}\")\n",
    "\n",
    "\t# adapt config\n",
    "\tconfig.update(\n",
    "    \t{\n",
    "        \t\"mask_ratio\": model_args.mask_ratio,\n",
    "        \t\"norm_pix_loss\": model_args.norm_pix_loss,\n",
    "    \t}\n",
    "\t)\n",
    "\n",
    "\t# create image processor\n",
    "\tif model_args.image_processor_name:\n",
    "\t\timage_processor = ViTImageProcessor.from_pretrained(model_args.image_processor_name, **config_kwargs)\n",
    "\telif model_args.model_name_or_path:\n",
    "\t\timage_processor = ViTImageProcessor.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "\telse:\n",
    "\t\timage_processor = ViTImageProcessor()\n",
    "\n",
    "\t# create model\n",
    "\tif model_args.model_name_or_path:\n",
    "\t\tmodel = ViTMAEForPreTraining.from_pretrained(\n",
    "\t\t\tmodel_args.model_name_or_path,\n",
    "\t\t\tfrom_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "\t\t\tconfig=config,\n",
    "\t\t\tcache_dir=model_args.cache_dir,\n",
    "\t\t\trevision=model_args.model_revision,\n",
    "\t\t\ttoken=model_args.token,\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tlogger.info(\"Training new model from scratch\")\n",
    "\t\tmodel = ViTMAEForPreTraining(config)\n",
    "\n",
    "\tif training_args.do_train:\n",
    "\t\tcolumn_names = ds[\"train\"].column_names\n",
    "\telse:\n",
    "\t\tcolumn_names = ds[\"validation\"].column_names\n",
    "\n",
    "\tif data_args.image_column_name is not None:\n",
    "\t\timage_column_name = data_args.image_column_name\n",
    "\telif \"image\" in column_names:\n",
    "\t\timage_column_name = \"image\"\n",
    "\telif \"img\" in column_names:\n",
    "\t\timage_column_name = \"img\"\n",
    "\telse:\n",
    "\t\timage_column_name = column_names[0]\n",
    "\n",
    "\t# transformations as done in original MAE paper\n",
    "\t# source: https://github.com/facebookresearch/mae/blob/main/main_pretrain.py\n",
    "\tif \"shortest_edge\" in image_processor.size:\n",
    "\t\tsize = image_processor.size[\"shortest_edge\"]\n",
    "\telse:\n",
    "\t\tsize = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\ttransforms = Compose(\n",
    "\t\t[\n",
    "\t\t\tLambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "\t\t\tRandomResizedCrop(size, scale=(0.2, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "\t\t\tRandomHorizontalFlip(),\n",
    "\t\t\tToTensor(),\n",
    "\t\t\tNormalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "\t\t]\n",
    "\t)\n",
    "\n",
    "\tdef preprocess_images(examples):\n",
    "\t\t\"\"\"Preprocess a batch of images by\"\"\"\n",
    "\t\texamples[\"pixel_values\"] = [transforms(image) for image in examples[image_column_name]]\n",
    "\t\treturn examples\n",
    "\n",
    "\tif training_args.do_train:\n",
    "\t\tif \"train\" not in ds:\n",
    "\t\t\traise ValueError(\"--do_train requires a train dataset\")\n",
    "\t\tif data_args.max_train_samples is not None:\n",
    "\t\t\tds[\"train\"] = ds[\"train\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\n",
    "\t\t# Set the training transforms\n",
    "\t\tds[\"train\"].set_transform(preprocess_images)\n",
    "\n",
    "\tif training_args.do_eval:\n",
    "\t\tif \"validation\" not in ds:\n",
    "\t\t\traise ValueError(\"--do_eval requires a validation dataset\")\n",
    "\t\tif data_args.max_eval_samples is not None:\n",
    "\t\t\tds[\"validation\"] = (\n",
    "\t\t\t\tds[\"validation\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\n",
    "\t\t\t)\n",
    "\t\t# Set the validation transforms\n",
    "\t\tds[\"validation\"].set_transform(preprocess_images)\n",
    "\n",
    "\t\t# Compute absolute learning rate\n",
    "\t\ttotal_train_batch_size = (\n",
    "\t\t\ttraining_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "\t\t)\n",
    "\t\tif training_args.base_learning_rate is not None:\n",
    "\t\t\ttraining_args.learning_rate = training_args.base_learning_rate * total_train_batch_size / 256\n",
    "\n",
    "\t\t# Initialize our trainer\n",
    "\t\ttrainer = Trainer(\n",
    "\t\t\tmodel=model,\n",
    "\t\t\targs=training_args,\n",
    "\t\t\ttrain_dataset=ds[\"train\"] if training_args.do_train else None,\n",
    "\t\t\teval_dataset=ds[\"validation\"] if training_args.do_eval else None,\n",
    "\t\t\ttokenizer=image_processor,\n",
    "\t\t\tdata_collator=collate_fn,\n",
    "\t\t)\n",
    "\n",
    "\t\t# Training\n",
    "\t\tif training_args.do_train:\n",
    "\t\t\tcheckpoint = None\n",
    "\t\t\tif training_args.resume_from_checkpoint is not None:\n",
    "\t\t\t\tcheckpoint = training_args.resume_from_checkpoint\n",
    "\t\t\telif last_checkpoint is not None:\n",
    "\t\t\t\tcheckpoint = last_checkpoint\n",
    "\t\t\ttrain_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\t\t\ttrainer.save_model()\n",
    "\t\t\ttrainer.log_metrics(\"train\", train_result.metrics)\n",
    "\t\t\ttrainer.save_metrics(\"train\", train_result.metrics)\n",
    "\t\t\ttrainer.save_state()\n",
    "\n",
    "\t\t# Evaluation\n",
    "\t\tif training_args.do_eval:\n",
    "\t\t\tmetrics = trainer.evaluate()\n",
    "\t\t\ttrainer.log_metrics(\"eval\", metrics)\n",
    "\t\t\ttrainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "\t\t# Write model card and (optionally) push to hub\n",
    "\t\tkwargs = {\n",
    "\t\t\t\"tasks\": \"masked-auto-encoding\",\n",
    "\t\t\t\"dataset\": data_args.dataset_name,\n",
    "\t\t\t\"tags\": [\"masked-auto-encoding\"],\n",
    "\t\t}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9189c604fa7d4be8ad78a630f982ea2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41315b8cc14f4b8ba76766f9aae7ea3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df585eda619e4d28b3344e97e68ba459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ce27b609d4a26bacfdc876367ecaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac0271a3f1f4ed3b182f0d0f1079692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Initialize our dataset.\n",
    "ds = load_dataset(\n",
    "\t    'cifar10',\n",
    "        #data_args.dataset_config_name,\n",
    "        data_files=data_args['data_files'],\n",
    "        cache_dir=None,\n",
    "        token=model_args['token'],\n",
    "        trust_remote_code=data_args['trust_remote_code'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_files': None, 'trust_remote_code': False, 'train_val_split': 0.15}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_splits(\n",
    "        ds, \n",
    "        data_args\n",
    "        ):\n",
    "\t\"check splits\"\n",
    "\tdata_args['train_val_split'] = None if 'validation' in ds.keys() else data_args['train_val_split']\n",
    "\tif isinstance(data_args['train_val_split'], float) and data_args['train_val_split'] > 0:\n",
    "\t\tsplit = ds['train'].train_test_split(data_args['train_val_split'])\n",
    "\t\tds['train'] = split['train']\n",
    "\t\tds['validation'] = split['test']\n",
    "\treturn ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = check_splits(ds, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 42500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 7500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args['train_val_split']=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_files': None, 'trust_remote_code': False, 'train_val_split': 0.15}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_mae.py \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir ./vit-mae-demo \\\n",
    "    --remove_unused_columns False \\\n",
    "    --label_names pixel_values \\\n",
    "    --mask_ratio 0.75 \\\n",
    "    --norm_pix_loss \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --base_learning_rate 1.5e-4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --num_train_epochs 800 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_strategy epoch \\\n",
    "    --save_strategy epoch \\\n",
    "    --load_best_model_at_end True \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args['model_revision'] = 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args['cache_dir'],\n",
    "    \"revision\": model_args['model_revision'],\n",
    "    \"token\": model_args['token']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args['config_name'] = None\n",
    "model_args['model_name_or_path'] = None\n",
    "model_args['config_overrides'] = None\n",
    "model_args['mask_ratio'] = 0.75\n",
    "model_args['norm_pix_loss'] = True\n",
    "model_args['image_processor_name'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(model_args):\n",
    "\tif model_args.config_name:\n",
    "\t\tconfig = ViTMAEConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "\telif model_args.model_name_or_path:\n",
    "\t\tconfig = ViTMAEConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "\telse:\n",
    "\t\tconfig = ViTMAEConfig()\n",
    "\t\tlogger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\t\tif model_args.config_overrides is not None:\n",
    "\t\t\tlogger.info(f\"Overriding config: {model_args.config_overrides}\")\n",
    "\t\t\tconfig.update_from_string(model_args.config_overrides)\n",
    "\t\t\tlogger.info(f\"New config: {config}\")\n",
    "\n",
    "    # adapt config\n",
    "\tconfig.update(\n",
    "        {\n",
    "            \"mask_ratio\": model_args.mask_ratio,\n",
    "            \"norm_pix_loss\": model_args.norm_pix_loss,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_local(model_args):\n",
    "\tif model_args['config_name']:\n",
    "\t\tconfig = ViTMAEConfig.from_pretrained(model_args['config_name'], **config_kwargs)\n",
    "\telif model_args['model_name_or_path']:\n",
    "\t\tconfig = ViTMAEConfig.from_pretrained(model_args['model_name_or_path'], **config_kwargs)\n",
    "\telse:\n",
    "\t\tconfig = ViTMAEConfig()\n",
    "\t\tlogger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\t\tif model_args['config_overrides'] is not None:\n",
    "\t\t\tlogger.info(f\"Overriding config: {model_args['config_overrides']}\")\n",
    "\t\t\tconfig.update_from_string(model_args['config_overrides'])\n",
    "\t\t\tlogger.info(f\"New config: {config}\")\n",
    "\n",
    "    # adapt config\n",
    "\tconfig.update(\n",
    "        {\n",
    "            \"mask_ratio\": model_args['mask_ratio'],\n",
    "            \"norm_pix_loss\": model_args['norm_pix_loss'],\n",
    "        }\n",
    "    )\n",
    "\treturn config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are instantiating a new config instance from scratch.\n"
     ]
    }
   ],
   "source": [
    "config = get_config_local(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 512,\n",
       "  \"decoder_intermediate_size\": 2048,\n",
       "  \"decoder_num_attention_heads\": 16,\n",
       "  \"decoder_num_hidden_layers\": 8,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mask_ratio\": 0.75,\n",
       "  \"model_type\": \"vit_mae\",\n",
       "  \"norm_pix_loss\": true,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.45.0.dev0\"\n",
       "}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_processor(model_args):\n",
    "\tif model_args.image_processor_name:\n",
    "\t\timage_processor = ViTImageProcessor.from_pretrained(model_args.image_processor_name, **config_kwargs)\n",
    "\telse:\n",
    "\t\timage_processor = ViTImageProcessor.from_config(config)\n",
    "\treturn image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_processor_local(model_args,confg):\n",
    "\tif model_args['image_processor_name']:\n",
    "\t\timage_processor = ViTImageProcessor.from_pretrained(model_args['image_processor_name'], **config_kwargs)\n",
    "\telif model_args['model_name_or_path']:\n",
    "\t\timage_processor = ViTImageProcessor.from_pretrained(model_args['model_name_or_path'], **config_kwargs)\n",
    "\telse:\n",
    "\t\timage_processor = ViTImageProcessor()\n",
    "\treturn image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor =get_image_processor_local(model_args,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_args, config):\n",
    "\tif model_args.model_name_or_path:\n",
    "\t\tmodel = ViTMAEForPreTraining.from_pretrained(\n",
    "\t\t\tmodel_args.model_name_or_path,\n",
    "\t\t\tfrom_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "\t\t\tconfig=config,\n",
    "\t\t\tcache_dir=model_args.cache_dir,\n",
    "\t\t\trevision=model_args.model_revision,\n",
    "\t\t\ttoken=model_args.token,\n",
    "\t)\n",
    "\telse:\n",
    "\t\tlogger.info(\"Training new model from scratch\")\n",
    "\t\tmodel = ViTMAEForPreTraining(config)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_local(model_args, config):\n",
    "\tif model_args['model_name_or_path']:\n",
    "\t\tmodel = ViTMAEForPreTraining.from_pretrained(\n",
    "\t\t\tmodel_args['model_name_or_path'],\n",
    "\t\t\tfrom_tf=bool(\".ckpt\" in model_args['model_name_or_path']),\n",
    "\t\t\tconfig=config,\n",
    "\t\t\tcache_dir=model_args['cache_dir'],\n",
    "\t\t\trevision=model_args['model_revision'],\n",
    "\t\t\ttoken=model_args['token'],\n",
    "\t)\n",
    "\telse:\n",
    "\t\tlogger.info(\"Training new model from scratch\")\n",
    "\t\tmodel = ViTMAEForPreTraining(config)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_local(model_args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEForPreTraining(\n",
       "  (vit): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAESdpaAttention(\n",
       "            (attention): ViTMAESdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAESdpaAttention(\n",
       "          (attention): ViTMAESdpaSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img', 'label']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_column_name(training_args, data_args, ds):\n",
    "\tif training_args.do_train:\n",
    "\t\tcolumn_names = ds['train'].column_names\n",
    "\telse:\t\n",
    "\t\tcolumn_names = ds['validation'].column_names\n",
    "\treturn column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_name_local(training_args, data_args, ds):\n",
    "\tif training_args['do_train']:\n",
    "\t\tcolumn_names = ds['train'].column_names\n",
    "\telse:\t\n",
    "\t\tcolumn_names = ds['validation'].column_names\n",
    "\treturn column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = get_column_name_local(training_args, data_args, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img', 'label']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_image_column_name(data_args, column_names):\n",
    "\n",
    "\tif data_args.image_column_name is not None:\n",
    "\t\timage_column_name = data_args.image_column_name\n",
    "\telif \"image\" in column_names:\n",
    "\t\timage_column_name = \"image\"\n",
    "\telif \"img\" in column_names:\n",
    "\t\timage_column_name = \"img\"\n",
    "\telse:\n",
    "\t\timage_column_name = column_names[0]\n",
    "\treturn image_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_column_name_local(data_args, column_names):\n",
    "\n",
    "\tif data_args['image_column_name'] is not None:\n",
    "\t\timage_column_name = data_args['image_column_name']\n",
    "\telif \"image\" in column_names:\n",
    "\t\timage_column_name = \"image\"\n",
    "\telif \"img\" in column_names:\n",
    "\t\timage_column_name = \"img\"\n",
    "\telse:\n",
    "\t\timage_column_name = column_names[0]\n",
    "\treturn image_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args['image_column_name'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_column_name = get_image_column_name_local(data_args, column_names)\n",
    "image_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m (image_processor\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m], image_processor\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m size \u001b[38;5;241m=\u001b[39m get_shortest_edge(\u001b[43mimage_processor\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_processor' is not defined"
     ]
    }
   ],
   "source": [
    "def get_shortest_edge(image_processor):\n",
    "\tif 'shortest_edge' in image_processor.size:\n",
    "\t\treturn image_processor.size['shortest_edge']\n",
    "\telse:\n",
    "\t\treturn (image_processor.size['height'], image_processor.size['width'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ds(\n",
    "\t\ttraining_args, \n",
    "\t\tdata_args, \n",
    "\t\tds, \n",
    "\t\timage_processor, \n",
    "\t\tsize\n",
    "\t\t):\n",
    "\tcolumn_names = get_column_name_local(training_args, data_args, ds)\n",
    "\timage_column_name = get_image_column_name_local(data_args, column_names)\n",
    "\treturn column_names, image_column_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose(\n",
    "        \t\t\t[\n",
    "            \t\tLambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "            \t\tRandomResizedCrop(size, scale=(0.2, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "            \t\tRandomHorizontalFlip(),\n",
    "            \t\tToTensor(),\n",
    "            \t\tNormalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "        \t\t\t]\n",
    "    \t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(example):\n",
    "\texample['pixel_values'] = [transforms(i) for i in example[image_column_name]]\n",
    "\treturn example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline_local(\n",
    "\t\tmodel:ViTMAEForPreTraining,\n",
    "        training_args:dict,\n",
    "\t\tmodel_args:dict,\n",
    "\t\tdata_args:dict,\n",
    "        ds:dict\n",
    "\n",
    "\t):\n",
    "\n",
    "\tif training_args.do_train:\n",
    "\t\tif \"train\" not in ds:\n",
    "\t\t\traise ValueError(\"--do_train requires a train dataset\")\n",
    "\t\tif data_args['max_train_samples'] is not None:\n",
    "\t\t\t# in case of training we want to shuffle the dataset\n",
    "\t\t\tds['train'] = ds['train'].shuffle(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tseed=training_args['seed']\n",
    "\t\t\t\t\t\t\t\t\t\t\t).select(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\trange(data_args['max_train_samples'])\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\n",
    "\t\tds[\"train\"].set_transform(preprocess_image)\n",
    "\n",
    "\tif training_args.do_eval:\n",
    "\t\tif \"validation\" not in ds:\n",
    "\t\t\traise ValueError(\"--do_eval requires a validation dataset\")\n",
    "\t\tif data_args['max_eval_samples'] is not None:\n",
    "\t\t\t\n",
    "\t\t\tds['validation'] = ds['validation'].select(range(data_args['max_eval_samples']))\n",
    "\t\tds['validation'].set_transform(preprocess_image)\n",
    "\n",
    "\t# Computer absolute learning rate\n",
    "\ttotal_train_batch_size = (\n",
    "\t\ttraining_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
    "\t)\n",
    "\n",
    "\tif training_args.base_learning_rate is not None:\n",
    "\t\ttraining_args.learning_rate = training_args.base_learning_rate * total_train_batch_size / 256\n",
    "\n",
    "\t# Initialize our Trainer\n",
    "\n",
    "\ttrainer = Trainer(\n",
    "\t                 model=model,\n",
    "\t\t\t\t\t args=training_args,\n",
    "\t\t\t\t\t train_dataset=ds['train'] if training_args.do_train else None,\n",
    "\t\t\t\t\t eval_dataset=ds['validation'] if training_args.do_eval else None,\n",
    "\t\t\t\t\t tokenizer=image_processor,\n",
    "\t\t\t\t\t data_collator=collate_fn,\n",
    "\t)\n",
    "\n",
    "\t# getting last checkpoint\n",
    "\tlast_checkpoint = None\n",
    "\tif os.path.isdir(\n",
    "\t\ttraining_args.output_dir\n",
    "\t\t) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "\n",
    "\t\tlast_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\t\tif last_checkpoint is not None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "\t\t\traise ValueError(\n",
    "\t\t\t\tF\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "\t\t\t\t\"Use --overwrite_output_dir to overcome.\"\n",
    "\t\t\t)\n",
    "\n",
    "\t\telif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "\t\t\tlogger.info(\n",
    "\t\t\t\tf\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "\t\t\t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tif training_args.do_train:\n",
    "\t\tlast_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "\t# Training\n",
    "\tif training_args.do_train:\n",
    "\t\tcheckpoint=None\n",
    "\t\tif training_args.resume_from_checkpoint is not None:\n",
    "\t\t\tcheckpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "\t\telif last_checkpoint is not None:\n",
    "\t\t\tcheckpoint = last_checkpoint\n",
    "\n",
    "\t\ttrain_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\t\ttrainer.save_model()\n",
    "\t\ttrainer.log_metrics(\"train\", train_result.metrics)\n",
    "\t\ttrainer.save_metrics(\"train\", train_result.metrics)\n",
    "\t\ttrainer.save_state()\n",
    "\n",
    "\t# Evaluation\n",
    "\tif training_args.do_eval:\n",
    "\t\tmetrics = trainer.evaluate()\n",
    "\t\ttrainer.log_metrics(\"eval\", metrics)\n",
    "\t\ttrainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "\t# Write model card and (optionally) push to hub\n",
    "\tkwargs = {\n",
    "    \t\"tasks\": \"masked-auto-encoding\",\n",
    "    \t\"dataset\": data_args.dataset_name,\n",
    "    \t\"tags\": [\"masked-auto-encoding\"],\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "\tmain_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('11_mae_pretraining.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib_dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
