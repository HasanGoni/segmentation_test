{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 10:54:40.702106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 10:54:40.813404: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import albumentations as A\n",
    "import random\n",
    "import cv2\n",
    "from fastcore.basics import patch\n",
    "from fastcore.all import *\n",
    "#from fastai.vision.all import *\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Union, List, Tuple, Optional, Callable, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "EPOCS = 5\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 30\n",
    "class_names = ['Pin']\n",
    "train_count = 107\n",
    "test_count = 27\n",
    "num_classes = len(class_names)\n",
    "steps_per_epoch = train_count // BATCH_SIZE\n",
    "validation_steps = test_count // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/models'),Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/X'),Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/y')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "path = Path(r'/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224')\n",
    "path.ls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Preprocess:\n",
    "    image_path:Union[Path, str]\n",
    "    label_path:Union[Path, str]\n",
    "    im_height:int = IMAGE_HEIGHT\n",
    "    im_width:int = IMAGE_WIDTH\n",
    "    bf_size:int = BUFFER_SIZE\n",
    "    bs:int = BATCH_SIZE\n",
    "    one_channel:bool=False\n",
    "    test_size:float = 0.2\n",
    "    img_ext:str = field(default_factory=str, init=False, repr=True)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.img_ext = Path(self.image_path).ls()[0].suffix\n",
    "        pat = f'*{self.img_ext}'\n",
    "        self.images = [str(i) for i in Path(self.image_path).rglob(pat)]\n",
    "        self.labels = [str(i) for i in Path(self.label_path).rglob(pat)]\n",
    "        self.train_images, self.test_images, \\\n",
    "            self.train_labels, self.test_labels = train_test_split(\n",
    "                         self.images,\n",
    "                         self.labels, \n",
    "                         test_size=self.test_size,\n",
    "                         random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Preprocess(\n",
    "                           image_path=path/'X',\n",
    "                           label_path=path/'y',\n",
    "                           im_height=IMAGE_HEIGHT,\n",
    "                           im_width=IMAGE_WIDTH,\n",
    "                           bs=BATCH_SIZE,\n",
    "                           one_channel=False,\n",
    "                           test_size=0.2\n",
    "                           )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Augmentation needs to be done before resizing the image\n",
    "- - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def show_image(self,im_file):\n",
    "    #image = self.from_file_to_image(im_file)\n",
    "    plt.imshow(im_file)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def read_image(self,im_file, one_channel=False):\n",
    "    if one_channel:\n",
    "        im = tf.io.read_file(im_file)\n",
    "        im = tf.image.decode_png(im, channels=1)\n",
    "    else:\n",
    "        im = tf.io.read_file(im_file)\n",
    "        im = tf.image.decode_png(im, channels=3)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# testing one channel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 10:54:58.979576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:54:58.996838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:54:58.997556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:54:58.999003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 10:54:59.003367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:54:59.004910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:54:59.005818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:55:00.242731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:55:00.243227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:55:00.243526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-18 10:55:00.243628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-18 10:55:00.243890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1189 MB memory:  -> device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "fn =str((path/'X').ls()[0])\n",
    "fn_lbl = str((path/'y').ls()[0])\n",
    "img_ = preprocess_obj.read_image(fn, one_channel=True)\n",
    "test_eq(img_.numpy().shape, (1024, 1224, 1))\n",
    "img_ = preprocess_obj.read_image(fn, one_channel=False)\n",
    "lbl_ = preprocess_obj.read_image(fn_lbl, one_channel=False)\n",
    "test_eq(img_.numpy().shape, (1024, 1224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def augmentation_(\n",
    "        self,\n",
    "        im_height:int,\n",
    "        im_width:int,\n",
    "        image:tf.Tensor,\n",
    "        mask:tf.Tensor,\n",
    "        ):\n",
    "    aug = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.RandomSizedCrop(min_max_height=(250, 250), height=im_height, width=im_width, p=0.5),\n",
    "        A.PadIfNeeded(\n",
    "                      #min_height=im_height,\n",
    "                     # min_width=im_width,\n",
    "                        p=0.5)\n",
    "    ], p=1),    \n",
    "    A.HorizontalFlip(p=0.5),              \n",
    "    A.VerticalFlip(p=0.5),              \n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
    "        A.GridDistortion(p=0.5),\n",
    "        A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)                  \n",
    "        ], p=0.8),\n",
    "    #A.CLAHE(p=0.8),\n",
    "    A.RandomBrightnessContrast(p=0.8),    \n",
    "    A.RandomGamma(p=0.8)])\n",
    "    aug_data = aug(image=image.numpy(), mask=mask.numpy())\n",
    "    image, mask = aug_data['image'], aug_data['mask']\n",
    "    #mask = tf.expand_dims(mask, axis=-1)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def show_aug(\n",
    "    self,\n",
    "    image,\n",
    "    mask,\n",
    "    original_image=None,\n",
    "    original_mask=None\n",
    "    ):\n",
    "    \n",
    "    fontsize = 18\n",
    "    \n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
    "        ax[0, 0].axis('off')\n",
    "        \n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
    "        ax[1, 0].axis('off')\n",
    "        \n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
    "        ax[0, 1].axis('off')\n",
    "\n",
    "        \n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)\n",
    "        ax[1, 1].axis('off')\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def read_aug(\n",
    "            self,\n",
    "            im_file:str,\n",
    "            lbl_file:str,\n",
    "            one_channel:bool=False, \n",
    "            aug:bool=False):\n",
    "    img = self.read_image(im_file=im_file, one_channel=one_channel)\n",
    "    mask = self.read_image(im_file=lbl_file, one_channel=one_channel)\n",
    "    if aug:\n",
    "        return self.augmentation_(im_height=img.shape[1], im_width=img.shape[0], image=img, mask=mask)\n",
    "    else:\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, msk = preprocess_obj.read_aug(im_file=fn, lbl_file=fn_lbl, one_channel=False, aug=True)\n",
    "im.shape, msk.shape\n",
    "test_eq_type(type(im), np.ndarray), test_eq_type(type(msk), np.ndarray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_obj.show_aug(image=im, mask=msk, original_image=img_.numpy(), original_mask=lbl_.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@ patch_to(Preprocess)\n",
    "def normalize(\n",
    "              self,\n",
    "              image:Union[np.ndarray, tf.Tensor], \n",
    "              min=0):\n",
    "    def _normalize(im):\n",
    "        img = tf.cast(im, tf.float32)\n",
    "        return img / 255.0\n",
    "\n",
    "    if min == 0:\n",
    "        return _normalize(image)\n",
    "    else:\n",
    "        return (_normalize(image) * 2.0) -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224, 1024, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0, 0.87058824, 222)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_img = preprocess_obj.normalize(im)\n",
    "np_img_ = norm_img.numpy()\n",
    "np_img_.min(), im.min(), np_img_.max(), im.max()\n",
    "#test_eq(np_img_.min(),0.0), test_eq(np_img_.max(),1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@ patch_to(Preprocess)\n",
    "def process_image_and_mask(\n",
    "                       self,\n",
    "                       im_file:str,\n",
    "                       lbl_file:str,\n",
    "                       norm:bool=True,\n",
    "                       one_channel:bool=False,\n",
    "                       aug_data:bool=True\n",
    "                       ):\n",
    "    image,mask = self.read_aug(im_file, lbl_file, one_channel=one_channel, aug=aug_data)\n",
    "    image, mask = tf.image.resize(image, (self.im_height, self.im_width)), tf.image.resize(mask, (self.im_height, self.im_width))\n",
    "    if norm:\n",
    "        image = self.normalize(image)\n",
    "        mask = self.normalize(mask)\n",
    "    if one_channel:\n",
    "        image = tf.reshape(image, (self.im_height, self.im_width, 1,))\n",
    "        mask = tf.reshape(mask, (self.im_height, self.im_width, 1,))\n",
    "    else:\n",
    "        image = tf.reshape(image, (self.im_height, self.im_width, 3,))\n",
    "        mask = tf.reshape(mask, (self.im_height, self.im_width, 3,))\n",
    "    return image,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, msk = preprocess_obj.process_image_and_mask(im_file=fn, lbl_file=fn_lbl, norm=True, one_channel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(im),  type(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(im.numpy().shape, (IMAGE_HEIGHT,IMAGE_WIDTH,3))\n",
    "test_eq(msk.numpy().shape, (IMAGE_HEIGHT,IMAGE_WIDTH,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@ patch_to(Preprocess)\n",
    "def process_data(\n",
    "                 self,\n",
    "                 image,\n",
    "                 label,\n",
    "                 norm:bool=True,\n",
    "                 one_channel:bool=False,\n",
    "                 aug_data:bool=True):\n",
    "    #@tf.function\n",
    "    aug_img, aug_lbl = tf.numpy_function(func=self.process_image_and_mask, inp=[image, label, norm, one_channel, aug_data], Tout=(tf.float32, tf.float32))\n",
    "    return aug_img, aug_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def set_shapes(\n",
    "              self,\n",
    "              img, \n",
    "              label, \n",
    "              img_shape):\n",
    "    img.set_shape(img_shape)\n",
    "    label.set_shape(img_shape)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def create_dataset(\n",
    "                  self,\n",
    "                  images, labels,\n",
    "                  train:bool=True,\n",
    "                  norm:bool=True,\n",
    "                  aug:bool=True\n",
    "                  ):\n",
    "    _dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    _dataset = _dataset.map(partial(self.process_data, aug_data=aug, norm=norm), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if self.one_channel:\n",
    "        _dataset = _dataset.map(partial(self.set_shapes, img_shape=(self.im_height, self.im_width, 1)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        _dataset = _dataset.map(partial(self.set_shapes, img_shape=(self.im_height, self.im_width, 3)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if train:\n",
    "        return  _dataset\\\n",
    "                   .cache()\\\n",
    "                   .shuffle(\n",
    "                            self.bf_size,\n",
    "                            reshuffle_each_iteration=True)\\\n",
    "                   .batch(self.bs)\\\n",
    "                   .repeat()\\\n",
    "                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        #_dataset = _dataset.map(self.process_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        return  _dataset.batch(self.bs).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def create_train_test_dataset(self):\n",
    "    self.train_dataset = self.create_dataset(\n",
    "                                            images=self.train_images, \n",
    "                                            labels=self.train_labels,\n",
    "                                            norm=True,\n",
    "                                            aug=True,\n",
    "                                            train=True)\n",
    "    self.test_dataset = self.create_dataset(\n",
    "                                           images=self.test_images,\n",
    "                                           labels=self.test_labels,\n",
    "                                           norm=True,\n",
    "                                           aug=False,\n",
    "                                           train=False)\n",
    "    return self.train_dataset, self.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "train_ds, test_ds = preprocess_obj.create_train_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TakeDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None))>,\n",
       " <TakeDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None))>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "train_ds.take(1), test_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 10:55:42.416394: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ds, masks_ds = one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_np_and_uint8(img:tf.Tensor)->Tuple[np.array, np.array]:\n",
    "    \"Convert img to np.array and uint8\"\n",
    "\n",
    "    if isinstance(img, tf.data.Dataset) or isinstance(img, tf.Tensor):\n",
    "        m_scale1=img.numpy()\n",
    "        m_scale255=(img * 255).numpy().astype(np.uint8)\n",
    "    elif isinstance(img, np.ndarray):\n",
    "            \n",
    "        if img.dtype in [np.float16, np.float32, np.float64]:\n",
    "            m_scale255 = (img * 255).astype(np.uint8)\n",
    "            m_scale1 = img\n",
    "            raise Exception(\"unknown dtype:\", img.dtype)\n",
    "\n",
    "    return m_scale1, m_scale255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_one_channel(img:tf.Tensor):\n",
    "    \"Convert image to one channel\"\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    img = tf.reshape(img, (IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def create_color_mask(\n",
    "             self,\n",
    "             mask:Union[np.array, tf.Tensor], \n",
    "             img:Union[np.array, tf.Tensor],\n",
    "             threshold:float=0.5):\n",
    "    \"Creating color mask for segmentation\"\n",
    "    overlay_mask = np.ones((self.im_height, self.im_width, 3,), dtype=np.uint8)\n",
    "    overlay_mask[:, :, 0] = img.reshape(*(self.im_height, self.im_width))\n",
    "    overlay_mask[:, :, 1] = img.reshape(*(self.im_height, self.im_width))\n",
    "    overlay_mask[:, :, 2] = img.reshape(*(self.im_height, self.im_width))\n",
    "    match = mask.reshape(*(self.im_height, self.im_width)) > (threshold * 255)\n",
    "    overlay_mask[match] = [0,255,0]\n",
    "    return overlay_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def display_np_batch(\n",
    "                    self,\n",
    "                    images:np.ndarray,\n",
    "                    masks:np.ndarray,\n",
    "                    threshold:float=0.5):\n",
    "    \"Displaying batch of images and masks\"\n",
    "    n_batch = images.shape[0]\n",
    "    for i in range(n_batch):\n",
    "        if images[i].shape[2] ==3:\n",
    "            img = images[i][:,:,0]\n",
    "            mask = masks[i][:,:,0]\n",
    "        else:\n",
    "            img = images[i].reshape(self.im_height, self.im_width )\n",
    "            mask = masks[i].reshape(self.im_height, self.im_width )\n",
    "        \n",
    "        _, ax = plt.subplots(1, 3, figsize=(15, 10)) \n",
    "        ax[0].imshow(img, cmap='gray')\n",
    "        ax[0].axis('off')\n",
    "        ax[0].set_title('only image')\n",
    "        clr_mask_ = self.create_color_mask(mask=mask, img=img, threshold=threshold)\n",
    "        ax[1].imshow(mask)\n",
    "        ax[1].axis('off')\n",
    "        ax[1].set_title('only mask')\n",
    "        ax[2].imshow(clr_mask_)\n",
    "        ax[2].axis('off')\n",
    "        ax[2].set_title('image with mask')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Preprocess)\n",
    "def display_ds(self,\n",
    "            ds:tf.data.Dataset):\n",
    "\n",
    "    images, masks = next(iter(ds))\n",
    "\n",
    "    # convert to numpy and uint8 and getting scaled(255) image and mask\n",
    "    images_np, images_255_np = convert_np_and_uint8(images)\n",
    "    masks_np, masks_255_np = convert_np_and_uint8(masks)\n",
    "    self.display_np_batch(\n",
    "                          images=images_255_np,\n",
    "                          masks=masks_255_np, \n",
    "                          threshold=0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_obj.display_ds(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
