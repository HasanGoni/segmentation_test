{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Training script in MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pretraining Training script in MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mae.pretraining_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import segmentation_test.mae.misc as misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import math\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Scaling and Norm Counting: \n",
    "\n",
    "\n",
    "Dealing with mixed precision training, where we use both 16-bit and 32-bit floating-point numbers to speed up computation and reduce memory usage. But this can lead to some tricky numerical issues. Enter the `NativeScalerWithGradNormCount` class!\n",
    "\n",
    "## What's this class all about?\n",
    "\n",
    "This nifty little class is a wrapper around PyTorch's `GradScaler`. It's designed to handle the intricacies of mixed precision training while also giving us some extra goodies like gradient norm calculation.\n",
    "\n",
    "## Let's break it down:\n",
    "\n",
    "1. **Initialization**: We start by creating a `GradScaler` object. This is PyTorch's built-in tool for automatic mixed precision training.\n",
    "\n",
    "2. **The `__call__` method**: This is where the magic happens!\n",
    "   - It scales the loss and performs backpropagation.\n",
    "   - If we're updating gradients, it handles gradient unscaling and clipping.\n",
    "   - It also calculates the gradient norm, which is super useful for monitoring training stability.\n",
    "\n",
    "3. **State management**: The `state_dict` and `load_state_dict` methods allow us to save and load the scaler's state. This is crucial for resuming training from checkpoints.\n",
    "\n",
    "## Why is this so cool?\n",
    "\n",
    "- It seamlessly integrates mixed precision training into our workflow.\n",
    "- It provides gradient clipping out of the box, which helps prevent exploding gradients.\n",
    "- The gradient norm calculation gives us valuable insights into our training process.\n",
    "\n",
    "By using this class, we're not just training our model - we're training it smartly and efficiently. It's like having a personal trainer for your neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(\n",
    "\t\tself, \n",
    "\t\tloss, \n",
    "\t\toptimizer, \n",
    "\t\tclip_grad=None, \n",
    "\t\tparameters=None, \n",
    "\t\tcreate_graph=False, \n",
    "\t\tupdate_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adjust_learning_rate(\n",
    "\toptimizer, \n",
    "\tepoch, \n",
    "\targs\n",
    "\t):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < args.warmup_epochs:\n",
    "        lr = args.lr * epoch / args.warmup_epochs \n",
    "    else:\n",
    "        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_adjust_learning_rate():\n",
    "    class DummyOptimizer:\n",
    "        def __init__(self):\n",
    "            self.param_groups = [\n",
    "\t\t\t\t{\"lr\": 0.1}, \n",
    "\t\t\t\t{\"lr\": 0.2, \"lr_scale\": 2}\n",
    "\t\t\t]\n",
    "\n",
    "    class DummyArgs:\n",
    "        def __init__(self):\n",
    "            self.warmup_epochs = 5\n",
    "            self.epochs = 100\n",
    "            self.lr = 0.1\n",
    "            self.min_lr = 0.001\n",
    "\n",
    "    optimizer = DummyOptimizer()\n",
    "    args = DummyArgs()\n",
    "\n",
    "    # Test during warmup\n",
    "    lr = adjust_learning_rate(optimizer, 2, args)\n",
    "    test_eq(lr, 0.04)\n",
    "    test_eq(optimizer.param_groups[0][\"lr\"], 0.04)\n",
    "    test_eq(optimizer.param_groups[1][\"lr\"], 0.08)\n",
    "\n",
    "    # Test after warmup\n",
    "    lr = adjust_learning_rate(optimizer, 50, args)\n",
    "    expected_lr = 0.001 + (0.1 - 0.001) * 0.5 * (1 + math.cos(math.pi * 45 / 95))\n",
    "    test_close(lr, expected_lr, eps=1e-6)\n",
    "    test_close(optimizer.param_groups[0][\"lr\"], expected_lr, eps=1e-6)\n",
    "    test_close(optimizer.param_groups[1][\"lr\"], expected_lr * 2, eps=1e-6)\n",
    "    print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_adjust_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_one_epoch(\n",
    "\t            model: torch.nn.Module,\n",
    "                data_loader: Iterable, \n",
    "\t\t\t\toptimizer: torch.optim.Optimizer,\n",
    "                device: torch.device, \n",
    "\t\t\t\tepoch: int, \n",
    "\t\t\t\tloss_scaler: NativeScalerWithGradNormCount,\n",
    "                log_writer=None,\n",
    "                args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 20\n",
    "\n",
    "    accum_iter = args.accum_iter\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "\n",
    "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss /= accum_iter\n",
    "        loss_scaler(\n",
    "\t\t\tloss, \n",
    "\t\t\toptimizer, \n",
    "\t\t\tparameters=model.parameters(),\n",
    "            update_grad=(data_iter_step + 1) % accum_iter == 0\n",
    "\t\t)\n",
    "        if (data_iter_step + 1) % accum_iter == 0:\n",
    "        \toptimizer.zero_grad()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=lr)\n",
    "\n",
    "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
    "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "\t\t\tThis calibrates different curves when batch size changes.\n",
    "\t\t\t\"\"\"\n",
    "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
    "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
    "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
    "\n",
    "\n",
    "\t# gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_pretrain.py script will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> engine_pretrain.py needs to be implemented here, and call main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
