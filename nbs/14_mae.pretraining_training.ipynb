{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Training script in MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pretraining Training script in MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mae.pretraining_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import inf\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import segmentation_test.mae.misc as misc\n",
    "import segmentation_test.mae.model_development as models_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import math\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Scaling and Norm Counting: \n",
    "\n",
    "\n",
    "Dealing with mixed precision training, where we use both 16-bit and 32-bit floating-point numbers to speed up computation and reduce memory usage. But this can lead to some tricky numerical issues. Enter the `NativeScalerWithGradNormCount` class!\n",
    "\n",
    "## What's this class all about?\n",
    "\n",
    "This nifty little class is a wrapper around PyTorch's `GradScaler`. It's designed to handle the intricacies of mixed precision training while also giving us some extra goodies like gradient norm calculation.\n",
    "\n",
    "## Let's break it down:\n",
    "\n",
    "1. **Initialization**: We start by creating a `GradScaler` object. This is PyTorch's built-in tool for automatic mixed precision training.\n",
    "\n",
    "2. **The `__call__` method**: This is where the magic happens!\n",
    "   - It scales the loss and performs backpropagation.\n",
    "   - If we're updating gradients, it handles gradient unscaling and clipping.\n",
    "   - It also calculates the gradient norm, which is super useful for monitoring training stability.\n",
    "\n",
    "3. **State management**: The `state_dict` and `load_state_dict` methods allow us to save and load the scaler's state. This is crucial for resuming training from checkpoints.\n",
    "\n",
    "## Why is this so cool?\n",
    "\n",
    "- It seamlessly integrates mixed precision training into our workflow.\n",
    "- It provides gradient clipping out of the box, which helps prevent exploding gradients.\n",
    "- The gradient norm calculation gives us valuable insights into our training process.\n",
    "\n",
    "By using this class, we're not just training our model - we're training it smartly and efficiently. It's like having a personal trainer for your neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_grad_norm_(\n",
    "        parameters, \n",
    "        norm_type: float = 2.0\n",
    "        ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the gradient norm of the given parameters.\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NativeScalerWithGradNormCount:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(\n",
    "\t\tself, \n",
    "\t\tloss, \n",
    "\t\toptimizer, \n",
    "\t\tclip_grad=None, \n",
    "\t\tparameters=None, \n",
    "\t\tcreate_graph=False, \n",
    "\t\tupdate_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adjust_learning_rate(\n",
    "\toptimizer, \n",
    "\tepoch, \n",
    "\targs\n",
    "\t):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < args.warmup_epochs:\n",
    "        lr = args.lr * epoch / args.warmup_epochs \n",
    "    else:\n",
    "        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_adjust_learning_rate():\n",
    "    class DummyOptimizer:\n",
    "        def __init__(self):\n",
    "            self.param_groups = [\n",
    "\t\t\t\t{\"lr\": 0.1}, \n",
    "\t\t\t\t{\"lr\": 0.2, \"lr_scale\": 2}\n",
    "\t\t\t]\n",
    "\n",
    "    class DummyArgs:\n",
    "        def __init__(self):\n",
    "            self.warmup_epochs = 5\n",
    "            self.epochs = 100\n",
    "            self.lr = 0.1\n",
    "            self.min_lr = 0.001\n",
    "\n",
    "    optimizer = DummyOptimizer()\n",
    "    args = DummyArgs()\n",
    "\n",
    "    # Test during warmup\n",
    "    lr = adjust_learning_rate(optimizer, 2, args)\n",
    "    test_eq(lr, 0.04)\n",
    "    test_eq(optimizer.param_groups[0][\"lr\"], 0.04)\n",
    "    test_eq(optimizer.param_groups[1][\"lr\"], 0.08)\n",
    "\n",
    "    # Test after warmup\n",
    "    lr = adjust_learning_rate(optimizer, 50, args)\n",
    "    expected_lr = 0.001 + (0.1 - 0.001) * 0.5 * (1 + math.cos(math.pi * 45 / 95))\n",
    "    test_close(lr, expected_lr, eps=1e-6)\n",
    "    test_close(optimizer.param_groups[0][\"lr\"], expected_lr, eps=1e-6)\n",
    "    test_close(optimizer.param_groups[1][\"lr\"], expected_lr * 2, eps=1e-6)\n",
    "    print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_adjust_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_one_epoch(\n",
    "\t            model: torch.nn.Module,\n",
    "                data_loader: Iterable, \n",
    "\t\t\t\toptimizer: torch.optim.Optimizer,\n",
    "                device: torch.device, \n",
    "\t\t\t\tepoch: int, \n",
    "\t\t\t\tloss_scaler: NativeScalerWithGradNormCount,\n",
    "                log_writer=None,\n",
    "                args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 20\n",
    "\n",
    "    accum_iter = args.accum_iter\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "\n",
    "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss /= accum_iter\n",
    "        loss_scaler(\n",
    "\t\t\tloss, \n",
    "\t\t\toptimizer, \n",
    "\t\t\tparameters=model.parameters(),\n",
    "            update_grad=(data_iter_step + 1) % accum_iter == 0\n",
    "\t\t)\n",
    "        if (data_iter_step + 1) % accum_iter == 0:\n",
    "        \toptimizer.zero_grad()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=lr)\n",
    "\n",
    "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
    "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "\t\t\tThis calibrates different curves when batch size changes.\n",
    "\t\t\t\"\"\"\n",
    "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
    "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
    "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
    "\n",
    "\n",
    "\t# gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_pretrain.py script will be implemented here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> engine_pretrain.py needs to be implemented here, and call main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='191504384' class='' max='191498213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [191504384/191498213 00:28&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageWang dataset downloaded and extracted to: /home/hasan/.fastai/data/imagewang-160\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import untar_data, URLs\n",
    "\n",
    "# Download and extract the ImageWang dataset (a subset of ImageNet)\n",
    "path = untar_data(URLs.IMAGEWANG_160)\n",
    "\n",
    "# Update the DATA_PATH to point to the downloaded dataset\n",
    "DATA_PATH = str(path)\n",
    "\n",
    "print(f\"ImageWang dataset downloaded and extracted to: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/hasan/.fastai/data/imagewang-160/unsup'),Path('/home/hasan/.fastai/data/imagewang-160/train'),Path('/home/hasan/.fastai/data/imagewang-160/val')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(DATA_PATH).ls()\n",
    "# MAE pre-training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_args_():\n",
    "    parser = ArgumentParser(description=\"MAE Pre-training Arguments\")\n",
    "    \n",
    "    # MAE pre-training arguments\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size per GPU (effective batch size is BATCH_SIZE * ACCUM_ITER * # gpus)\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=400, help=\"Number of epochs for training\")\n",
    "    parser.add_argument(\"--accum_iter\", type=int, default=1, help=\"Accumulate gradient iterations (for increasing the effective batch size under memory constraints)\")\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument(\"--model\", type=str, default='mae_vit_base_patch16', help=\"Name of model to train\")\n",
    "    parser.add_argument(\"--input_size\", type=int, default=224, help=\"Images input size\")\n",
    "    parser.add_argument(\"--mask_ratio\", type=float, default=0.75, help=\"Masking ratio (percentage of removed patches)\")\n",
    "    parser.add_argument(\"--norm_pix_loss\", action='store_true', help=\"Use (per-patch) normalized pixels as targets for computing loss\")\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.05, help=\"Weight decay for optimizer\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=None, help=\"Learning rate (absolute lr)\")\n",
    "    parser.add_argument(\"--blr\", type=float, default=1e-3, help=\"Base learning rate: absolute_lr = base_lr * total_batch_size / 256\")\n",
    "    parser.add_argument(\"--min_lr\", type=float, default=0., help=\"Lower lr bound for cyclic schedulers that hit 0\")\n",
    "    parser.add_argument(\"--warmup_epochs\", type=int, default=40, help=\"Epochs to warmup LR\")\n",
    "    \n",
    "    # Dataset parameters\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\" /home/hasan/.fastai/data/imagewang-160\", help=\"Dataset path\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default='./output_dir', help=\"Path where to save, empty for no saving\")\n",
    "    parser.add_argument(\"--log_dir\", type=str, default='./output_dir', help=\"Path where to tensorboard log\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda', help=\"Device to use for training / testing\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Seed for reproducibility\")\n",
    "    parser.add_argument(\"--resume\", type=str, default='', help=\"Resume from checkpoint\")\n",
    "    parser.add_argument(\"--start_epoch\", type=int, default=0, help=\"Starting epoch\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of workers for data loading\")\n",
    "    parser.add_argument(\"--pin_mem\", action='store_true', help=\"Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\")\n",
    "    \n",
    "    # Distributed training parameters\n",
    "    parser.add_argument(\"--world_size\", type=int, default=1, help=\"Number of distributed processes\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training\")\n",
    "    parser.add_argument(\"--dist_on_itp\", action='store_true', help=\"Distributed training on ITP\")\n",
    "    parser.add_argument(\"--dist_url\", type=str, default='env://', help=\"URL used to set up distributed training\")\n",
    "    \n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 400\n",
    "ACCUM_ITER = 1\n",
    "MODEL = 'mae_vit_base_patch16'\n",
    "INPUT_SIZE = 224\n",
    "MASK_RATIO = 0.75\n",
    "NORM_PIX_LOSS = True\n",
    "WEIGHT_DECAY = 0.05\n",
    "LR = None\n",
    "BLR = 1e-3\n",
    "MIN_LR = 0.\n",
    "WARMUP_EPOCHS = 40\n",
    "DATA_PATH = str(path)\n",
    "OUTPUT_DIR = './output_dir'\n",
    "LOG_DIR = './output_dir'\n",
    "DEVICE = 'cuda'\n",
    "SEED = 0\n",
    "RESUME = ''\n",
    "START_EPOCH = 0\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEM = True\n",
    "WORLD_SIZE = 1\n",
    "LOCAL_RANK = -1\n",
    "DIST_ON_ITP = True\n",
    "DIST_URL = 'env://'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleNamespace object to mimic argparse.Namespace\n",
    "args = SimpleNamespace(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    accum_iter=ACCUM_ITER,\n",
    "    model=MODEL,\n",
    "    input_size=INPUT_SIZE,\n",
    "    mask_ratio=MASK_RATIO,\n",
    "    norm_pix_loss=NORM_PIX_LOSS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr=LR,\n",
    "    blr=BLR,\n",
    "    min_lr=MIN_LR,\n",
    "    warmup_epochs=WARMUP_EPOCHS,\n",
    "    data_path=DATA_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    resume=RESUME,\n",
    "    start_epoch=START_EPOCH,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_mem=PIN_MEM,\n",
    "    world_size=WORLD_SIZE,\n",
    "    local_rank=LOCAL_RANK,\n",
    "    dist_on_itp=DIST_ON_ITP,\n",
    "    dist_url=DIST_URL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "misc.init_distributed_mode(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:54:12.740073] job dir: /home/hasan/Schreibtisch/projects/git_data/segmentation_test/nbs\n"
     ]
    }
   ],
   "source": [
    "print('job dir: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the seed for reproducibility\n",
    "seed = args.seed + misc.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple augmentation\n",
    "transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                args.input_size, \n",
    "                scale=(0.2, 1.0), \n",
    "                interpolation=3),  # 3 is bicubic\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.ImageFolder(\n",
    "    os.path.join(\n",
    "        args.data_path, 'train'), \n",
    "        transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 14669\n",
       "    Root location: /home/hasan/.fastai/data/imagewang-160/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:54:17.903333] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1122cae810>\n"
     ]
    }
   ],
   "source": [
    "if True:  # args.distributed:\n",
    "    num_tasks = misc.get_world_size()\n",
    "    global_rank = misc.get_rank()\n",
    "    sampler_train = torch.utils.data.DistributedSampler(\n",
    "        dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "    )\n",
    "    print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_rank == 0 and args.log_dir is not None:\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "    log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "else:\n",
    "    log_writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mae_vit_large_patch16'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# define the model\n",
    "model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(\"Model = %s\" % str(model_without_ddp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "eff_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8125e-06"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.lr is None:  # only base_lr is specified\n",
    "\targs.lr = args.blr * eff_batch_size / 256\n",
    "args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:54:32.810797] base lr: 1.00e-03\n",
      "[08:54:32.810862] actual lr: 7.81e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "print(\"actual lr: %.2e\" % args.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:54:36.804349] accumulate grad iterations: 1\n",
      "[08:54:36.804411] effective batch size: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"accumulate grad iterations: %d\" % args.accum_iter)\n",
    "print(\"effective batch size: %d\" % eff_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n",
    "    model_without_ddp = model.module\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# following timm: set wd as 0 for bias and norm layers\n",
    "# this was found in original repo, but may be old timm version\n",
    "#param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)\n",
    "# it was not working, so changeing param_groups_weight_decay\n",
    "param_groups = optim_factory.param_groups_weight_decay(model_without_ddp, args.weight_decay)\n",
    "param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    param_groups, \n",
    "    lr=args.lr, \n",
    "    betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "NativeScaler = NativeScalerWithGradNormCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:54:46.765071] AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 7.8125e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.95)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 7.8125e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7020/453686777.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)\n",
    "loss_scaler = NativeScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc.load_model(\n",
    "    args=args, \n",
    "    model_without_ddp=model_without_ddp, \n",
    "    optimizer=optimizer, \n",
    "    loss_scaler=loss_scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function call is responsible for loading the model, optimizer, and loss scaler for the training process.\n",
    "It takes in the following arguments:\n",
    "- `args`: The arguments parsed from the command line or configuration file.\n",
    "- `model_without_ddp`: The model instance without DistributedDataParallel (DDP) wrapping.\n",
    "- `optimizer`: The optimizer instance to be used for training.\n",
    "- `loss_scaler`: The loss scaler instance for automatic mixed precision training.\n",
    "The purpose of this function is to prepare the model, optimizer, and loss scaler for the training loop.\n",
    "It ensures that the model is correctly configured for training, including setting the optimizer and loss scaler.\n",
    "This is a crucial step in the training process as it sets up the necessary components for the model to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main():\n",
    "    args = parse_args_()\n",
    "    misc.init_distributed_mode(args)\n",
    "    print('job dir: {}'.format(os.getcwd()))\n",
    "    print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + misc.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # simple augmentation\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    dataset_train = datasets.ImageFolder(os.path.join(args.data_path, 'train'), transform=transform_train)\n",
    "    print(dataset_train)\n",
    "\n",
    "    if True:  # args.distributed:\n",
    "        num_tasks = misc.get_world_size()\n",
    "        global_rank = misc.get_rank()\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "        print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "\n",
    "    if global_rank == 0 and args.log_dir is not None:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "    else:\n",
    "        log_writer = None\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    # define the model\n",
    "    model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    print(\"Model = %s\" % str(model_without_ddp))\n",
    "\n",
    "    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "    \n",
    "    if args.lr is None:  # only base_lr is specified\n",
    "        args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "    print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "    print(\"actual lr: %.2e\" % args.lr)\n",
    "\n",
    "    print(\"accumulate grad iterations: %d\" % args.accum_iter)\n",
    "    print(\"effective batch size: %d\" % eff_batch_size)\n",
    "\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n",
    "        model_without_ddp = model.module\n",
    "    \n",
    "    # following timm: set wd as 0 for bias and norm layers\n",
    "    param_groups = optim_factory.param_groups_weight_decay(\n",
    "        model_without_ddp, \n",
    "        args.weight_decay)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        param_groups, \n",
    "        lr=args.lr, \n",
    "        betas=(0.9, 0.95))\n",
    "    print(optimizer)\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "    misc.load_model(\n",
    "        args=args, \n",
    "        model_without_ddp=model_without_ddp, \n",
    "        optimizer=optimizer, \n",
    "        loss_scaler=loss_scaler)\n",
    "\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            data_loader_train.sampler.set_epoch(epoch)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, data_loader_train,\n",
    "            optimizer, device, epoch, loss_scaler,\n",
    "            log_writer=log_writer, args=args\n",
    "        )\n",
    "        if args.output_dir and (epoch % 20 == 0 or epoch + 1 == args.epochs):\n",
    "            misc.save_model(\n",
    "                args=args, \n",
    "                model=model, \n",
    "                model_without_ddp=model_without_ddp, \n",
    "                optimizer=optimizer,\n",
    "                loss_scaler=loss_scaler, \n",
    "                epoch=epoch)\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        'epoch': epoch,}\n",
    "\n",
    "        # This section of code is responsible for logging the training statistics to a file.\n",
    "        # It checks if an output directory is specified and if the current process is the main process.\n",
    "        # If both conditions are true, it flushes the log writer (if it exists) to ensure all logs are written.\n",
    "        # Then, it appends the current training statistics (log_stats) to a file named \"log.txt\" in the output directory.\n",
    "        # The statistics are written in JSON format followed by a newline character.\n",
    "        if args.output_dir and misc.is_main_process():\n",
    "            if log_writer is not None:\n",
    "                log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('14_mae.pretraining_training.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
