{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating different Unet models in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> different type of `unet model` from tversky focal loss paper will be implemented here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tf_model_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastcore.all import *\n",
    "from fastcore.test import *\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Union, Callable, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.environ.get('CUDA_PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pooling(\n",
    "        inputs,\n",
    "        max_pool_only=False,\n",
    "        both=True,\n",
    "        pool_size=2):\n",
    "    if both:\n",
    "        p1 = tf.keras.layers.MaxPooling2D(\n",
    "            (pool_size, pool_size))(inputs)\n",
    "        p2 = tf.keras.layers.AvgPool2D(\n",
    "            pool_size=(pool_size, pool_size))(inputs)\n",
    "        return tf.keras.layers.concatenate([p1, p2])\n",
    "    elif max_pool_only:\n",
    "        return tf.keras.layers.MaxPooling2D(\n",
    "            (pool_size, pool_size))(inputs)\n",
    "    else:\n",
    "        return tf.keras.layers.AvgPool2D(\n",
    "            pool_size=(pool_size, pool_size))(inputs)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def conv_block(\n",
    "        inputs, \n",
    "        filter_no, \n",
    "        kernel_size, \n",
    "        batch_nm=True, \n",
    "        dropout=True, \n",
    "        drp_rt=0.1,\n",
    "        kernel_initializer='glorot_normal' #option he_normal\n",
    "        ):\n",
    "    'Create a conv block with batch norma and dropout'\n",
    "    c1 = tf.keras.layers.Conv2D(\n",
    "        filter_no,\n",
    "        (kernel_size, kernel_size),\n",
    "\n",
    "        kernel_initializer='glorot_normal',\n",
    "        padding='same',\n",
    "        activation=None)(inputs)\n",
    "    if batch_nm:\n",
    "        c1 = tf.keras.layers.BatchNormalization()(c1)\n",
    "    c1 = tf.keras.layers.Activation('relu')(c1)\n",
    "    if dropout:\n",
    "        c1 = tf.keras.layers.Dropout(drp_rt)(c1)\n",
    "    return c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 11:35:12.108532: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.140230: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.140500: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.142665: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.142851: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.143017: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.221872: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.222060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.222213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-25 11:35:12.222329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8830 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:0a:00.0, compute capability: 7.5\n",
      "2023-11-25 11:35:12.479654: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n"
     ]
    }
   ],
   "source": [
    "input_size = tf.random.normal((1, 1052, 1632, 1))\n",
    "out_ = conv_block(input_size, 32, 3, batch_nm=False, dropout=False)\n",
    "test_eq([1,1052, 1632, 32], out_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def double_conv(\n",
    "        inputs, \n",
    "        filter_no, \n",
    "        kernel_size, \n",
    "        batch_nm=True, \n",
    "        dropout=True, \n",
    "        drp_rt=0.1,\n",
    "        kernel_initializer='glorot_normal' #option he_normal\n",
    "        ):\n",
    "    'Create double conv block with batch norma and dropout'\n",
    "\n",
    "    c1 = tf.keras.layers.Conv2D(\n",
    "        filter_no,\n",
    "        (kernel_size, kernel_size),\n",
    "\n",
    "        kernel_initializer='glorot_normal',\n",
    "        padding='same',\n",
    "        activation=None)(inputs)\n",
    "    if batch_nm:\n",
    "        c1 = tf.keras.layers.BatchNormalization()(c1)\n",
    "    c1 = tf.keras.layers.Activation('relu')(c1)\n",
    "    if dropout:\n",
    "        c1 = tf.keras.layers.Dropout(drp_rt)(c1)\n",
    "    c2 = tf.keras.layers.Conv2D(\n",
    "        filter_no,\n",
    "        (kernel_size, kernel_size),\n",
    "\n",
    "        kernel_initializer='glorot_normal',\n",
    "        padding='same',\n",
    "        activation=None)(c1)\n",
    "    if batch_nm:\n",
    "        c2 = tf.keras.layers.BatchNormalization()(c2)\n",
    "    c2 = tf.keras.layers.Activation('relu')(c2)\n",
    "    if dropout:\n",
    "        c2 = tf.keras.layers.Dropout(drp_rt)(c2)\n",
    "    return c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 526, 816, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_c = double_conv(inputs=input_size, filter_no=32, kernel_size=3, batch_nm=True, dropout=True, drp_rt=0.1)\n",
    "out_p = pooling(out_c, both=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([1,1052, 1632, 32], out_c.shape)\n",
    "test_eq([1,526, 816, 64], out_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encoder_block(\n",
    "        input_size:tf.Tensor,\n",
    "        filter_size:List[int]=[32,64,128,256,512, 1024],\n",
    "        kernel_initializer:str='glorot_normal' #option he_normal\n",
    "                 ):\n",
    "    'Create Encoder block for Unet'\n",
    "\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    outputs = []\n",
    "    p = inputs \n",
    "    \n",
    "\n",
    "    for i in filter_size:\n",
    "        c = double_conv(\n",
    "            inputs=p, \n",
    "            filter_no=i, \n",
    "            kernel_size=3, \n",
    "            batch_nm=True, \n",
    "            dropout=True, \n",
    "            drp_rt=0.1,\n",
    "            kernel_initializer='glorot_normal' #option he_normal\n",
    "            )\n",
    "        p = pooling(c, both=True)\n",
    "        outputs.append(p)\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w, c = 1052, 1632, 1\n",
    "input_img = tf.random.normal((1, h, w, c))\n",
    "inputs,enc_outputs = encoder_block(\n",
    "    input_size=(h, w, c),\n",
    "    filter_size=[32,64,128,256,512, 1024],\n",
    "    kernel_initializer='glorot_normal' #option he_normal\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq((None, 1052, 1632, 1), inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_shapes = [\n",
    "    (None, 526, 816, 64),\n",
    "    (None, 263, 408, 128),\n",
    "    (None, 131, 204, 256),\n",
    "    (None, 65, 102, 512),\n",
    "    (None, 32, 51, 1024),\n",
    "    (None, 16, 25, 2048),\n",
    "]\n",
    "\n",
    "for (expected, actual) in zip(expected_shapes, enc_outputs):\n",
    "    test_eq(expected, actual.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 16, 25, 2048) dtype=float32 (created by layer 'concatenate_19')>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1024, 512, 256, 128, 64, 32]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_list=[32,64,128,256,512, 1024]\n",
    "decoder_fl = filter_list[::-1]\n",
    "decoder_fl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decoder_block(\n",
    "    filter_list:List[int],\n",
    "    encoder_outputs:List[tf.Tensor],\n",
    "   ):\n",
    "    'Create a decoder block for unet'\n",
    "\n",
    "    filter_list = filter_list[::-1]\n",
    "    decoder_input = encoder_outputs[-1]\n",
    "\n",
    "    for i in range(1, len(encoder_outputs)):\n",
    "        filter_no = filter_list[i-1]\n",
    "        up_ = tf.keras.layers.Conv2DTranspose(\n",
    "                                            filter_no,\n",
    "                                            (2,2),\n",
    "                                            strides=(2,2),\n",
    "                                            padding='same',\n",
    "                                            )(decoder_input)\n",
    "        \n",
    "        # Handle the shape mismatch (if any)\n",
    "        if up_.shape[1:3] != encoder_outputs[-(i + 1)].shape[1:3]:\n",
    "            # Adjust padding, cropping, or use other techniques as needed\n",
    "            print(f'up_.shape: {up_.shape}')\n",
    "            pass\n",
    "\n",
    "        # Concatenate the upsampled feature map with the corresponding encoder feature map\n",
    "        up_ = tf.keras.layers.concatenate([up_, encoder_outputs[-(i + 1)]])\n",
    "\n",
    "        c_= double_conv(\n",
    "            inputs=up_, \n",
    "            filter_no=filter_no, \n",
    "            kernel_size=3, \n",
    "            batch_nm=True, \n",
    "            dropout=True, \n",
    "            drp_rt=0.1,\n",
    "            kernel_initializer='glorot_normal' #option he_normal\n",
    "            )\n",
    "        decoder_input = c_\n",
    "    return decoder_input\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
