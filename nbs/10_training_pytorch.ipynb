{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training, loss functions and metris will be developed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pytorch_training_and_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#import torch\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#from torch.optim.swa_utils import SWALR\n",
    "#from torch import nn\n",
    "#from torchvision.transforms.functional import to_pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Callable\n",
    "import inspect\n",
    "from statistics import mean\n",
    "from cv_tools.core import *\n",
    "from cv_tools.imports import *\n",
    "import monai\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "from fastcore.all import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        return F_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def iou_metric(preds, labels, threshold=0.5):\n",
    "    # Convert predictions to binary format\n",
    "    preds = (preds > threshold).float().squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    \n",
    "    # Ensure labels are binary\n",
    "    labels = labels.float().squeeze(1)  # Similarly squeeze if necessary\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = (preds * labels).sum((1, 2))  # Logical AND\n",
    "    union = ((preds + labels) > 0).float().sum((1, 2))  # Logical OR\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)  # Smoothing to avoid 0/0\n",
    "\n",
    "    return iou.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def false_positive_negative(outputs, labels):\n",
    "    FP = ((outputs == 1) & (labels == 0)).float().sum()\n",
    "    FN = ((outputs == 0) & (labels == 1)).float().sum()\n",
    "    return FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_test.pytorch_model_development import *\n",
    "from segmentation_test.dataloader_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = UNet(in_channels=1, out_channels=1,max_pool_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1642) [Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_147_p_7.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_51_p_10.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_164_p_1.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_122_p_4.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_134_p_1.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_80_p_2.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_77_p_8.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_47_p_7.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_78_p_10.png'),Path('/home/hasan/workspace/data/microscopy_data/patch_images/img_83_p_2.png')...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_path = Path(r'/home/hasan/workspace/data/microscopy_data/patch_images')\n",
    "msk_path = Path(r'/home/hasan/workspace/data/microscopy_data/patch_masks')\n",
    "im_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of images found = 1642\n",
      " training dataset length = 1313 and validation dataset length=  329\n",
      "torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = create_pytorch_dataloader(\n",
    "    split_type='random',\n",
    "    split_per=0.8,\n",
    "    batch_size=2,\n",
    "    image_path=im_path,\n",
    "    mask_path=msk_path,\n",
    "    transforms=None,\n",
    "    num_workers=0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, optimizer, and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = FocalLoss()\n",
    "#| export\n",
    "total_epochs = 10\n",
    "# Define LR scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs, eta_min=0)\n",
    "warmup_epochs = 0.1 * total_epochs\n",
    "initial_lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, msk = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 256, 256]), torch.Size([2, 1, 256, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, msk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3454eec44e0042b9b2277f93794598a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/657 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "focal_loss = FocalLoss()\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_iou_sum, train_fp_sum, train_fn_sum = 0.0, 0.0, 0.0, 0.0\n",
    "    train_total = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader),desc=f'Epoch {epoch+1}/{total_epochs}', unit=\"batch\") as pbar:\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = focal_loss(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            iou_score = iou_metric(outputs, target)\n",
    "            train_iou_sum += iou_score.item()\n",
    "            fp, fn = false_positive_negative(outputs, target)\n",
    "            train_fp_sum += fp.item()\n",
    "            train_fn_sum += fn.item()\n",
    "            train_total += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                              {'loss': loss.item(), 'iou': iou_score.item(), 'fp': fp.item(), 'fn': fn.item()}\n",
    "                              )\n",
    "            pbar.update()\n",
    "    \n",
    "    train_loss /= train_total\n",
    "    train_iou = train_iou_sum / train_total\n",
    "    train_fp = train_fp_sum / train_total\n",
    "    train_fn = train_fn_sum / train_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_iou_sum, val_fp_sum, val_fn_sum = 0.0, 0.0, 0.0, 0.0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = FocalLoss(outputs, target)\n",
    "\n",
    "            # Update metrics\n",
    "            val_loss += loss.item()\n",
    "            iou_score = iou_metric(outputs, target)\n",
    "            val_iou_sum += iou_score.item()\n",
    "            fp, fn = false_positive_negative(outputs, target)\n",
    "            val_fp_sum += fp.item()\n",
    "            val_fn_sum += fn.item()\n",
    "            val_total += 1\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_iou = val_iou_sum / val_total\n",
    "    val_fp = val_fp_sum / val_total\n",
    "    val_fn = val_fn_sum / val_total\n",
    "\n",
    "    # Adjust learning rate after warm-up\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = initial_lr * (epoch / warmup_epochs)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save model with metrics in the filename\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch+1}_valiou_{val_iou:.2f}_fp_{val_fp:.0f}_fn_{val_fn:.0f}.pth')\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}/{total_epochs} - Train Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, FP: {train_fp}, FN: {train_fn}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, FP: {val_fp}, FN: {val_fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_front(\n",
    "        model:nn.Module, \n",
    "        dataloader:DataLoader,\n",
    "        loss_fn:monai.losses.dice, \n",
    "        metrics:List[Callable]=[],\n",
    "        device:str='cpu',\n",
    "        threshold:float=0.5\n",
    "        ):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    val_losses = []\n",
    "\n",
    "    epoch_metrics = {metric.__name__ if inspect.isfunction(metric) else metric.__class__.__name__: [] for metric in metrics}\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            img, msk = batch\n",
    "            img = img.to(device)\n",
    "            msk = msk.to(device)\n",
    "            ground_truth_mask = msk\n",
    "            outputs = model(img)\n",
    "            #predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            #ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            #loss = loss_fn(outputs.squeeze(1), ground_truth_mask.stqueeze(1))\n",
    "            loss = loss_fn(outputs, ground_truth_mask)\n",
    "            val_losses.append(loss.item())\n",
    "            for metric in metrics:\n",
    "                metric_value = metric(\n",
    "                    outputs>=threshold, \n",
    "                    ground_truth_mask>=threshold\n",
    "                    )\n",
    "                if inspect.isfunction(metric):\n",
    "                    epoch_metrics[metric.__name__].append(\n",
    "                        metric_value)\n",
    "                else:\n",
    "                    epoch_metrics[metric.__class__.__name__].append(\n",
    "                        metric_value)\n",
    "        \n",
    "        for metric_name, values in epoch_metrics.items():\n",
    "            print(f'Mean Validation {metric_name}: {mean(values)}')\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return mean(val_losses), np.mean(list(epoch_metrics.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_and_validate_front(\n",
    "        model: nn.Module,  # Model to train\n",
    "        num_epochs: int,  # Number of epochs to train for\n",
    "        optimizer: torch.optim.Optimizer, # Optimizer to use\n",
    "        scheduler: torch.optim.lr_scheduler._LRScheduler,  # Learning rate scheduler\n",
    "        train_dataloader: DataLoader,  # DataLoader for training data\n",
    "        val_dataloader: DataLoader, # DataLoader for validation data\n",
    "        loss_fn: nn.Module,  # Loss function used for training\n",
    "        metrics: List[Callable],  # List of metrics to compute\n",
    "        threshold: float = 0.5,  # Threshold for binarizing the predictions\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu', # Device to train on\n",
    "        save_path: str = './models', # Path to save the model after training\n",
    "        model_fn: str = 'model', # Base filename of model\n",
    "        ):\n",
    "    \"\"\"Train and validate a model with the given parameters.\"\"\"\n",
    "    \n",
    "    model = model.float().to(device)\n",
    "    best_val_metric = float('-inf')\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_metrics = {metric.__name__ if callable(metric) else type(metric).__name__: [] for metric in metrics}\n",
    "        print(f'###################')\n",
    "        print(f'Started epoch = {epoch + 1}')\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            img, msk = batch\n",
    "            img, msk = img.float().to(device), msk.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img)\n",
    "            loss = loss_fn(outputs, msk)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_masks = (outputs > threshold).float()\n",
    "                for metric in metrics:\n",
    "                    metric_value = metric(predicted_masks, msk)\n",
    "                    metric_name = metric.__name__ if callable(metric) else type(metric).__name__\n",
    "                    epoch_metrics[metric_name].append(metric_value.item() if isinstance(metric_value, torch.Tensor) else metric_value)\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_metrics = validate_front(\n",
    "            model=model, \n",
    "            dataloader=val_dataloader, \n",
    "            loss_fn=loss_fn, \n",
    "            metrics=metrics,\n",
    "            threshold=threshold,\n",
    "            device=device)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch: {epoch + 1}')\n",
    "        print(f'Mean Training Loss: {np.mean(epoch_losses):.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        for metric_name, values in epoch_metrics.items():\n",
    "            train_metric = np.mean(values)\n",
    "            val_metric = val_metrics[metric_name]\n",
    "            print(f'Train {metric_name}: {train_metric:.4f}, Validation {metric_name}: {val_metric:.4f}')\n",
    "\n",
    "        # Check if the current validation metric is better than the best so far\n",
    "        current_val_metric = val_metrics[metrics[0].__name__ if callable(metrics[0]) else type(metrics[0]).__name__]\n",
    "        if current_val_metric > best_val_metric:\n",
    "            best_val_metric = current_val_metric\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            # Save the model if it's the best so far\n",
    "            save_dir = Path(save_path)\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model_save_path = save_dir / f\"{model_fn}_best_val_{best_val_metric:.4f}_epoch_{best_epoch}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': best_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_metric': best_val_metric,\n",
    "            }, model_save_path)\n",
    "            print(f'New best model saved to {model_save_path}')\n",
    "\n",
    "        print(f'###################')\n",
    "        print(f'Ended epoch = {epoch + 1}')\n",
    "\n",
    "    print(f'Training completed. Best validation metric: {best_val_metric:.4f} at epoch {best_epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_segmentation_model(\n",
    "                            train_loader,\n",
    "                            val_loader,\n",
    "                            model,\n",
    "                            optimizer,\n",
    "                            scheduler, \n",
    "                            loss_fn, \n",
    "                            model_save_path, \n",
    "                            warmup_epochs, \n",
    "                            total_epochs, \n",
    "                            initial_lr, \n",
    "                            dtype=torch.float,\n",
    "                            device='cuda'):\n",
    "    # Set the device for training\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device=device, dtype=dtype)\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_loss, train_iou_sum, train_fp_sum, train_fn_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        train_total = 0\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{total_epochs}', unit=\"batch\") as pbar:\n",
    "            for data, target in train_loader:\n",
    "                # Move data and target to the specified device and data type\n",
    "                data = data.to(device=device, dtype=dtype)\n",
    "                target = target.to(device=device, dtype=dtype)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = loss_fn(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update metrics\n",
    "                train_loss += loss.item()\n",
    "                iou_score = iou_metric(outputs, target)\n",
    "                train_iou_sum += iou_score.item()\n",
    "                fp, fn = false_positive_negative(outputs, target)\n",
    "                train_fp_sum += fp.item()\n",
    "                train_fn_sum += fn.item()\n",
    "                train_total += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({'loss': loss.item(), 'iou': iou_score.item(), 'fp': fp.item(), 'fn': fn.item()})\n",
    "                pbar.update()\n",
    "\n",
    "        # ... Rest of the training loop as before ...\n",
    "        train_loss /= train_total\n",
    "        train_iou = train_iou_sum / train_total\n",
    "        train_fp = train_fp_sum / train_total\n",
    "        train_fn = train_fn_sum / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_iou_sum, val_fp_sum, val_fn_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "\n",
    "                data = data.to(device=device, dtype=dtype)\n",
    "                target = target.to(device=device, dtype=dtype)\n",
    "\n",
    "                outputs = model(data)\n",
    "                loss = loss_fn(outputs, target)\n",
    "\n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                iou_score = iou_metric(outputs, target)\n",
    "                val_iou_sum += iou_score.item()\n",
    "                fp, fn = false_positive_negative(outputs, target)\n",
    "                val_fp_sum += fp.item()\n",
    "                val_fn_sum += fn.item()\n",
    "                val_total += 1\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_iou = val_iou_sum / val_total\n",
    "        val_fp = val_fp_sum / val_total\n",
    "        val_fn = val_fn_sum / val_total\n",
    "\n",
    "        # Adjust learning rate after warm-up\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = initial_lr * (epoch / warmup_epochs)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "          # Save model\n",
    "        torch.save(model.state_dict(), f'{model_save_path}/model_epoch_{epoch+1}_valiou_{val_iou:.2f}_fp_{val_fp:.0f}_fn_{val_fn:.0f}.pth')\n",
    "\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{total_epochs} - Train Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, FP: {train_fp}, FN: {train_fn}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, FP: {val_fp}, FN: {val_fn}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('10_training_pytorch.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
