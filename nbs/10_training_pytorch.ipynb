{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training, loss functions and metris will be developed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pytorch_training_and_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#import torch\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#from torch.optim.swa_utils import SWALR\n",
    "#from torch import nn\n",
    "#from torchvision.transforms.functional import to_pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        return F_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def iou_metric(preds, labels):\n",
    "    preds = preds.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    intersection = (preds & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (preds | labels).float().sum((1, 2))  # Will be zero if both are 0\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)  # We smooth our devision to avoid 0/0\n",
    "    return iou.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def false_positive_negative(outputs, labels):\n",
    "    FP = ((outputs == 1) & (labels == 0)).float().sum()\n",
    "    FN = ((outputs == 0) & (labels == 1)).float().sum()\n",
    "    return FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, optimizer, and loss\n",
    "model = AttentionUnet(num_classes=1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = FocalLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "total_epochs = 10\n",
    "# Define LR scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs, eta_min=0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_iou_sum, train_fp_sum, train_fn_sum = 0.0, 0.0, 0.0, 0.0\n",
    "    train_total = 0\n",
    "\n",
    "    with tqdm(total=len(train_loader),desc=f'Epoch {epoch+1}/{total_epochs}', unit=\"batch\") as pbar:\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = FocalLoss(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            iou_score = iou(outputs, target)\n",
    "            train_iou_sum += iou_score.item()\n",
    "            fp, fn = false_positive_negative(outputs, target)\n",
    "            train_fp_sum += fp.item()\n",
    "            train_fn_sum += fn.item()\n",
    "            train_total += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss.item(), 'iou': iou_score.item()})\n",
    "            pbar.update()\n",
    "    \n",
    "    train_loss /= train_total\n",
    "    train_iou = train_iou_sum / train_total\n",
    "    train_fp = train_fp_sum / train_total\n",
    "    train_fn = train_fn_sum / train_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_iou_sum, val_fp_sum, val_fn_sum = 0.0, 0.0, 0.0, 0.0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = FocalLoss(outputs, target)\n",
    "\n",
    "            # Update metrics\n",
    "            val_loss += loss.item()\n",
    "            iou_score = iou(outputs, target)\n",
    "            val_iou_sum += iou_score.item()\n",
    "            fp, fn = false_positive_negative(outputs, target)\n",
    "            val_fp_sum += fp.item()\n",
    "            val_fn_sum += fn.item()\n",
    "            val_total += 1\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_iou = val_iou_sum / val_total\n",
    "    val_fp = val_fp_sum / val_total\n",
    "    val_fn = val_fn_sum / val_total\n",
    "\n",
    "\n",
    "\n",
    "    # Adjust learning rate after warm-up\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = initial_lr * (epoch / warmup_epochs)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    # Save model with metrics in the filename\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch+1}_valiou_{val_iou:.2f}_fp_{val_fp:.0f}_fn_{val_fn:.0f}.pth')\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}/{total_epochs} - Train Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, FP: {train_fp}, FN: {train_fn}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, FP: {val_fp}, FN: {val_fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True, val_range=None):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.val_range = val_range\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 - pytorch_ssim.SSIM()#ssim(img1, img2, window_size=self.window_size, size_average=self.size_average, val_range=self.val_range)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import nbdev; nbdev.nbdev_export('10_training.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
