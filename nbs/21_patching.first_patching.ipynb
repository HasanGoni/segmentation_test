{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp patching.first_patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patchify Image\n",
    "> Create first patching of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f17969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cv_tools.core import *\n",
    "from cv_tools.imports import *\n",
    "from cv_tools.data_processing.smb_tools import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1, 1, 1152, 1632)\n",
    "H, W = img.shape[2:]\n",
    "tile_size = 256\n",
    "overlap = 32\n",
    "h_steps = max(1, (H - overlap) // (tile_size - overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TileProcessor(nn.Module):\n",
    "    def __init__(self, tile_size=256, overlap=32):\n",
    "        super().__init__()\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Split image into overlapping tiles and return tiles with their positions\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        tile_size = self.tile_size\n",
    "        overlap = self.overlap\n",
    "        \n",
    "        tiles = []\n",
    "        positions = []\n",
    "        \n",
    "        # Calculate steps with overlap\n",
    "        h_steps = max(1, (H - overlap) // (tile_size - overlap))\n",
    "        w_steps = max(1, (W - overlap) // (tile_size - overlap))\n",
    "        \n",
    "        for i in range(h_steps):\n",
    "            h_start = min(i * (tile_size - overlap), H - tile_size)\n",
    "            for j in range(w_steps):\n",
    "                w_start = min(j * (tile_size - overlap), W - tile_size)\n",
    "                \n",
    "                # Extract tile\n",
    "                tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "                tiles.append(tile)\n",
    "                \n",
    "                # Store position\n",
    "                positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        # If we have any space left at the bottom or right, add extra tiles\n",
    "        if h_steps * (tile_size - overlap) + overlap < H:\n",
    "            h_start = H - tile_size\n",
    "            for j in range(w_steps):\n",
    "                w_start = min(j * (tile_size - overlap), W - tile_size)\n",
    "                tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "                tiles.append(tile)\n",
    "                positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        if w_steps * (tile_size - overlap) + overlap < W:\n",
    "            w_start = W - tile_size\n",
    "            for i in range(h_steps):\n",
    "                h_start = min(i * (tile_size - overlap), H - tile_size)\n",
    "                tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "                tiles.append(tile)\n",
    "                positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        # Also add the corner if needed\n",
    "        if (h_steps * (tile_size - overlap) + overlap < H and \n",
    "            w_steps * (tile_size - overlap) + overlap < W):\n",
    "            h_start = H - tile_size\n",
    "            w_start = W - tile_size\n",
    "            tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "            tiles.append(tile)\n",
    "            positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        # Stack tiles into a batch\n",
    "        tiles = torch.cat(tiles, dim=0)\n",
    "        positions_tensor = torch.tensor(positions, device=x.device)\n",
    "        \n",
    "        return tiles, positions_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdd8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlap: 76\n",
      "tile_size: 256\n",
      "H: 1152\n",
      "W: 1632\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "img = torch.randn(1, 1, 1152, 1632)\n",
    "H, W = img.shape[2:]\n",
    "tile_size = 256\n",
    "overlap_percentage = 0.3\n",
    "overlap = int(tile_size * overlap_percentage)\n",
    "print(f'overlap: {overlap}')\n",
    "print(f'tile_size: {tile_size}')\n",
    "print(f'H: {H}')\n",
    "print(f'W: {W}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7dba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLAHELayer(nn.Module):\n",
    "    \"\"\"CLAHE implementation using PyTorch operations (differentiable approximation)\"\"\"\n",
    "    def __init__(self, clip_limit=4.0, grid_size=(8, 8)):\n",
    "        super().__init__()\n",
    "        self.clip_limit = clip_limit\n",
    "        self.grid_size = grid_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # This is a simplified differentiable approximation of CLAHE\n",
    "        # Actual implementation would be more complex but using standard ops\n",
    "        \n",
    "        # Local contrast normalization (approximation of CLAHE)\n",
    "        B, C, H, W = x.shape\n",
    "        kh, kw = H//self.grid_size[0], W//self.grid_size[1]\n",
    "        unfold = nn.Unfold(kernel_size=(kh, kw), stride=(kh, kw))\n",
    "        fold = nn.Fold(output_size=(H, W), kernel_size=(kh, kw), stride=(kh, kw))\n",
    "        \n",
    "        # Use local normalization as an approximation\n",
    "        unfold = nn.Unfold(\n",
    "            kernel_size=(H // self.grid_size[0], W // self.grid_size[1]), \n",
    "            stride=(H // self.grid_size[0], W // self.grid_size[1])\n",
    "        )\n",
    "        \n",
    "        # Reshape and normalize locally\n",
    "        patches = unfold(x)\n",
    "        patches = patches.view(B, C, kh*kw, -1)\n",
    "        mean = patches.mean(dim=2, keepdim=True)\n",
    "        std = patches.std(dim=2, keepdim=True, unbiased=False) + 1e-6\n",
    "\n",
    "        global_std = std.mean()\n",
    "        std = torch.clamp(std, max=self.clip_limit * global_std)\n",
    "\n",
    "        normed = (patches - mean) / std\n",
    "        normed = normed.view(B, C*kh*kw, -1)\n",
    "        out = fold(normed)\n",
    "\n",
    "        ones = torch.ones_like(x)\n",
    "        norm_map = fold(unfold(ones))\n",
    "        out = out / (norm_map + 1e-6)\n",
    "        \n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222142cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_gaussian_kernel(kernel_size=5, sigma=1.0, channels=1):\n",
    "    \"\"\"Returns a 2D Gaussian kernel for depthwise convolution.\"\"\"\n",
    "    # Create 1D kernel\n",
    "    x = torch.arange(kernel_size) - kernel_size // 2\n",
    "    gauss = torch.exp(-x**2 / (2 * sigma**2))\n",
    "    gauss = gauss / gauss.sum()\n",
    "    # Outer product to get 2D kernel\n",
    "    kernel2d = gauss[:, None] * gauss[None, :]\n",
    "    kernel2d = kernel2d / kernel2d.sum()\n",
    "    # Expand to (channels, 1, k, k) for depthwise conv\n",
    "    kernel = kernel2d.expand(channels, 1, kernel_size, kernel_size).contiguous()\n",
    "    return kernel\n",
    "\n",
    "class GaussianDenoiseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ONNX-compatible Gaussian denoising layer.\n",
    "    Args:\n",
    "        kernel_size: Size of the Gaussian kernel (odd integer).\n",
    "        sigma: Standard deviation of the Gaussian.\n",
    "        channels: Number of input channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=5, sigma=1.0, channels=1):\n",
    "        super().__init__()\n",
    "        kernel = get_gaussian_kernel(kernel_size, sigma, channels)\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        return F.conv2d(x, self.weight, padding=self.padding, groups=self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BiasFieldCorrectionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ONNX-compatible bias field correction layer.\n",
    "    Learns a smooth multiplicative mask to correct intensity inhomogeneity.\n",
    "    Args:\n",
    "        in_channels: Number of input channels.\n",
    "        reduction: Downsampling factor for the bias field (higher = smoother).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction = reduction\n",
    "        # Learnable low-res bias field\n",
    "        self.bias_field = nn.Parameter(torch.ones(1, in_channels, reduction, reduction))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # Upsample bias field to input size\n",
    "        bias = F.interpolate(self.bias_field, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        # Multiplicative correction, clamp to avoid division by zero\n",
    "        corrected = x / (bias + 1e-6)\n",
    "        return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4384ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing pipeline\n",
    "# 1. Denoising - Removes random noise from images\n",
    "#    Example: Noisy image [⚫⚪⚫⚪] → Denoised [⚫⚫⚫⚫]\n",
    "#\n",
    "# 2. Bias field correction - Fixes uneven illumination across the image\n",
    "#    Example: Darker on left [⚫⚫⚪⚪] → Balanced [⚫⚫⚫⚫]\n",
    "# 3. Normalization - Scales pixel values to a range of [0, 1]\n",
    "#    Example: Pixel values [0, 255] → Normalized [0, 1]\n",
    "# 4. CLAHE (Contrast Limited Adaptive Histogram Equalization) - Enhances local contrast\n",
    "#    Example: Low contrast [⚫⚫⚪⚪] → Better contrast [⚫⚪⚫⚪]\n",
    "# 5. Edge enhancement - Highlights boundaries between structures\n",
    "#    Example: Blurry edge [⚫⚫⚪⚪] → Sharp edge [⚫⚪⚪⚪]\n",
    "# 6. Tile - Divides large image into smaller overlapping patches\n",
    "#    Example: Large image [⚫⚫⚫⚫|⚪⚪⚪⚪] → Tiles [⚫⚫⚫⚫], [⚪⚪⚪⚪]\n",
    "# 7. Segmentation - Identifies and labels regions of interest\n",
    "#    Example: Raw image [⚫⚫⚪⚪] → Segmented [1️⃣1️⃣2️⃣2️⃣]\n",
    "# 8. Merge tiles - Combines segmented tiles back into full image\n",
    "#    Example: Tiles [1️⃣1️⃣], [2️⃣2️⃣] → Merged [1️⃣1️⃣2️⃣2️⃣]\n",
    "#\n",
    "# 9. Post-processing - Refines segmentation with morphological operations\n",
    "#    Example: Rough mask [1️⃣⚪1️⃣1️⃣] → Cleaned [1️⃣1️⃣1️⃣1️⃣]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MorphologyLayer(nn.Module):\n",
    "    \"\"\"Differentiable approximation of morphological operations\"\"\"\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Create dilation and erosion kernels\n",
    "        self.dilation_kernel = nn.Parameter(\n",
    "            torch.ones(1, 1, kernel_size, kernel_size), \n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        self.erosion_kernel = nn.Parameter(\n",
    "            torch.ones(1, 1, kernel_size, kernel_size), \n",
    "            requires_grad=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Closing operation: dilation followed by erosion\n",
    "        # Dilation (max pooling is an approximation of dilation)\n",
    "        dilated = F.max_pool2d(\n",
    "            x, \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=1, \n",
    "            padding=self.kernel_size//2\n",
    "        )\n",
    "        \n",
    "        # Erosion (negative of max pooling on negative image is an approximation)\n",
    "        eroded = -F.max_pool2d(\n",
    "            -dilated, \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=1, \n",
    "            padding=self.kernel_size//2\n",
    "        )\n",
    "        \n",
    "        return eroded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4545140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETPinSegmentationPipeline(nn.Module):\n",
    "    def __init__(self, in_channels=1, tile_size=256, overlap=32):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # 1. Preprocessing\n",
    "        self.normalize = nn.BatchNorm2d(in_channels, affine=False)\n",
    "        self.clahe = CLAHELayer(clip_limit=4.0, grid_size=(8, 8))\n",
    "        \n",
    "        # Edge enhancement through a learnable filter\n",
    "        self.edge_enhance = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.edge_enhance.weight.data = torch.FloatTensor([\n",
    "            [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]\n",
    "        ]).unsqueeze(0).unsqueeze(0) * 0.1\n",
    "        \n",
    "        # 2. Tiling mechanism\n",
    "        self.tile_processor = TileProcessor(tile_size=tile_size, overlap=overlap)\n",
    "        \n",
    "        # 3. Segmentation model\n",
    "        self.encoder = EfficientNetEncoder(in_channels=in_channels)\n",
    "        self.decoder = UNetDecoder()\n",
    "        \n",
    "        # 4. Post-processing\n",
    "        self.tile_merger = TileMerger()\n",
    "        self.boundary_refinement = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.refine_activation = nn.ReLU(inplace=True)\n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=3, padding=1)\n",
    "        self.morphology = MorphologyLayer(kernel_size=5)\n",
    "        self.final_activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    " @patch\n",
    " def forward(self:ETPinSegmentationPipeline, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # 1. Preprocessing\n",
    "        x = self.normalize(x)\n",
    "        x = self.clahe(x)\n",
    "        edge = self.edge_enhance(x)\n",
    "        x = x + edge  # Residual edge enhancement\n",
    "        \n",
    "        # 2. Generate tiles\n",
    "        tiles, positions = self.tile_processor(x)\n",
    "        \n",
    "        # 3. Process each tile (need to handle batch processing)\n",
    "        results = []\n",
    "        # Process in smaller batches to avoid memory issues\n",
    "        batch_size = 4  # Adjust based on GPU memory\n",
    "        for i in range(0, tiles.shape[0], batch_size):\n",
    "            batch_tiles = tiles[i:i+batch_size]\n",
    "            \n",
    "            # Extract features\n",
    "            features = self.encoder(batch_tiles)\n",
    "            \n",
    "            # Decode features\n",
    "            tile_masks = self.decoder(features)\n",
    "            \n",
    "            results.append(tile_masks)\n",
    "        \n",
    "        # Combine results\n",
    "        all_results = torch.cat(results, dim=0)\n",
    "        \n",
    "        # 4. Merge tiles back to full image\n",
    "        merged_mask = self.tile_merger(all_results, positions, H, W)\n",
    "        \n",
    "        # 5. Post-processing\n",
    "        refined = self.boundary_refinement(merged_mask)\n",
    "        refined = self.refine_activation(refined)\n",
    "        final_mask = self.final_conv(refined)\n",
    "        final_mask = self.morphology(final_mask)  # Apply morphological operations\n",
    "        final_mask = self.final_activation(final_mask)\n",
    "        \n",
    "        return final_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17930cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " img has a min value = 4.740786075592041 and a max value = -5.191678047180176\n",
      " After normalizing img has a max of 4.738675117492676 and min value -5.189996719360352\n",
      "torch.Size([1, 1, 1152, 1632])\n"
     ]
    }
   ],
   "source": [
    "in_channels=1\n",
    "normalize = nn.BatchNorm2d(in_channels, affine=False)\n",
    "print(f' img has a min value = {img.max()} and a max value = {img.min()}')\n",
    "norm_img = normalize(img)\n",
    "print(f' After normalizing img has a max of {norm_img.max()} and min value {norm_img.min()}')\n",
    "print(norm_img.shape)\n",
    "clahe_layer = CLAHELayer(clip_limit=4.0, grid_size=(8, 8))\n",
    "morphology_layer = MorphologyLayer(kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b76b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sobel_filter():\n",
    "    # 3x3 Sobel kernels for x and y\n",
    "    sobel_x = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "    sobel_y = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "    return sobel_x, sobel_y\n",
    "\n",
    "def edge_loss(pred, target):\n",
    "    # pred, target: (B, 1, H, W), values in [0,1]\n",
    "    sobel_x, sobel_y = sobel_filter()\n",
    "    sobel_x, sobel_y = sobel_x.to(pred.device), sobel_y.to(pred.device)\n",
    "    pred_x = F.conv2d(pred, sobel_x, padding=1)\n",
    "    pred_y = F.conv2d(pred, sobel_y, padding=1)\n",
    "    target_x = F.conv2d(target, sobel_x, padding=1)\n",
    "    target_y = F.conv2d(target, sobel_y, padding=1)\n",
    "    pred_grad = torch.sqrt(pred_x**2 + pred_y**2 + 1e-6)\n",
    "    target_grad = torch.sqrt(target_x**2 + target_y**2 + 1e-6)\n",
    "    return F.l1_loss(pred_grad, target_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(pred):\n",
    "    # pred: (B, 1, H, W)\n",
    "    tv_h = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :]).mean()\n",
    "    tv_w = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1]).mean()\n",
    "    return tv_h + tv_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss(pred, target, alpha=1.0, beta=0.1, gamma=0.05):\n",
    "    # pred: (B, 1, H, W), logits or probabilities\n",
    "    # target: (B, 1, H, W), binary mask\n",
    "    seg_loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "    pred_prob = torch.sigmoid(pred)\n",
    "    edge = edge_loss(pred_prob, target)\n",
    "    tv = total_variation_loss(pred_prob)\n",
    "\t# add focal loss + dice loss or only focal loss\n",
    "\t#fl = FocalLoss()\n",
    "    return alpha * seg_loss + beta * edge + gamma * tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265d5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa9425",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'efficientnet_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mefficientnet_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EfficientNet\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'efficientnet_pytorch'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# =============================================\n",
    "# 1. MODEL ARCHITECTURE\n",
    "# =============================================\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return x * psi\n",
    "\n",
    "class EfficientNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        # Load pre-trained EfficientNet-B0\n",
    "        self.efficient_net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Modify first layer to accept grayscale input\n",
    "        self.efficient_net._conv_stem = nn.Conv2d(\n",
    "            in_channels, 32, kernel_size=3, stride=2, bias=False\n",
    "        )\n",
    "        \n",
    "        # Define feature extraction points\n",
    "        self.feature_indices = [3, 5, 9, 16]  # These correspond to specific layers in EfficientNet\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for idx, layer in enumerate(self.efficient_net._blocks):\n",
    "            x = layer(x)\n",
    "            if idx in self.feature_indices:\n",
    "                features.append(x)\n",
    "        \n",
    "        # Add the final features\n",
    "        x = self.efficient_net._conv_head(x)\n",
    "        x = self.efficient_net._bn1(x)\n",
    "        x = self.efficient_net._swish(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, channels=[320, 112, 40, 24]):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Upsampling blocks\n",
    "        self.up_conv1 = nn.ConvTranspose2d(channels[0], channels[0]//2, kernel_size=2, stride=2)\n",
    "        self.up_conv2 = nn.ConvTranspose2d(channels[1], channels[1]//2, kernel_size=2, stride=2)\n",
    "        self.up_conv3 = nn.ConvTranspose2d(channels[2], channels[2]//2, kernel_size=2, stride=2)\n",
    "        self.up_conv4 = nn.ConvTranspose2d(channels[3], channels[3]//2, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Attention gates\n",
    "        self.attention1 = AttentionGate(F_g=channels[0]//2, F_l=channels[1], F_int=channels[1]//2)\n",
    "        self.attention2 = AttentionGate(F_g=channels[1]//2, F_l=channels[2], F_int=channels[2]//2)\n",
    "        self.attention3 = AttentionGate(F_g=channels[2]//2, F_l=channels[3], F_int=channels[3]//2)\n",
    "        \n",
    "        # Convolutional blocks after concatenation\n",
    "        self.conv1 = ConvBlock(channels[0]//2 + channels[1], channels[1])\n",
    "        self.conv2 = ConvBlock(channels[1]//2 + channels[2], channels[2])\n",
    "        self.conv3 = ConvBlock(channels[2]//2 + channels[3], channels[3])\n",
    "        self.conv4 = ConvBlock(channels[3]//2, channels[3]//2)\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(channels[3]//2, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Decoder with skip connections and attention gates\n",
    "        x = self.up_conv1(features[4])\n",
    "        skip1_att = self.attention1(x, features[3])\n",
    "        x = torch.cat([x, skip1_att], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.up_conv2(x)\n",
    "        skip2_att = self.attention2(x, features[2])\n",
    "        x = torch.cat([x, skip2_att], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.up_conv3(x)\n",
    "        skip3_att = self.attention3(x, features[1])\n",
    "        x = torch.cat([x, skip3_att], dim=1)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.up_conv4(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Final 1x1 convolution to get segmentation map\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TileProcessor(nn.Module):\n",
    "    def __init__(self, tile_size=256, overlap=32):\n",
    "        super().__init__()\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Split image into overlapping tiles and return tiles with their positions\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        tile_size = self.tile_size\n",
    "        overlap = self.overlap\n",
    "        \n",
    "        tiles = []\n",
    "        positions = []\n",
    "        \n",
    "        # Calculate steps with overlap\n",
    "        h_steps = max(1, (H - overlap) // (tile_size - overlap))\n",
    "        w_steps = max(1, (W - overlap) // (tile_size - overlap))\n",
    "        \n",
    "        for i in range(h_steps):\n",
    "            h_start = min(i * (tile_size - overlap), H - tile_size)\n",
    "            for j in range(w_steps):\n",
    "                w_start = min(j * (tile_size - overlap), W - tile_size)\n",
    "                \n",
    "                # Extract tile\n",
    "                tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "                tiles.append(tile)\n",
    "                \n",
    "                # Store position\n",
    "                positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        # If we have any space left at the bottom or right, add extra tiles\n",
    "        if h_steps * (tile_size - overlap) + overlap < H:\n",
    "            h_start = H - tile_size\n",
    "            for j in range(w_steps):\n",
    "                w_start = min(j * (tile_size - overlap), W - tile_size)\n",
    "                tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "                tiles.append(tile)\n",
    "                positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        if w_steps * (tile_size - overlap) + overlap < W:\n",
    "            w_start = W - tile_size\n",
    "            for i in range(h_steps):\n",
    "                h_start = min(i * (tile_size - overlap), H - tile_size)\n",
    "                tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "                tiles.append(tile)\n",
    "                positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        # Also add the corner if needed\n",
    "        if (h_steps * (tile_size - overlap) + overlap < H and \n",
    "            w_steps * (tile_size - overlap) + overlap < W):\n",
    "            h_start = H - tile_size\n",
    "            w_start = W - tile_size\n",
    "            tile = x[:, :, h_start:h_start+tile_size, w_start:w_start+tile_size]\n",
    "            tiles.append(tile)\n",
    "            positions.append([h_start, w_start, h_start+tile_size, w_start+tile_size])\n",
    "        \n",
    "        # Stack tiles into a batch\n",
    "        tiles = torch.cat(tiles, dim=0)\n",
    "        positions_tensor = torch.tensor(positions, device=x.device)\n",
    "        \n",
    "        return tiles, positions_tensor\n",
    "\n",
    "class TileMerger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, tiles, positions, height, width):\n",
    "        \"\"\"Merge processed tiles back into a full image\"\"\"\n",
    "        device = tiles.device\n",
    "        B, C, tile_h, tile_w = tiles.shape\n",
    "        \n",
    "        # Create empty output and weight matrices\n",
    "        output = torch.zeros((1, C, height, width), device=device)\n",
    "        weights = torch.zeros((1, C, height, width), device=device)\n",
    "        \n",
    "        # Create a weight map for blending overlapping regions\n",
    "        # Use cosine weighting for smooth transitions\n",
    "        h_weight = torch.cosine_similarity(\n",
    "            torch.arange(tile_h, device=device).float().unsqueeze(0),\n",
    "            torch.tensor([tile_h//2], device=device).float().unsqueeze(0), \n",
    "            dim=0\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        w_weight = torch.cosine_similarity(\n",
    "            torch.arange(tile_w, device=device).float().unsqueeze(0),\n",
    "            torch.tensor([tile_w//2], device=device).float().unsqueeze(0), \n",
    "            dim=0\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        weight_map = h_weight.T @ w_weight\n",
    "        weight_map = weight_map.expand(1, C, tile_h, tile_w)\n",
    "        \n",
    "        # Place tiles back into the full image with weighted blending\n",
    "        for i in range(B):\n",
    "            h_start, w_start, h_end, w_end = positions[i]\n",
    "            output[:, :, h_start:h_end, w_start:w_end] += tiles[i:i+1] * weight_map\n",
    "            weights[:, :, h_start:h_end, w_start:w_end] += weight_map\n",
    "        \n",
    "        # Normalize by the weights to get the final output\n",
    "        output = output / (weights + 1e-8)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CLAHELayer(nn.Module):\n",
    "    \"\"\"CLAHE implementation using PyTorch operations (differentiable approximation)\"\"\"\n",
    "    def __init__(self, clip_limit=4.0, grid_size=(8, 8)):\n",
    "        super().__init__()\n",
    "        self.clip_limit = clip_limit\n",
    "        self.grid_size = grid_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # This is a simplified differentiable approximation of CLAHE\n",
    "        # Actual implementation would be more complex but using standard ops\n",
    "        \n",
    "        # Local contrast normalization (approximation of CLAHE)\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Use local normalization as an approximation\n",
    "        unfold = nn.Unfold(\n",
    "            kernel_size=(H // self.grid_size[0], W // self.grid_size[1]), \n",
    "            stride=(H // self.grid_size[0], W // self.grid_size[1])\n",
    "        )\n",
    "        \n",
    "        # Reshape and normalize locally\n",
    "        patches = unfold(x)\n",
    "        patches = patches.reshape(B, C, -1, self.grid_size[0] * self.grid_size[1])\n",
    "        \n",
    "        # Local mean and std\n",
    "        mean = torch.mean(patches, dim=3, keepdim=True)\n",
    "        std = torch.std(patches, dim=3, keepdim=True) + 1e-6\n",
    "        \n",
    "        # Clip contrast (approximation of clip limit)\n",
    "        std = torch.clamp(std, max=self.clip_limit * torch.mean(std))\n",
    "        \n",
    "        # Normalize patches\n",
    "        normalized = (patches - mean) / std\n",
    "        \n",
    "        # Fold back (this is not exact but approximates CLAHE effect)\n",
    "        normalized = normalized.reshape(B, C, -1)\n",
    "        fold = nn.Fold(\n",
    "            output_size=(H, W),\n",
    "            kernel_size=(H // self.grid_size[0], W // self.grid_size[1]),\n",
    "            stride=(H // self.grid_size[0], W // self.grid_size[1])\n",
    "        )\n",
    "        \n",
    "        output = fold(normalized)\n",
    "        \n",
    "        # Scale to proper range and add back to original for residual effect\n",
    "        return torch.sigmoid(output + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867c033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebab71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('21_patching.first_patching.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e31574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
