{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_embedding.semantic_deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove semantically similar images\n",
    "> Remove semantically similar images from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cv_tools.core import *\n",
    "from cv_tools.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6df7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from private_front_easy_pin_detection.pytorch_model_development import UnetManualMaxPoolOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5406d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25beb0e7",
   "metadata": {},
   "source": [
    "> installing faiss was a huge pain, but it works now\n",
    "- conda install -c conda-forge faiss-gpu libblas=*=*mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637c34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ImageFeatureDataset(Dataset):\n",
    "    def __init__(self, image_paths: List[str], transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((1152, 1632)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        image = Image.fromarray(image)\n",
    "        image = image/255.0\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6eee90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2929e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UNet and modify it to only use the encoder\n",
    "class EmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, unet_model):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.encoder = unet_model.encoder  # Use only the encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1, c2, c3, c4, c5 = self.encoder(x)  # Extract the deepest features\n",
    "\t\t# Global average pooling on the last feature map\n",
    "        pooled = nn.AdaptiveAvgPool2d((1, 1))(c5)\n",
    "        return pooled.view(pooled.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ab4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_features(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    feature_dim: int = 256\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Extract features from images using the encoder.\"\"\"\n",
    "    features = np.zeros((len(dataloader.dataset), feature_dim), dtype=np.float32)\n",
    "    paths = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch, batch_paths) in enumerate(tqdm(dataloader, desc=\"Extracting features\")):\n",
    "            batch = batch.to(device)\n",
    "            batch_features = model(batch).cpu().numpy()\n",
    "            \n",
    "            start_idx = idx * dataloader.batch_size\n",
    "            end_idx = start_idx + len(batch)\n",
    "            features[start_idx:end_idx] = batch_features\n",
    "            paths.extend(batch_paths)\n",
    "    \n",
    "    return features, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20580c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(\n",
    "    features: np.ndarray,\n",
    "    paths: List[str],\n",
    "    similarity_threshold: float = 0.95\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"Find duplicate images using FAISS indexing.\"\"\"\n",
    "    # Normalize features for cosine similarity\n",
    "    features = features.astype(np.float32)\n",
    "    faiss.normalize_L2(features)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatIP(features.shape[1])  # Inner product for cosine similarity\n",
    "    index.add(features)\n",
    "    \n",
    "    # Search for similar images\n",
    "    similarities, indices = index.search(features, k=50)  # Get top 50 similar images\n",
    "    \n",
    "    # Group duplicates\n",
    "    duplicates = {}\n",
    "    processed = set()\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        if i in processed:\n",
    "            continue\n",
    "            \n",
    "        current_duplicates = []\n",
    "        for j, sim in zip(indices[i], similarities[i]):\n",
    "            if sim > similarity_threshold and i != j and j not in processed:\n",
    "                current_duplicates.append(paths[j])\n",
    "                processed.add(j)\n",
    "                \n",
    "        if current_duplicates:\n",
    "            duplicates[paths[i]] = current_duplicates\n",
    "            processed.add(i)\n",
    "    \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a793d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mUnetManualMaxPoolOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboth_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ".. note::\n",
      "    As per the example above, an ``__init__()`` call to the parent class\n",
      "    must be made before assignment on the child.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Schreibtisch/projects/git_data/private_front_easy_pin_detection/private_front_easy_pin_detection/pytorch_model_development.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "UnetManualMaxPoolOnly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0cfdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(\n",
    "    image_dir: str,\n",
    "    output_dir: str,\n",
    "    batch_size: int = 32,\n",
    "    similarity_threshold: float = 0.95\n",
    ") -> Tuple[Dict[str, List[str]], int]:\n",
    "    \"\"\"Main function to process images and find duplicates.\"\"\"\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image_paths = [str(p) for p in Path(image_dir).glob(\"**/*\") if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}]\n",
    "    \n",
    "    # Initialize model\n",
    "    encoder = UnetManualMaxPoolOnly(in_channels=1, both_pool=False)\n",
    "    feature_extractor = EmbeddingModel(encoder).to(device)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataset = ImageFeatureDataset(image_paths)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Extract features\n",
    "    features, paths = extract_features(feature_extractor, dataloader, device)\n",
    "\n",
    "\t# Find duplicates\n",
    "    duplicates = find_duplicates(features, paths, similarity_threshold)\n",
    "    \n",
    "    # Create output directory structure\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    unique_dir = os.path.join(output_dir, \"unique\")\n",
    "    duplicate_dir = os.path.join(output_dir, \"duplicates\")\n",
    "    os.makedirs(unique_dir, exist_ok=True)\n",
    "    os.makedirs(duplicate_dir, exist_ok=True)\n",
    "    \n",
    "    # Move files\n",
    "    processed_count = 0\n",
    "    for original, duplicate_list in duplicates.items():\n",
    "        # Copy original to unique folder\n",
    "        shutil.copy2(original, unique_dir)\n",
    "        \n",
    "        # Copy duplicates to duplicate folder\n",
    "        for dup in duplicate_list:\n",
    "            shutil.copy2(dup, duplicate_dir)\n",
    "            processed_count += 1\n",
    "    \n",
    "    return duplicates, processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1d98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('19_model_embedding.semantic_deduplication.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa4e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
