{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp medSAM finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune medSAM\n",
    "> Finetune medSAM with own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# cv_tools\n",
    "from cv_tools.core import *\n",
    "from cv_tools.imports import *\n",
    "from cv_tools.data_processing.smb_tools import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d07810fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5a0d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import SamModel, SamProcessor\n",
    "import monai\n",
    "from monai.losses import DiceCELoss, DiceLoss, FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddaf7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import warnings\n",
    "from statistics import mean\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39d961e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class SAMFineTuneConfig:\n",
    "    \"\"\"Configuration class for SAM fine-tuning with production-ready defaults.\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    model_name: str = \"facebook/sam-vit-base\"\n",
    "    freeze_encoder: bool = True\n",
    "    freeze_prompt_encoder: bool = True\n",
    "    \n",
    "    # Training configuration\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 1e-5\n",
    "    weight_decay: float = 0.01\n",
    "    num_epochs: int = 100\n",
    "    warmup_steps: int = 100\n",
    "    gradient_clip_norm: float = 1.0\n",
    "    \n",
    "    # Loss configuration\n",
    "    loss_type: str = \"dice_ce\"  # \"dice_ce\", \"dice\", \"focal\", \"bce\"\n",
    "    dice_weight: float = 1.0\n",
    "    ce_weight: float = 1.0\n",
    "    focal_alpha: float = 0.25\n",
    "    focal_gamma: float = 2.0\n",
    "    \n",
    "    # Data configuration\n",
    "    image_size: int = 1024\n",
    "    mask_threshold: float = 0.5\n",
    "    bbox_perturbation_range: int = 20\n",
    "    \n",
    "    # Augmentation configuration\n",
    "    use_augmentation: bool = True\n",
    "    augmentation_prob: float = 0.5\n",
    "    \n",
    "    # Training configuration\n",
    "    device: str = \"auto\"  # \"auto\", \"cuda\", \"cpu\"\n",
    "    mixed_precision: bool = True\n",
    "    save_best_model: bool = True\n",
    "    early_stopping_patience: int = 10\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    log_interval: int = 10\n",
    "    save_interval: int = 50\n",
    "    output_dir: str = \"./sam_finetune_output\"\n",
    "    experiment_name: str = \"sam_finetune\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-initialization validation and setup.\"\"\"\n",
    "        if self.device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Create output directory\n",
    "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(Path(self.output_dir) / \"training.log\"),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74c657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0473ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BoundingBoxGenerator:\n",
    "    \"\"\"bounding box generator. \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            perturbation_range: int = 20, # max perturbation range\n",
    "            min_box_size: int = 10, # min box size\n",
    "            ):\n",
    "        self.perturbation_range = perturbation_range\n",
    "        self.min_box_size = min_box_size\n",
    "    \n",
    "    def get_bounding_box(\n",
    "            self, \n",
    "            mask: np.ndarray,  # binary mask\n",
    "            add_perturbation: bool = True, # whether to add random perturbation to bbox\n",
    "            ) -> List[int]: # bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "        \"\"\"\n",
    "        Generate bounding box from segmentation mask with robust error handling.\n",
    "        \"\"\"\n",
    "        if not isinstance(mask, np.ndarray):\n",
    "            raise TypeError(f\"Expected numpy array, got {type(mask)}\")\n",
    "        \n",
    "        if mask.ndim != 2:\n",
    "            raise ValueError(f\"Expected 2D mask, got {mask.ndim}D\")\n",
    "        \n",
    "        # Find non-zero pixels\n",
    "        y_indices, x_indices = np.where(mask > 0)\n",
    "        \n",
    "        if len(y_indices) == 0:\n",
    "            raise ValueError(\"Empty mask - no positive pixels found\")\n",
    "        \n",
    "        # Get bounding box coordinates\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        \n",
    "        # Ensure minimum box size\n",
    "        if (x_max - x_min) < self.min_box_size:\n",
    "            center_x = (x_min + x_max) // 2\n",
    "            x_min = max(0, center_x - self.min_box_size // 2)\n",
    "            x_max = min(mask.shape[1], center_x + self.min_box_size // 2)\n",
    "        \n",
    "        if (y_max - y_min) < self.min_box_size:\n",
    "            center_y = (y_min + y_max) // 2\n",
    "            y_min = max(0, center_y - self.min_box_size // 2)\n",
    "            y_max = min(mask.shape[0], center_y + self.min_box_size // 2)\n",
    "        \n",
    "        # Add perturbation if requested\n",
    "        if add_perturbation and self.perturbation_range > 0:\n",
    "            H, W = mask.shape\n",
    "            x_min = max(0, x_min - np.random.randint(0, self.perturbation_range + 1))\n",
    "            x_max = min(W, x_max + np.random.randint(0, self.perturbation_range + 1))\n",
    "            y_min = max(0, y_min - np.random.randint(0, self.perturbation_range + 1))\n",
    "            y_max = min(H, y_max + np.random.randint(0, self.perturbation_range + 1))\n",
    "        \n",
    "        return [x_min, y_min, x_max, y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da9b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SAM dataset with comprehensive preprocessing and augmentation.\n",
    "    \n",
    "    Supports multiple data formats:\n",
    "    - HuggingFace datasets\n",
    "    - Directory structure (images/ and masks/ folders)\n",
    "    - Custom data loaders\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: Union[str, Path, Any],\n",
    "        processor: SamProcessor,\n",
    "        config: SAMFineTuneConfig,\n",
    "        bbox_generator: Optional[BoundingBoxGenerator] = None,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        is_training: bool = True\n",
    "    ):\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.bbox_generator = bbox_generator or BoundingBoxGenerator(\n",
    "            perturbation_range=config.bbox_perturbation_range\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # Load data based on source type\n",
    "        self.data = self._load_data(data_source)\n",
    "        \n",
    "        logging.info(f\"Loaded {len(self.data)} samples for {'training' if is_training else 'validation'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07a19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _load_data(\n",
    "    self: SAMDataset, \n",
    "    data_source: Union[str, Path, Any]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load data from various sources with robust error handling.\"\"\"\n",
    "        \n",
    "    if hasattr(data_source, '__getitem__') and hasattr(data_source, '__len__'):\n",
    "        # HuggingFace dataset or similar\n",
    "        return [data_source[i] for i in range(len(data_source))]\n",
    "        \n",
    "    elif isinstance(data_source, (str, Path)):\n",
    "        # Directory structure\n",
    "        return self._load_from_directory(Path(data_source))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data source type: {type(data_source)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8ff94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4626039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch    \n",
    "def _load_from_directory(\n",
    "    self: SAMDataset,\n",
    "    data_dir: Path # path to data directory\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load data from directory structure: data_dir/{images,masks}/filename.ext\"\"\"\n",
    "        \n",
    "    images_dir = data_dir / \"images\"\n",
    "    masks_dir = data_dir / \"masks\"\n",
    "        \n",
    "    if not images_dir.exists():\n",
    "        raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "    if not masks_dir.exists():\n",
    "        raise FileNotFoundError(f\"Masks directory not found: {masks_dir}\")\n",
    "        \n",
    "    # Find matching image-mask pairs\n",
    "    image_files = sorted(list(images_dir.glob(\"*\")))\n",
    "    data = []\n",
    "        \n",
    "    for img_path in image_files:\n",
    "        # Look for corresponding mask\n",
    "        mask_path = masks_dir / img_path.name\n",
    "            \n",
    "        # Try different extensions if exact match not found\n",
    "        if not mask_path.exists():\n",
    "            stem = img_path.stem\n",
    "            for ext in ['.png', '.jpg', '.jpeg', '.tif', '.tiff']:\n",
    "                mask_path = masks_dir / f\"{stem}{ext}\"\n",
    "                if mask_path.exists():\n",
    "                    break\n",
    "            else:\n",
    "                logging.warning(f\"No mask found for image: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "        data.append({\n",
    "            \"image_path\": str(img_path),\n",
    "            \"mask_path\": str(mask_path)\n",
    "        })\n",
    "        \n",
    "    return data\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b06e6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __len__(self: SAMDataset) -> int:\n",
    "    return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da01a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_dummy_sample(\n",
    "    self: SAMDataset,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Create a dummy sample for error recovery.\"\"\"\n",
    "    dummy_image = Image.new('RGB', (256, 256), color='black')\n",
    "    dummy_mask = np.zeros((256, 256), dtype=np.float32)\n",
    "    dummy_bbox = [0, 0, 10, 10]\n",
    "        \n",
    "    inputs = self.processor(\n",
    "        dummy_image,\n",
    "        input_boxes=[[dummy_bbox]],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    inputs[\"ground_truth_mask\"] = torch.from_numpy(dummy_mask)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ada8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __getitem__(\n",
    "    self: SAMDataset, \n",
    "    idx: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Get a single training example with comprehensive preprocessing.\"\"\"\n",
    "        \n",
    "    try:\n",
    "        item = self.data[idx]\n",
    "            \n",
    "        # Load image and mask\n",
    "        if \"image\" in item and \"label\" in item:\n",
    "            # HuggingFace dataset format\n",
    "            image = item[\"image\"]\n",
    "            if hasattr(image, 'convert'):\n",
    "                image = image.convert('RGB')\n",
    "            mask = np.array(item[\"label\"])\n",
    "        else:\n",
    "            # File path format\n",
    "            image = Image.open(item[\"image_path\"]).convert('RGB')\n",
    "            mask = np.array(Image.open(item[\"mask_path\"]))\n",
    "            \n",
    "        # Convert to numpy for processing\n",
    "        image_np = np.array(image)\n",
    "            \n",
    "        # Ensure mask is binary\n",
    "        if mask.max() > 1:\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "            \n",
    "        # Apply augmentations if training\n",
    "        if self.is_training and self.transform is not None:\n",
    "            augmented = self.transform(image=image_np, mask=mask)\n",
    "            image_np = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        # Convert back to PIL for processor\n",
    "        if isinstance(image_np, np.ndarray):\n",
    "            image = Image.fromarray(image_np)\n",
    "            \n",
    "        # Generate bounding box prompt\n",
    "        bbox = self.bbox_generator.get_bounding_box(\n",
    "            mask, add_perturbation=self.is_training\n",
    "        )\n",
    "            \n",
    "        # Prepare inputs for model\n",
    "        inputs = self.processor(\n",
    "            image, \n",
    "            input_boxes=[[bbox]], \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "            \n",
    "        # Remove batch dimension added by processor\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "            \n",
    "        # Add ground truth mask\n",
    "        inputs[\"ground_truth_mask\"] = torch.from_numpy(mask.astype(np.float32))\n",
    "            \n",
    "        return inputs\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing item {idx}: {str(e)}\")\n",
    "        # Return a dummy sample to prevent training interruption\n",
    "        return self._get_dummy_sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c834ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba020c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AugmentationFactory:\n",
    "    \"\"\"Factory for creating medical image augmentation pipelines.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_training_transform(config: SAMFineTuneConfig) -> A.Compose:\n",
    "        \"\"\"Create training augmentation pipeline optimized for medical images.\"\"\"\n",
    "        \n",
    "        transforms = []\n",
    "        \n",
    "        if config.use_augmentation:\n",
    "            # Geometric transformations (preserve anatomical structure)\n",
    "            transforms.extend([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.3),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.1,\n",
    "                    scale_limit=0.1,\n",
    "                    rotate_limit=15,\n",
    "                    p=0.5,\n",
    "                    border_mode=0\n",
    "                ),\n",
    "            ])\n",
    "            \n",
    "            # Intensity transformations (medical imaging specific)\n",
    "            transforms.extend([\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.2,\n",
    "                    contrast_limit=0.2,\n",
    "                    p=0.5\n",
    "                ),\n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "                A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n",
    "            ])\n",
    "            \n",
    "            # Noise and blur (simulate acquisition artifacts)\n",
    "            transforms.extend([\n",
    "                A.GaussNoise(var_limit=(10, 50), p=0.2),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "            ])\n",
    "        \n",
    "        return A.Compose(transforms)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_validation_transform() -> A.Compose:\n",
    "        \"\"\"Create validation transform (no augmentation).\"\"\"\n",
    "        return A.Compose([])  # No augmentation for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d18bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LossFactory:\n",
    "    \"\"\"Factory for creating different loss functions for segmentation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_loss(config: SAMFineTuneConfig) -> nn.Module:\n",
    "        \"\"\"Create loss function based on configuration.\"\"\"\n",
    "        \n",
    "        if config.loss_type == \"dice_ce\":\n",
    "            return DiceCELoss(\n",
    "                sigmoid=True,  # Apply sigmoid activation to convert logits to probabilities\n",
    "                squared_pred=True,  # Square predicted values in Dice calculation for smoother gradients\n",
    "                reduction='mean',  # Average loss across batch dimension for stable training\n",
    "                ce_weight=config.ce_weight,  # Weight for cross-entropy component in combined loss\n",
    "                dice_weight=config.dice_weight  # Weight for Dice component in combined loss\n",
    "            )\n",
    "        \n",
    "        elif config.loss_type == \"dice\":\n",
    "            return DiceLoss(\n",
    "                sigmoid=True,\n",
    "                squared_pred=True,\n",
    "                reduction='mean'\n",
    "            )\n",
    "        \n",
    "        elif config.loss_type == \"focal\":\n",
    "            return FocalLoss(\n",
    "                alpha=config.focal_alpha, # Weight for positive class\n",
    "                gamma=config.focal_gamma, # Focusing parameter\n",
    "                reduction='mean' # Average loss across batch dimension for stable training\n",
    "            )\n",
    "        \n",
    "        elif config.loss_type == \"bce\":\n",
    "            return nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {config.loss_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590b8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Calculate segmentation metrics for evaluation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def dice_coefficient(\n",
    "        pred: torch.Tensor, \n",
    "        target: torch.Tensor, \n",
    "        smooth: float = 1e-6 # small constant to avoid division by zero\n",
    "        ) -> float:\n",
    "        \"\"\"Calculate Dice coefficient.\"\"\"\n",
    "        pred = (pred > 0.5).float()\n",
    "        target = target.float()\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum()\n",
    "        \n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        return dice.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def iou_score(pred: torch.Tensor, target: torch.Tensor, smooth: float = 1e-6) -> float:\n",
    "        \"\"\"Calculate IoU (Jaccard) score.\"\"\"\n",
    "        pred = (pred > 0.5).float()\n",
    "        target = target.float()\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum() - intersection\n",
    "        \n",
    "        iou = (intersection + smooth) / (union + smooth)\n",
    "        return iou.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def pixel_accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"Calculate pixel-wise accuracy.\"\"\"\n",
    "        pred = (pred > 0.5).float()\n",
    "        target = target.float()\n",
    "        \n",
    "        correct = (pred == target).float().sum()\n",
    "        total = target.numel()\n",
    "        \n",
    "        return (correct / total).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbd130a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SAMTrainer:\n",
    "    \"\"\"\n",
    "    SAM trainer with comprehensive features:\n",
    "    - Mixed precision training\n",
    "    - Gradient clipping\n",
    "    - Learning rate scheduling\n",
    "    - Early stopping\n",
    "    - Model checkpointing\n",
    "    - Comprehensive logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: SAMFineTuneConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        \n",
    "        # Initialize model and processor\n",
    "        self.processor = SamProcessor.from_pretrained(config.model_name)\n",
    "        self.model = self._setup_model()\n",
    "        \n",
    "        # Initialize training components\n",
    "        self.optimizer = self._setup_optimizer()\n",
    "        self.scheduler = self._setup_scheduler()\n",
    "        self.loss_fn = LossFactory.create_loss(config)\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None\n",
    "        \n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.training_history = []\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics_calc = MetricsCalculator()\n",
    "        \n",
    "        logging.info(f\"SAMTrainer initialized with device: {self.device}\")\n",
    "        logging.info(f\"Model parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a9400fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _setup_model(\n",
    "    self: SAMTrainer,\n",
    "    ) -> SamModel:\n",
    "        \"\"\"Setup and configure the SAM model.\"\"\"\n",
    "        model = SamModel.from_pretrained(self.config.model_name)\n",
    "        \n",
    "        # Freeze components based on configuration\n",
    "        if self.config.freeze_encoder:\n",
    "            for param in model.vision_encoder.parameters():\n",
    "                param.requires_grad_(False)\n",
    "            logging.info(\"Vision encoder frozen\")\n",
    "        \n",
    "        if self.config.freeze_prompt_encoder:\n",
    "            for param in model.prompt_encoder.parameters():\n",
    "                param.requires_grad_(False)\n",
    "            logging.info(\"Prompt encoder frozen\")\n",
    "        \n",
    "        # Only train mask decoder by default\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logging.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        return model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "874623b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _setup_optimizer(\n",
    "    self: SAMTrainer,\n",
    "    ) -> torch.optim.Optimizer:\n",
    "        \"\"\"Setup optimizer with proper parameter filtering.\"\"\"\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        \n",
    "        return AdamW(\n",
    "            trainable_params,\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0041ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _setup_scheduler(\n",
    "    self: SAMTrainer,\n",
    "    ) -> torch.optim.lr_scheduler._LRScheduler:\n",
    "    \"\"\"Setup learning rate scheduler.\"\"\"\n",
    "    return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        self.optimizer,\n",
    "        T_max=self.config.num_epochs,\n",
    "        eta_min=self.config.learning_rate * 0.01\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f6a5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _train_one_batch(\n",
    "    self: SAMTrainer,\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    batch_idx: int\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Train on a single batch and return metrics.\"\"\"\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):\n",
    "            outputs = self.model(\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                input_boxes=batch[\"input_boxes\"],\n",
    "                multimask_output=False\n",
    "            )\n",
    "            \n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"]\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "        \n",
    "        # Backward pass\n",
    "        # Clear gradients from previous iteration\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Handle mixed precision training with gradient scaler\n",
    "        if self.scaler is not None:\n",
    "            # Scale loss to prevent gradient underflow in fp16\n",
    "            self.scaler.scale(loss).backward()\n",
    "            # Unscale gradients before clipping to get true gradient norms\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(), \n",
    "                self.config.gradient_clip_norm\n",
    "            )\n",
    "            # Update parameters with scaled gradients\n",
    "            self.scaler.step(self.optimizer)\n",
    "            # Update scaler for next iteration\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            # Standard fp32 training without gradient scaling\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(), \n",
    "                self.config.gradient_clip_norm\n",
    "            )\n",
    "            # Update model parameters\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Calculate metrics without gradient computation\n",
    "        with torch.no_grad():\n",
    "            # Apply sigmoid to get probabilities from logits\n",
    "            pred_sigmoid = torch.sigmoid(predicted_masks)\n",
    "            # Calculate Dice coefficient for segmentation overlap\n",
    "            dice = self.metrics_calc.dice_coefficient(pred_sigmoid, ground_truth_masks)\n",
    "            # Calculate Intersection over Union score\n",
    "            iou = self.metrics_calc.iou_score(pred_sigmoid, ground_truth_masks)\n",
    "            # Calculate pixel-wise classification accuracy\n",
    "            accuracy = self.metrics_calc.pixel_accuracy(pred_sigmoid, ground_truth_masks)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'dice': dice,\n",
    "            'iou': iou,\n",
    "            'accuracy': accuracy\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ec9784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train_epoch(\n",
    "    self: SAMTrainer,\n",
    "    train_loader: DataLoader\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch with comprehensive logging.\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_metrics = {'dice': [], 'iou': [], 'accuracy': []}\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {self.current_epoch}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            batch_stats = self._train_one_batch(batch, batch_idx)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_losses.append(batch_stats['loss'])\n",
    "            epoch_metrics['dice'].append(batch_stats['dice'])\n",
    "            epoch_metrics['iou'].append(batch_stats['iou'])\n",
    "            epoch_metrics['accuracy'].append(batch_stats['accuracy'])\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{batch_stats['loss']:.4f}\",\n",
    "                'dice': f\"{batch_stats['dice']:.4f}\",\n",
    "                'lr': f\"{self.optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        epoch_stats = {\n",
    "            'loss': mean(epoch_losses),\n",
    "            'dice': mean(epoch_metrics['dice']),\n",
    "            'iou': mean(epoch_metrics['iou']),\n",
    "            'accuracy': mean(epoch_metrics['accuracy']),\n",
    "            'lr': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        return epoch_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2fd0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def validate(\n",
    "    self: SAMTrainer,\n",
    "    val_loader: DataLoader\n",
    "    ) -> Dict[str, float]:\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    self.model.eval()\n",
    "    val_losses = []\n",
    "    val_metrics = {'dice': [], 'iou': [], 'accuracy': []}\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "                \n",
    "            outputs = self.model(\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                input_boxes=batch[\"input_boxes\"],\n",
    "                multimask_output=False\n",
    "            )\n",
    "                \n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"]\n",
    "                \n",
    "            loss = self.loss_fn(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "                \n",
    "            # Calculate metrics\n",
    "            pred_sigmoid = torch.sigmoid(predicted_masks)\n",
    "            dice = self.metrics_calc.dice_coefficient(pred_sigmoid, ground_truth_masks)\n",
    "            iou = self.metrics_calc.iou_score(pred_sigmoid, ground_truth_masks)\n",
    "            accuracy = self.metrics_calc.pixel_accuracy(pred_sigmoid, ground_truth_masks)\n",
    "                \n",
    "            val_losses.append(loss.item())\n",
    "            val_metrics['dice'].append(dice)\n",
    "            val_metrics['iou'].append(iou)\n",
    "            val_metrics['accuracy'].append(accuracy)\n",
    "        \n",
    "    return {\n",
    "        'val_loss': mean(val_losses),\n",
    "        'val_dice': mean(val_metrics['dice']),\n",
    "        'val_iou': mean(val_metrics['iou']),\n",
    "        'val_accuracy': mean(val_metrics['accuracy'])\n",
    "    }\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07d7dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_checkpoint(\n",
    "    self: SAMTrainer,\n",
    "    epoch: int,\n",
    "    is_best: bool = False\n",
    "    ):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': self.model.state_dict(),\n",
    "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "        'best_loss': self.best_loss,\n",
    "        'config': self.config,\n",
    "        'training_history': self.training_history\n",
    "    }\n",
    "        \n",
    "    # Save regular checkpoint\n",
    "    checkpoint_path = Path(self.config.output_dir) / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        best_path = Path(self.config.output_dir) / \"best_model.pt\"\n",
    "        torch.save(checkpoint, best_path)\n",
    "        logging.info(f\"New best model saved with loss: {self.best_loss:.6f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9607946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train(\n",
    "    self: SAMTrainer,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: Optional[DataLoader] = None\n",
    "    ):\n",
    "    \"\"\"Main training loop with all production features.\"\"\"\n",
    "    logging.info(\"Starting training...\")\n",
    "        \n",
    "    for epoch in range(self.config.num_epochs):\n",
    "        self.current_epoch = epoch\n",
    "            \n",
    "        # Training\n",
    "        train_stats = self.train_epoch(train_loader)\n",
    "            \n",
    "        # Validation\n",
    "        val_stats = {}\n",
    "        if val_loader is not None:\n",
    "            val_stats = self.validate(val_loader)\n",
    "            \n",
    "        # Learning rate scheduling\n",
    "        self.scheduler.step()\n",
    "            \n",
    "        # Combine stats\n",
    "        epoch_stats = {**train_stats, **val_stats}\n",
    "        self.training_history.append(epoch_stats)\n",
    "            \n",
    "        # Logging\n",
    "        log_msg = f\"Epoch {epoch}: \"\n",
    "        log_msg += f\"Loss: {train_stats['loss']:.6f}, \"\n",
    "        log_msg += f\"Dice: {train_stats['dice']:.4f}, \"\n",
    "        log_msg += f\"IoU: {train_stats['iou']:.4f}\"\n",
    "            \n",
    "        if val_stats:\n",
    "            log_msg += f\", Val Loss: {val_stats['val_loss']:.6f}\"\n",
    "            log_msg += f\", Val Dice: {val_stats['val_dice']:.4f}\"\n",
    "            \n",
    "        logging.info(log_msg)\n",
    "            \n",
    "        # Model checkpointing and early stopping\n",
    "        current_loss = val_stats.get('val_loss', train_stats['loss'])\n",
    "        is_best = current_loss < self.best_loss\n",
    "            \n",
    "        if is_best:\n",
    "            self.best_loss = current_loss\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if epoch % self.config.save_interval == 0 or is_best:\n",
    "            self.save_checkpoint(epoch, is_best)\n",
    "            \n",
    "        # Early stopping\n",
    "        if self.patience_counter >= self.config.early_stopping_patience:\n",
    "            logging.info(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "        \n",
    "    logging.info(\"Training completed!\")\n",
    "        \n",
    "    # Save final model\n",
    "    self.save_checkpoint(self.current_epoch, False)\n",
    "        \n",
    "    return self.training_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c79ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9066fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SAMInference:\n",
    "    \"\"\"Production-ready inference class for fine-tuned SAM models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, config: SAMFineTuneConfig):\n",
    "        self.config = config # configuration object\n",
    "        self.device = torch.device(config.device) # device to use for inference\n",
    "        \n",
    "        # Load model and processor\n",
    "        self.processor = SamProcessor.from_pretrained(config.model_name) # processor for image preprocessing\n",
    "        self.model = self._load_model(model_path) # load model from checkpoint\n",
    "        self.bbox_generator = BoundingBoxGenerator(perturbation_range=0)  # No perturbation for inference\n",
    "        \n",
    "        logging.info(f\"SAMInference initialized with model from: {model_path}\")\n",
    "\n",
    "\n",
    "    def _load_model(\n",
    "            self, \n",
    "            model_path: str) -> SamModel:\n",
    "        \"\"\"Load the fine-tuned model.\"\"\"\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(\n",
    "            model_path, \n",
    "            map_location=self.device)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SamModel.from_pretrained(\n",
    "            self.config.model_name\n",
    "            )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31ff56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_from_bbox(\n",
    "    self: SAMInference, \n",
    "    image: Union[Image.Image, np.ndarray], # input image (PIL Image or numpy array)\n",
    "    bbox: List[int], # bounding box [x_min, y_min, x_max, y_max]\n",
    "    return_logits: bool = False # whether to return raw logits or sigmoid probabilities\n",
    ") -> np.ndarray: # segmentation mask as numpy array (H, W)\n",
    "    \"\"\"\n",
    "    Predict segmentation mask from image and bounding box.\n",
    "            \n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "    # Prepare inputs\n",
    "    inputs = self.processor(\n",
    "        image,\n",
    "        input_boxes=[[bbox]],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(self.device)\n",
    "        \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(**inputs, multimask_output=False)\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            \n",
    "        if return_logits:\n",
    "            return predicted_masks.cpu().numpy().squeeze()\n",
    "        else:\n",
    "            # Apply sigmoid and threshold\n",
    "            mask_prob = torch.sigmoid(predicted_masks)\n",
    "            mask = (mask_prob > self.config.mask_threshold).float()\n",
    "            return mask.cpu().numpy().squeeze()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cf437b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predict_from_mask(\n",
    "    self: SAMInference,\n",
    "    image: Union[Image.Image, np.ndarray], # input image (PIL Image or numpy array)\n",
    "    rough_mask: np.ndarray, # rough segmentation mask (H, W)\n",
    "    return_logits: bool = False # whether to return raw logits or sigmoid probabilities\n",
    ") -> np.ndarray: # refined segmentation mask as numpy array (H, W)\n",
    "    \"\"\"\n",
    "    Predict segmentation from image and rough mask (generates bbox automatically).\n",
    "    \"\"\"\n",
    "        # Generate bounding box from rough mask\n",
    "    bbox = self.bbox_generator.get_bounding_box(rough_mask, add_perturbation=False)\n",
    "        \n",
    "    return self.predict_from_bbox(image, bbox, return_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dda9dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def predict_batch(\n",
    "    self: SAMInference,\n",
    "    images: List[Union[Image.Image, np.ndarray]], # list of input images (PIL Image or numpy array)\n",
    "    bboxes: List[List[int]], # list of bounding boxes [x_min, y_min, x_max, y_max]\n",
    "    batch_size: int = 4 # batch size for processing\n",
    "    ) -> List[np.ndarray]: # list of segmentation masks as numpy arrays (H, W)\n",
    "    \"\"\"Batch prediction for multiple images.\"\"\"\n",
    "    results = [] # list to store segmentation masks\n",
    "        \n",
    "    for i in range(0, len(images), batch_size): # process in batches\n",
    "        batch_images = images[i:i+batch_size] # get batch of images\n",
    "        batch_bboxes = bboxes[i:i+batch_size] # get batch of bounding boxes\n",
    "            \n",
    "        batch_results = [] # list to store segmentation masks for batch\n",
    "        for img, bbox in zip(batch_images, batch_bboxes): # predict for each image in batch\n",
    "            mask = self.predict_from_bbox(img, bbox) # predict segmentation mask\n",
    "            batch_results.append(mask) # add to batch results\n",
    "            \n",
    "        results.extend(batch_results) # add batch results to final results\n",
    "        \n",
    "    return results # return list of segmentation masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c24519ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VisualizationUtils:\n",
    "    \"\"\"Utilities for visualizing segmentation results.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def show_mask(mask: np.ndarray, ax: plt.Axes, random_color: bool = False, alpha: float = 0.6):\n",
    "        \"\"\"Display segmentation mask overlay.\"\"\"\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n",
    "        else:\n",
    "            color = np.array([30/255, 144/255, 255/255, alpha])\n",
    "        \n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "    \n",
    "    @staticmethod\n",
    "    def show_bbox(\n",
    "        bbox: List[int], # bounding box [x_min, y_min, x_max, y_max]\n",
    "        ax: plt.Axes, # matplotlib axes object\n",
    "        color: str = 'red', # color of bounding box\n",
    "        linewidth: int = 2 # width of bounding box\n",
    "    ):\n",
    "        \"\"\"Display bounding box.\"\"\"\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        rect = plt.Rectangle(\n",
    "            (x_min, y_min), \n",
    "            x_max - x_min, \n",
    "            y_max - y_min,\n",
    "            fill=False, \n",
    "            color=color, \n",
    "            linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_predictions(\n",
    "        image: Union[Image.Image, np.ndarray], # input image (PIL Image or numpy array)\n",
    "        ground_truth: np.ndarray, # ground truth mask (H, W)\n",
    "        prediction: np.ndarray, # predicted mask (H, W)\n",
    "        bbox: Optional[List[int]] = None, # bounding box [x_min, y_min, x_max, y_max]\n",
    "        title: str = \"Comparison\"\n",
    "    ):\n",
    "        \"\"\"Create comparison visualization.\"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(image)\n",
    "        if bbox is not None:\n",
    "            VisualizationUtils.show_bbox(bbox, axes[0])\n",
    "        axes[0].set_title(\"Original Image + BBox\")\n",
    "        axes[0].axis(\"off\")\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[1].imshow(image)\n",
    "        VisualizationUtils.show_mask(ground_truth, axes[1], alpha=0.7)\n",
    "        axes[1].set_title(\"Ground Truth\")\n",
    "        axes[1].axis(\"off\")\n",
    "        \n",
    "        # Prediction\n",
    "        axes[2].imshow(image)\n",
    "        VisualizationUtils.show_mask(prediction, axes[2], random_color=True, alpha=0.7)\n",
    "        axes[2].set_title(\"Prediction\")\n",
    "        axes[2].axis(\"off\")\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_history(\n",
    "        history: List[Dict[str, float]] # training history\n",
    "    ):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        if not history:\n",
    "            return\n",
    "        \n",
    "        epochs = range(len(history))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Loss\n",
    "        train_loss = [h['loss'] for h in history]\n",
    "        val_loss = [h.get('val_loss', None) for h in history]\n",
    "        \n",
    "        axes[0, 0].plot(epochs, train_loss, label='Train Loss')\n",
    "        if any(v is not None for v in val_loss):\n",
    "            val_loss_clean = [v for v in val_loss if v is not None]\n",
    "            val_epochs = [i for i, v in enumerate(val_loss) if v is not None]\n",
    "            axes[0, 0].plot(val_epochs, val_loss_clean, label='Val Loss')\n",
    "        axes[0, 0].set_title('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Dice Score\n",
    "        train_dice = [h['dice'] for h in history]\n",
    "        val_dice = [h.get('val_dice', None) for h in history]\n",
    "        \n",
    "        axes[0, 1].plot(epochs, train_dice, label='Train Dice')\n",
    "        if any(v is not None for v in val_dice):\n",
    "            val_dice_clean = [v for v in val_dice if v is not None]\n",
    "            val_epochs = [i for i, v in enumerate(val_dice) if v is not None]\n",
    "            axes[0, 1].plot(val_epochs, val_dice_clean, label='Val Dice')\n",
    "        axes[0, 1].set_title('Dice Score')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # IoU Score\n",
    "        train_iou = [h['iou'] for h in history]\n",
    "        val_iou = [h.get('val_iou', None) for h in history]\n",
    "        \n",
    "        axes[1, 0].plot(epochs, train_iou, label='Train IoU')\n",
    "        if any(v is not None for v in val_iou):\n",
    "            val_iou_clean = [v for v in val_iou if v is not None]\n",
    "            val_epochs = [i for i, v in enumerate(val_iou) if v is not None]\n",
    "            axes[1, 0].plot(val_epochs, val_iou_clean, label='Val IoU')\n",
    "        axes[1, 0].set_title('IoU Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Learning Rate\n",
    "        lr = [h['lr'] for h in history]\n",
    "        axes[1, 1].plot(epochs, lr)\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b91a460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_sam_finetune_pipeline(\n",
    "    train_data_source: Union[str, Path, Any], # training data source\n",
    "    val_data_source: Optional[Union[str, Path, Any]] = None, # validation data source\n",
    "    config: Optional[SAMFineTuneConfig] = None, # configuration object\n",
    "    **config_kwargs\n",
    ") -> Tuple[SAMTrainer, DataLoader, Optional[DataLoader]]:\n",
    "    \"\"\"\n",
    "    Create a complete SAM fine-tuning pipeline with sensible defaults.\n",
    "    \n",
    "    This is the main entry point for users who want to quickly set up training.\n",
    "    \n",
    "    Args:\n",
    "        train_data_source: Training data (HuggingFace dataset, directory path, etc.)\n",
    "        val_data_source: Validation data (optional)\n",
    "        config: Configuration object (optional, will create default if None)\n",
    "        **config_kwargs: Additional configuration parameters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trainer, train_loader, val_loader)\n",
    "        \n",
    "    Example:\n",
    "        ```python\n",
    "        # Using HuggingFace dataset\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")\n",
    "        \n",
    "        trainer, train_loader, val_loader = create_sam_finetune_pipeline(\n",
    "            train_data_source=dataset,\n",
    "            batch_size=4,\n",
    "            num_epochs=50,\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        history = trainer.train(train_loader, val_loader)\n",
    "        ```\n",
    "        \n",
    "        ```python\n",
    "        # Using directory structure\n",
    "        trainer, train_loader, val_loader = create_sam_finetune_pipeline(\n",
    "            train_data_source=\"./data/train\",\n",
    "            val_data_source=\"./data/val\",\n",
    "            use_augmentation=True,\n",
    "            mixed_precision=True\n",
    "        )\n",
    "        \n",
    "        history = trainer.train(train_loader, val_loader)\n",
    "        ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create configuration\n",
    "    if config is None:\n",
    "        config = SAMFineTuneConfig(**config_kwargs)\n",
    "    else:\n",
    "        # Update config with any additional kwargs\n",
    "        for key, value in config_kwargs.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "    \n",
    "    # Initialize trainer (this also initializes processor)\n",
    "    trainer = SAMTrainer(config)\n",
    "    \n",
    "    # Create augmentation transforms\n",
    "    train_transform = AugmentationFactory.create_training_transform(config)\n",
    "    val_transform = AugmentationFactory.create_validation_transform()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SAMDataset(\n",
    "        data_source=train_data_source,\n",
    "        processor=trainer.processor,\n",
    "        config=config,\n",
    "        transform=train_transform,\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = None\n",
    "    if val_data_source is not None:\n",
    "        val_dataset = SAMDataset(\n",
    "            data_source=val_data_source,\n",
    "            processor=trainer.processor,\n",
    "            config=config,\n",
    "            transform=val_transform,\n",
    "            is_training=False\n",
    "        )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if config.device == \"cuda\" else False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = None\n",
    "    if val_dataset is not None:\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True if config.device == \"cuda\" else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "    \n",
    "    logging.info(f\"Pipeline created successfully!\")\n",
    "    logging.info(f\"Training samples: {len(train_dataset)}\")\n",
    "    if val_dataset:\n",
    "        logging.info(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    return trainer, train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10203f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def quick_train_sam(\n",
    "    train_data_source: Union[str, Path, Any],\n",
    "    val_data_source: Optional[Union[str, Path, Any]] = None,\n",
    "    output_dir: str = \"./sam_finetune_output\",\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 4,\n",
    "    learning_rate: float = 1e-5,\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Quick training function for SAM fine-tuning with minimal setup.\n",
    "    \n",
    "    This function provides a one-liner solution for training SAM on custom data.\n",
    "    \n",
    "    Args:\n",
    "        train_data_source: Training data source\n",
    "        val_data_source: Validation data source (optional)\n",
    "        output_dir: Directory to save outputs\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Training batch size\n",
    "        learning_rate: Learning rate\n",
    "        **kwargs: Additional configuration parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing training history and model paths\n",
    "        \n",
    "    Example:\n",
    "        ```python\n",
    "        # Train SAM on your data with one line\n",
    "        results = quick_train_sam(\n",
    "            train_data_source=\"./my_data/train\",\n",
    "            val_data_source=\"./my_data/val\",\n",
    "            num_epochs=100,\n",
    "            batch_size=8\n",
    "        )\n",
    "        \n",
    "        # Access training history\n",
    "        history = results['history']\n",
    "        best_model_path = results['best_model_path']\n",
    "        ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create configuration\n",
    "    config = SAMFineTuneConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    trainer, train_loader, val_loader = create_sam_finetune_pipeline(\n",
    "        train_data_source=train_data_source,\n",
    "        val_data_source=val_data_source,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train(train_loader, val_loader)\n",
    "    \n",
    "    # Plot training history\n",
    "    VisualizationUtils.plot_training_history(history)\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'history': history,\n",
    "        'config': config,\n",
    "        'best_model_path': str(Path(config.output_dir) / \"best_model.pt\"),\n",
    "        'final_model_path': str(Path(config.output_dir) / f\"checkpoint_epoch_{trainer.current_epoch}.pt\"),\n",
    "        'trainer': trainer\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Usage Examples\n",
    "\n",
    "Here are comprehensive examples showing how to use the SAM fine-tuning framework with your own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Quick training with HuggingFace dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset\n",
    "dataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")\n",
    "\n",
    "# Quick training with minimal setup\n",
    "results = quick_train_sam(\n",
    "    train_data_source=dataset,\n",
    "    output_dir=\"./sam_breast_cancer\",\n",
    "    num_epochs=10,  # Reduced for demo\n",
    "    batch_size=2,   # Reduced for demo\n",
    "    learning_rate=1e-5,\n",
    "    use_augmentation=True,\n",
    "    mixed_precision=True\n",
    ")\n",
    "\n",
    "print(f\"Training completed! Best model saved at: {results['best_model_path']}\")\n",
    "print(f\"Final validation dice score: {results['history'][-1].get('val_dice', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c27108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Advanced training with custom configuration\n",
    "config = SAMFineTuneConfig(\n",
    "    model_name=\"facebook/sam-vit-base\",\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_epochs=100,\n",
    "    loss_type=\"dice_ce\",\n",
    "    use_augmentation=True,\n",
    "    mixed_precision=True,\n",
    "    early_stopping_patience=15,\n",
    "    output_dir=\"./advanced_sam_training\"\n",
    ")\n",
    "\n",
    "# Create pipeline with custom config\n",
    "trainer, train_loader, val_loader = create_sam_finetune_pipeline(\n",
    "    train_data_source=dataset,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train with custom monitoring\n",
    "history = trainer.train(train_loader, val_loader)\n",
    "\n",
    "# Plot results\n",
    "VisualizationUtils.plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using your own data from directories\n",
    "# Assuming you have data organized as:\n",
    "# my_data/\n",
    "# ├── train/\n",
    "# │   ├── images/\n",
    "# │   │   ├── img1.jpg\n",
    "# │   │   └── img2.jpg\n",
    "# │   └── masks/\n",
    "# │       ├── img1.png\n",
    "# │       └── img2.png\n",
    "# └── val/\n",
    "#     ├── images/\n",
    "#     └── masks/\n",
    "\n",
    "# Uncomment and modify paths for your data:\n",
    "\"\"\"\n",
    "results = quick_train_sam(\n",
    "    train_data_source=\"./my_data/train\",\n",
    "    val_data_source=\"./my_data/val\",\n",
    "    output_dir=\"./my_sam_model\",\n",
    "    num_epochs=50,\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Inference with trained model\n",
    "# Load trained model for inference\n",
    "config = SAMFineTuneConfig(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inference_model = SAMInference(\n",
    "    model_path=\"./sam_breast_cancer/best_model.pt\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Test inference on a sample\n",
    "sample_idx = 0\n",
    "sample = dataset[sample_idx]\n",
    "image = sample[\"image\"]\n",
    "ground_truth = np.array(sample[\"label\"])\n",
    "\n",
    "# Generate bounding box from ground truth (in practice, you'd provide this)\n",
    "bbox_gen = BoundingBoxGenerator(perturbation_range=0)\n",
    "bbox = bbox_gen.get_bounding_box(ground_truth, add_perturbation=False)\n",
    "\n",
    "# Predict\n",
    "prediction = inference_model.predict_from_bbox(image, bbox)\n",
    "\n",
    "# Visualize results\n",
    "VisualizationUtils.compare_predictions(\n",
    "    image=image,\n",
    "    ground_truth=ground_truth,\n",
    "    prediction=prediction,\n",
    "    bbox=bbox,\n",
    "    title=\"SAM Fine-tuning Results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Key Features\n",
    "\n",
    "This production-ready SAM fine-tuning framework includes:\n",
    "\n",
    "### 🚀 **Easy-to-Use API**\n",
    "- One-line training with `quick_train_sam()`\n",
    "- Flexible pipeline creation with `create_sam_finetune_pipeline()`\n",
    "- Support for multiple data formats (HuggingFace datasets, directory structure)\n",
    "\n",
    "### 🔧 **Production Features**\n",
    "- **Mixed precision training** for faster training and lower memory usage\n",
    "- **Gradient clipping** to prevent exploding gradients\n",
    "- **Learning rate scheduling** with cosine annealing\n",
    "- **Early stopping** to prevent overfitting\n",
    "- **Model checkpointing** with automatic best model saving\n",
    "- **Comprehensive logging** with file and console output\n",
    "\n",
    "### 📊 **Advanced Monitoring**\n",
    "- Real-time metrics calculation (Dice, IoU, Pixel Accuracy)\n",
    "- Training history visualization\n",
    "- Progress bars with live metrics\n",
    "- Configurable logging intervals\n",
    "\n",
    "### 🎯 **Medical Image Optimizations**\n",
    "- **Medical-specific augmentations** (CLAHE, controlled rotations, intensity variations)\n",
    "- **Robust bounding box generation** with perturbation and error handling\n",
    "- **Multiple loss functions** (Dice-CE, Dice, Focal, BCE)\n",
    "- **Proper preprocessing** following medical imaging best practices\n",
    "\n",
    "### 🔄 **Flexible Configuration**\n",
    "- Dataclass-based configuration with validation\n",
    "- Support for different SAM model variants\n",
    "- Configurable freezing of encoder components\n",
    "- Customizable augmentation pipelines\n",
    "\n",
    "### 🎨 **Visualization Tools**\n",
    "- Training curve plotting\n",
    "- Prediction comparison visualizations\n",
    "- Mask overlay utilities\n",
    "- Bounding box visualization\n",
    "\n",
    "### 💾 **Data Handling**\n",
    "- Support for various image formats\n",
    "- Automatic image-mask pairing\n",
    "- Robust error handling with fallback mechanisms\n",
    "- Efficient data loading with multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('22_medSAM_finetune.ipynb')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Installation & Requirements\n",
    "\n",
    "To use this SAM fine-tuning framework, install the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision transformers datasets\n",
    "pip install monai albumentations\n",
    "pip install matplotlib pillow tqdm\n",
    "pip install numpy scipy\n",
    "```\n",
    "\n",
    "For CUDA support (recommended):\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "## Quick Start Guide\n",
    "\n",
    "### 1. Prepare Your Data\n",
    "\n",
    "**Option A: Directory Structure**\n",
    "```\n",
    "your_data/\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.png\n",
    "│   │   └── ...\n",
    "│   └── masks/\n",
    "│       ├── image1.png  # Binary mask (0=background, 255=foreground)\n",
    "│       ├── image2.png\n",
    "│       └── ...\n",
    "└── val/\n",
    "    ├── images/\n",
    "    └── masks/\n",
    "```\n",
    "\n",
    "**Option B: HuggingFace Dataset**\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"your-username/your-dataset\")\n",
    "```\n",
    "\n",
    "### 2. Train Your Model\n",
    "\n",
    "**Simple Training (One Line):**\n",
    "```python\n",
    "results = quick_train_sam(\n",
    "    train_data_source=\"./your_data/train\",\n",
    "    val_data_source=\"./your_data/val\",\n",
    "    num_epochs=50,\n",
    "    batch_size=4\n",
    ")\n",
    "```\n",
    "\n",
    "**Advanced Training:**\n",
    "```python\n",
    "config = SAMFineTuneConfig(\n",
    "    model_name=\"facebook/sam-vit-base\",  # or sam-vit-large, sam-vit-huge\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    num_epochs=100,\n",
    "    loss_type=\"dice_ce\",  # dice_ce, dice, focal, bce\n",
    "    use_augmentation=True,\n",
    "    mixed_precision=True,\n",
    "    early_stopping_patience=15\n",
    ")\n",
    "\n",
    "trainer, train_loader, val_loader = create_sam_finetune_pipeline(\n",
    "    train_data_source=\"./your_data/train\",\n",
    "    val_data_source=\"./your_data/val\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "history = trainer.train(train_loader, val_loader)\n",
    "```\n",
    "\n",
    "### 3. Use Your Trained Model\n",
    "\n",
    "```python\n",
    "# Load trained model\n",
    "inference_model = SAMInference(\n",
    "    model_path=\"./sam_finetune_output/best_model.pt\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Predict with bounding box\n",
    "prediction = inference_model.predict_from_bbox(image, [x_min, y_min, x_max, y_max])\n",
    "\n",
    "# Predict from rough mask\n",
    "prediction = inference_model.predict_from_mask(image, rough_mask)\n",
    "```\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "The `SAMFineTuneConfig` class provides extensive customization:\n",
    "\n",
    "```python\n",
    "config = SAMFineTuneConfig(\n",
    "    # Model settings\n",
    "    model_name=\"facebook/sam-vit-base\",  # Model variant\n",
    "    freeze_encoder=True,                 # Freeze vision encoder\n",
    "    freeze_prompt_encoder=True,          # Freeze prompt encoder\n",
    "    \n",
    "    # Training settings\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_epochs=100,\n",
    "    gradient_clip_norm=1.0,\n",
    "    \n",
    "    # Loss settings\n",
    "    loss_type=\"dice_ce\",                 # Loss function type\n",
    "    dice_weight=1.0,\n",
    "    ce_weight=1.0,\n",
    "    \n",
    "    # Data settings\n",
    "    mask_threshold=0.5,                  # Prediction threshold\n",
    "    bbox_perturbation_range=20,          # Bbox augmentation\n",
    "    \n",
    "    # Training features\n",
    "    mixed_precision=True,                # Use AMP\n",
    "    early_stopping_patience=10,          # Early stopping\n",
    "    save_best_model=True,               # Save best checkpoint\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir=\"./sam_finetune_output\",\n",
    "    experiment_name=\"my_experiment\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Best Practices for Medical Images\n",
    "\n",
    "1. **Data Preprocessing**: Images are automatically normalized to [0,1] range\n",
    "2. **Augmentation**: Medical-specific augmentations (CLAHE, controlled rotations)\n",
    "3. **Loss Functions**: Use `dice_ce` for balanced precision/recall\n",
    "4. **Batch Size**: Start with 4-8, adjust based on GPU memory\n",
    "5. **Learning Rate**: 1e-5 is a good starting point for fine-tuning\n",
    "6. **Validation**: Always use validation data for proper evaluation\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Out of Memory:**\n",
    "- Reduce `batch_size`\n",
    "- Enable `mixed_precision=True`\n",
    "- Use smaller model variant (`sam-vit-base` instead of `sam-vit-large`)\n",
    "\n",
    "**Poor Performance:**\n",
    "- Increase `num_epochs`\n",
    "- Try different `loss_type` (dice_ce, focal)\n",
    "- Adjust `learning_rate`\n",
    "- Enable data augmentation\n",
    "\n",
    "**Training Too Slow:**\n",
    "- Enable `mixed_precision=True`\n",
    "- Increase `batch_size` if memory allows\n",
    "- Use GPU with CUDA support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ExportModuleProc._default_exp_() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnbdev\u001b[39;00m; nbdev.nbdev_export(\u001b[33m'\u001b[39m\u001b[33m22_medSAM_finetune.ipynb\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/fastcore/script.py:116\u001b[39m, in \u001b[36mcall_parse.<locals>._f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_f\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     mod = inspect.getmodule(inspect.currentframe().f_back)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mod: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SCRIPT_INFO.func \u001b[38;5;129;01mand\u001b[39;00m mod.\u001b[34m__name__\u001b[39m==\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m: SCRIPT_INFO.func = func.\u001b[34m__name__\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys.argv)>\u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys.argv[\u001b[32m1\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m: sys.argv.pop(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/nbdev/doclinks.py:155\u001b[39m, in \u001b[36mnbdev_export\u001b[39m\u001b[34m(path, procs, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m     procs = [\u001b[38;5;28mgetattr\u001b[39m(nbdev.export, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m L(procs)]\n\u001b[32m    154\u001b[39m files = nbglob(path=path, as_path=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs).sorted(\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files: \u001b[43mnb_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m add_init(get_config().lib_path)\n\u001b[32m    157\u001b[39m _build_modidx()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/nbdev/export.py:82\u001b[39m, in \u001b[36mnb_export\u001b[39m\u001b[34m(nbname, lib_path, procs, name, mod_maker, debug, solo_nb)\u001b[39m\n\u001b[32m     80\u001b[39m exp = ExportModuleProc()\n\u001b[32m     81\u001b[39m nb = NBProcessor(nbname, [exp]+L(procs), debug=debug)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod,cells \u001b[38;5;129;01min\u001b[39;00m exp.modules.items():\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m first(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m cells \u001b[38;5;28;01mif\u001b[39;00m o.cell_type==\u001b[33m'\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/nbdev/process.py:129\u001b[39m, in \u001b[36mNBProcessor.process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    128\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mProcess all cells with all processors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.procs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_proc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/nbdev/process.py:122\u001b[39m, in \u001b[36mNBProcessor._proc\u001b[39m\u001b[34m(self, proc)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_proc\u001b[39m(\u001b[38;5;28mself\u001b[39m, proc):\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc,\u001b[33m'\u001b[39m\u001b[33mbegin\u001b[39m\u001b[33m'\u001b[39m): proc.begin()\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nb.cells: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc,\u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m): proc.end()\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.nb.cells = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nb.cells \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(c,\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/nbdev/process.py:112\u001b[39m, in \u001b[36mNBProcessor._process_cell\u001b[39m\u001b[34m(self, proc, cell)\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m cell.directives_:\n\u001b[32m    111\u001b[39m         f = \u001b[38;5;28mgetattr\u001b[39m(proc, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m f: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_comment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(proc) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_direc(proc): cell = opt_set(cell, proc(cell))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/nbdev/process.py:118\u001b[39m, in \u001b[36mNBProcessor._process_comment\u001b[39m\u001b[34m(self, proc, cell, cmd)\u001b[39m\n\u001b[32m    116\u001b[39m args = cell.directives_[cmd]\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug: \u001b[38;5;28mprint\u001b[39m(cmd, args, proc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: ExportModuleProc._default_exp_() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('22_medSAM_finetune.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997cb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
