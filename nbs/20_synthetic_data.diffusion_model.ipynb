{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp synthetic_data.diffusion_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data with diffusion model\n",
    "> Generate synthetic data with diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cv_tools.core import *\n",
    "from cv_tools.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from fastcore.all import *\n",
    "import time\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1aa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92f3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, tokenizer, size=512):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        \n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(size, size),\n",
    "            A.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert('L'))\n",
    "        \n",
    "        # Apply transforms\n",
    "        transformed = self.transform(image=image, mask=mask)\n",
    "        image = transformed['image']\n",
    "        mask = transformed['mask']\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        \n",
    "        # Generate text embedding\n",
    "        text = \"X-ray image of chest\"  # Customize based on your dataset\n",
    "        text_ids = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"mask\": mask,\n",
    "            \"text_ids\": text_ids,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0061ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fine_tune_stable_diffusion(\n",
    "    train_dataset,\n",
    "    output_dir=\"./sd-xray-model\",\n",
    "    num_epochs=100,\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-5\n",
    "):\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Load models\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "    \n",
    "    # Freeze text encoder\n",
    "    text_encoder.requires_grad_(False)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Prepare everything for accelerator\n",
    "    unet, optimizer, train_dataloader = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Get input images and text\n",
    "            clean_images = batch[\"pixel_values\"]\n",
    "            text_embeddings = text_encoder(batch[\"text_ids\"])[0]\n",
    "            \n",
    "            # Add noise to images\n",
    "            noise = torch.randn_like(clean_images)\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (clean_images.shape[0],))\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = unet(noisy_images, timesteps, text_embeddings).sample\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Step {step}: Loss {loss.item():.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if epoch % 10 == 0:\n",
    "            accelerator.save_state(f\"{output_dir}/checkpoint-{epoch}\")\n",
    "    \n",
    "    return unet\n",
    "\n",
    "def generate_xray_with_mask(\n",
    "    prompt,\n",
    "    pipeline,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    num_images=1\n",
    "):\n",
    "    # Generate images\n",
    "    images = pipeline(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_images_per_prompt=num_images\n",
    "    ).images\n",
    "    \n",
    "    # For each generated image, create a corresponding mask\n",
    "    masks = []\n",
    "    for image in images:\n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Simple thresholding for demonstration\n",
    "        # You might want to use more sophisticated segmentation here\n",
    "        mask = (img_array.mean(axis=2) > 128).astype(np.uint8) * 255\n",
    "        masks.append(Image.fromarray(mask))\n",
    "    \n",
    "    return images, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb80515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup paths\n",
    "    image_paths = [\"path/to/xray/images\"]\n",
    "    mask_paths = [\"path/to/mask/images\"]\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = XRayDataset(image_paths, mask_paths, tokenizer)\n",
    "    \n",
    "    # Fine-tune model\n",
    "    fine_tuned_unet = fine_tune_stable_diffusion(dataset)\n",
    "    \n",
    "    # Load fine-tuned pipeline\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        \"CompVis/stable-diffusion-v1-4\",\n",
    "        unet=fine_tuned_unet,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate new images with masks\n",
    "    images, masks = generate_xray_with_mask(\n",
    "        \"X-ray image of chest with clear lung fields\",\n",
    "        pipeline\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e383f",
   "metadata": {},
   "source": [
    "# with memory optimization with 2080ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a1474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "import psutil\n",
    "import GPUtil\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "067f06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPUMonitor:\n",
    "    \"\"\"\n",
    "    Real-time GPU memory monitoring\n",
    "    \"\"\"\n",
    "    def __init__(self, delay=1):\n",
    "        self.delay = delay\n",
    "        self.monitoring = False\n",
    "        self.memory_history = []\n",
    "\n",
    "    def start(self):\n",
    "        self.monitoring = True\n",
    "        Thread(target=self._monitor).start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.monitoring = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07158a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(GPUMonitor)\n",
    "def _monitor(self):\n",
    "    while self.monitoring:\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\t\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "\n",
    "        self.memory_history.append(\n",
    "\t\t\t{\n",
    "                'used':memory_used,\n",
    "\t\t\t\t'reserved':memory_reserved,\n",
    "\t\t\t\t'timestamp':time.time()\n",
    "\t\t\t}\n",
    "        )\n",
    "\t\t# Alert if memory usage is too high (>90% of available memory)\n",
    "        if memory_used / torch.cuda.get_device_properties(0).total_memory > 0.9:\n",
    "            print(f\"\\n⚠️ WARNING: High GPU memory usage: {memory_used:.2f}GB\")\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "\n",
    "        time.sleep(self.delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60548836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(GPUMonitor)\n",
    "def get_stats(self):\n",
    "\tif not self.memory_history:\n",
    "\t\treturn None\n",
    "\tmemory_used = [x['used'] for x in self.memory_history]\n",
    "\treturn {\n",
    "\t\t'max_used':np.max(memory_used),\n",
    "\t\t'avg_used':sum(memory_used)/len(memory_used),\n",
    "\t\t'current': memory_used[-1] if memory_used else 0\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d99d4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MemoryOptimizedDataset(Dataset):\n",
    "\t\"Memory efficient dataset\"\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\timage_paths,\n",
    "\t\t\tmask_paths,\n",
    "\t\t\tcache_dir='./cache',\n",
    "\t\t\tmax_images=None,\n",
    "\t\t\tresolution=512\n",
    "\t\t):\n",
    "\t\tself.image_paths = [Path(p) for p in image_paths]\n",
    "\t\tself.mask_paths = [Path(p) for p in mask_paths]\n",
    "\n",
    "\t\tif len(self.image_paths) != len(self.mask_paths):\n",
    "\t\t\traise ValueError(\n",
    "\t\t\t\tf\"Number of images ({len(self.image_paths)}) and masks ({len(self.mask_paths)}) must be the same\"\n",
    "\t\t\t)\n",
    "\t\t\n",
    "\t\tif max_images is not None:\n",
    "\t\t\tif not isinstance(max_images, int):\n",
    "\t\t\t\traise ValueError(\n",
    "\t\t\t\t\tf\"max_images must be an integer, got {type(max_images)}\"\n",
    "\t\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\tif max_images < 0:\n",
    "\t\t\t\traise ValueError(\n",
    "\t\t\t\t\tf\"max_images must be a non-negative integer, got {max_images}\"\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tif max_images > len(image_paths):\n",
    "\t\t\t\tlogging.warning(\n",
    "                    f\"max_images ({max_images}) is greater than available images \"\n",
    "                    f\"({len(self.image_paths)}). Using all available images.\"\n",
    "                )\n",
    "\t\t\t\tmax_images = len(self.image_paths)\n",
    "\n",
    "\t\t\tself.image_paths = sorted(image_paths)[:max_images]\n",
    "\t\t\tself.mask_paths = sorted(mask_paths)[:max_images]\n",
    "\t\t\n",
    "\t\t# set up cache directory\n",
    "\t\tself.cache_dir = Path(cache_dir)\n",
    "\n",
    "\t\tPath(self.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\t\tself.resolution = resolution\n",
    "\n",
    "\t\tself.transform = self._setup_transforms()\n",
    "\t\tself._validate_files()\n",
    "\n",
    "\n",
    "\t\tlogging.info(\n",
    "\t\t\tf\"Initialized MemoryOptimizedDataset with {len(self)} images\"\n",
    "\t\t)\n",
    "\n",
    "\tdef _setup_transforms(self)->A.Compose:\n",
    "\t\t\"\"\"Setup memory-efficient transforms\"\"\"\n",
    "\n",
    "\t\t# Memory-efficient transforms\n",
    "\t\treturn \tA.Compose([\n",
    "\t\t\tA.SmallestMaxSize(max_size=self.resolution),\n",
    "\t\t\tA.CenterCrop(self.resolution, self.resolution),\n",
    "\t\t\tA.Normalize(mean=[0.5], std=[0.5]),\n",
    "\t\t])\n",
    "\n",
    "\tdef _validate_files(self):\n",
    "\t\t\"\"\"Validate all files exist and are readable\"\"\"\n",
    "\t\tinvalid_pairs = []\n",
    "\t\tfor img_path, mask_path in zip(self.image_paths, self.mask_paths):\n",
    "\t\t\tif not img_path.exists():\n",
    "\t\t\t\tinvalid_pairs.append((str(img_path), \"Image file missing\"))\n",
    "\t\t\tif not mask_path.exists():\n",
    "\t\t\t\tinvalid_pairs.append((str(mask_path), \"Mask file missing\"))\n",
    "                \n",
    "\t\tif invalid_pairs:\n",
    "\t\t\terror_msg = \"\\nInvalid files found:\"\n",
    "\t\t\tfor path, reason in invalid_pairs:\n",
    "\t\t\t\terror_msg += f\"\\n{path}: {reason}\"\n",
    "\t\t\traise FileNotFoundError(error_msg)\n",
    "        \n",
    "        # Pre-compute and cache image statistics\n",
    "\t\tself._compute_dataset_stats()\n",
    "\n",
    "\tdef _compute_dataset_stats(self):\n",
    "\t\t\"\"\"Compute dataset statistics for memory-efficient normalization\"\"\"\n",
    "\t\tprint(\"Computing dataset statistics...\")\n",
    "\t\tmeans, stds = [], []\n",
    "\t\t\n",
    "\t\tfor img_path in self.image_paths[:min(100, len(self.image_paths))]:\n",
    "\t\t\timg = np.array(Image.open(img_path).convert('RGB'))\n",
    "\t\t\tmeans.append(img.mean() / 255.0)\n",
    "\t\t\tstds.append(img.std() / 255.0)\n",
    "        \n",
    "\t\tself.dataset_mean = np.mean(means)\n",
    "\t\tself.dataset_std = np.max(stds)  # Use max std for better normalization\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60eb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MemoryOptimizedDataset)\n",
    "def __len__(self):\n",
    "\treturn len(self.image_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee349464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MemoryOptimizedDataset)\n",
    "def get_cache_path(self, idx):\n",
    "\treturn self.cache_dir / f\"processed_{idx}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d0e8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MemoryOptimizedDataset)\n",
    "def load_image(self, path: Path) -> Tuple[np.ndarray, bool]:\n",
    "        \"\"\"Load image with error handling\"\"\"\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                if path in self.mask_paths:\n",
    "                    img = img.convert('L')\n",
    "                else:\n",
    "                    img = img.convert('RGB')\n",
    "                return np.array(img), True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading image {path}: {str(e)}\")\n",
    "            return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54b39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9da6bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MemoryOptimizedDataset)\n",
    "def __getitem__(self, idx):\n",
    "        # Load and process image with minimal memory usage\n",
    "        try:\n",
    "            # Check if cached version exists\n",
    "            cache_path = self.get_cache_path(idx)\n",
    "            if cache_path.exists():\n",
    "                return torch.load(cache_path)\n",
    "\n",
    "\n",
    "            img_path = self.image_paths[idx]\n",
    "            mask_path = self.mask_paths[idx]\n",
    "\n",
    "            img, img_success = self.load_image(img_path)\n",
    "            mask, mask_success = self.load_image(mask_path)\n",
    "\n",
    "            if not (img_success and mask_success):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Error loading image or mask file at index {idx}\"\n",
    "\t\t\t\t)\n",
    "            \n",
    "            # Apply transforms\n",
    "            transformed = self.transform(image=img, mask=mask)\n",
    "            img_tensor = torch.from_numpy(transformed['image']).permute(2, 0, 1)\n",
    "            mask_tensor = torch.from_numpy(transformed['mask']).unsqueeze(0)\n",
    "            \n",
    "            # Cache the processed tensors\n",
    "            data = {\n",
    "                'image': img_tensor,\n",
    "                'mask': mask_tensor,\n",
    "                'path': str(self.image_paths[idx])\n",
    "            }\n",
    "            torch.save(data, cache_path)\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing image {self.image_paths[idx]}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d52122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed93ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def handle_oom_error():\n",
    "    \"\"\"\n",
    "    Handle out-of-memory errors\n",
    "    \"\"\"\n",
    "    print(\"\\n🚨 Out of Memory Error detected! Taking corrective actions...\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Get current memory status\n",
    "    gpu = GPUtil.getGPUs()[0]\n",
    "    memory_used = gpu.memoryUsed\n",
    "    memory_total = gpu.memoryTotal\n",
    "    \n",
    "    print(f\"\\nGPU Memory Status:\")\n",
    "    print(f\"Used: {memory_used}MB / {memory_total}MB\")\n",
    "    \n",
    "    # Recommendations based on memory usage\n",
    "    if memory_used / memory_total > 0.9:\n",
    "        print(\"\\nRecommendations:\")\n",
    "        print(\"1. Reduce batch size\")\n",
    "        print(\"2. Enable gradient checkpointing\")\n",
    "        print(\"3. Use mixed precision training\")\n",
    "        print(\"4. Reduce image resolution\")\n",
    "        print(\"5. Enable attention slicing\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf663c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_memory_efficient_training(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    max_memory_usage=0.9\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup training with memory limits and monitoring\n",
    "    \"\"\"\n",
    "    # Initialize memory monitor\n",
    "    monitor = GPUMonitor()\n",
    "    \n",
    "    try:\n",
    "        # Configure training components with memory limits\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            \"CompVis/stable-diffusion-v1-4\",\n",
    "            torch_dtype=torch.float16,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False\n",
    "        )\n",
    "        \n",
    "        # Enable all memory optimizations\n",
    "        pipeline.enable_attention_slicing(slice_size=\"max\")\n",
    "        pipeline.enable_vae_slicing()\n",
    "        pipeline.enable_sequential_cpu_offload()\n",
    "\n",
    "\n",
    "\t\t# Memory-efficient dataloader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "            pin_memory=False,  # Disable pin_memory to save RAM\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        return pipeline, dataloader, monitor\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            handle_oom_error()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9060322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def monitor_training(monitor, pipeline, dataloader, num_epochs):\n",
    "    \"\"\"\n",
    "    Monitor training process with memory tracking\n",
    "    \"\"\"\n",
    "    monitor.start()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in dataloader:\n",
    "                # Check memory status\n",
    "                stats = monitor.get_stats()\n",
    "                if stats and stats['current'] > 10:  # 10GB threshold for 2080 Ti\n",
    "                    print(f\"\\n⚠️ High memory usage detected: {stats['current']:.2f}GB\")\n",
    "                    \n",
    "                    # Take preventive actions\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Training step here\n",
    "                # ...\n",
    "                \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            handle_oom_error()\n",
    "            raise e\n",
    "    finally:\n",
    "        monitor.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Initialize dataset\n",
    "    dataset = MemoryOptimizedDataset(\n",
    "        image_paths=[\"path/to/images\"],\n",
    "        mask_paths=[\"path/to/masks\"],\n",
    "        resolution=512\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Setup training\n",
    "        pipeline, dataloader, monitor = setup_memory_efficient_training(dataset)\n",
    "        \n",
    "        # Print initial memory status\n",
    "        print(\"\\nInitial GPU Memory Status:\")\n",
    "        print(f\"Total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB\")\n",
    "        print(f\"Reserved: {torch.cuda.memory_reserved()/1024**3:.1f}GB\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n",
    "        \n",
    "        # Start training with monitoring\n",
    "        monitor_training(monitor, pipeline, dataloader)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\")\n",
    "        handle_oom_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23dfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_2080ti_optimized_training(\n",
    "    dataset,\n",
    "    model_id=\"CompVis/stable-diffusion-v1-4\",\n",
    "    output_dir=\"./sd-xray-model\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized setup for RTX 2080 Ti (11GB VRAM)\n",
    "    \"\"\"\n",
    "    # Memory optimization settings\n",
    "    config = {\n",
    "        \"batch_size\": 1,\n",
    "        \"image_size\": 512,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"mixed_precision\": \"fp16\",\n",
    "        \"cache_latents\": True,\n",
    "        \"use_8bit_adam\": True\n",
    "    }\n",
    "    \n",
    "    # Initialize accelerator with mixed precision\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=\"fp16\",\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    )\n",
    "    \n",
    "    # Load pipeline with memory optimizations\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None,  # Disable safety checker to save memory\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    \n",
    "    # Apply memory optimizations\n",
    "    pipeline.enable_attention_slicing(slice_size=\"max\")\n",
    "    pipeline.enable_vae_slicing()\n",
    "    pipeline.enable_sequential_cpu_offload()\n",
    "    pipeline.enable_model_cpu_offload()\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    pipeline.unet.enable_gradient_checkpointing()\n",
    "    \n",
    "    # Use 8-bit Adam optimizer to save memory\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        optimizer = bnb.optim.AdamW8bit(\n",
    "            pipeline.unet.parameters(),\n",
    "            lr=1e-5,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "    except ImportError:\n",
    "        print(\"bitsandbytes not installed. Falling back to regular AdamW\")\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            pipeline.unet.parameters(),\n",
    "            lr=1e-5\n",
    "        )\n",
    "    \n",
    "    # Memory efficient dataloader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=1,  # Reduced workers to save memory\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return pipeline, train_dataloader, optimizer, accelerator, config\n",
    "\n",
    "def train_with_memory_monitoring(\n",
    "    pipeline, \n",
    "    train_dataloader, \n",
    "    optimizer, \n",
    "    accelerator, \n",
    "    num_epochs=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop with memory monitoring for RTX 2080 Ti\n",
    "    \"\"\"\n",
    "    # Get device\n",
    "    device = accelerator.device\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Clear cache periodically\n",
    "            if step % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Get memory status\n",
    "            gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "            gpu_memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "            \n",
    "            # Print memory usage\n",
    "            if step % 10 == 0:\n",
    "                print(f\"\\nGPU Memory in use: {gpu_memory:.2f}GB\")\n",
    "                print(f\"GPU Memory reserved: {gpu_memory_reserved:.2f}GB\")\n",
    "                \n",
    "                # Warning if memory is getting too high\n",
    "                if gpu_memory > 10:  # 10GB threshold for 11GB card\n",
    "                    print(\"WARNING: High memory usage detected!\")\n",
    "            \n",
    "            with accelerator.accumulate(pipeline.unet):\n",
    "                # Forward pass with memory optimization\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Your training step here\n",
    "                    # ... (rest of training logic)\n",
    "                    pass\n",
    "                \n",
    "                # Backward pass\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(pipeline.unet.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Save memory by moving batch to CPU\n",
    "            del batch\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save checkpoint with memory optimization\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Saving checkpoint...\")\n",
    "            # Move model to CPU before saving\n",
    "            pipeline.to(\"cpu\")\n",
    "            accelerator.save_state(f\"{output_dir}/checkpoint-{epoch}\")\n",
    "            pipeline.to(device)\n",
    "            print(\"Checkpoint saved!\")\n",
    "\n",
    "def check_system_compatibility():\n",
    "    \"\"\"\n",
    "    Check if system meets minimum requirements\n",
    "    \"\"\"\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    ram = psutil.virtual_memory().total / 1024**3\n",
    "    \n",
    "    print(f\"\\nSystem Check:\")\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f}GB\")\n",
    "    print(f\"RAM: {ram:.1f}GB\")\n",
    "    \n",
    "    if gpu_memory < 10:\n",
    "        print(\"\\nWARNING: GPU memory might be insufficient for Stable Diffusion training\")\n",
    "    if ram < 16:\n",
    "        print(\"\\nWARNING: More RAM recommended for optimal performance\")\n",
    "    \n",
    "    return gpu_memory >= 10 and ram >= 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    if check_system_compatibility():\n",
    "        pipeline, dataloader, optimizer, accelerator, config = setup_2080ti_optimized_training(\n",
    "            dataset=your_dataset  # Your dataset here\n",
    "        )\n",
    "        \n",
    "        print(\"\\nConfiguration for RTX 2080 Ti:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        train_with_memory_monitoring(pipeline, dataloader, optimizer, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952a9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export('20_synthetic_data.diffusion_model.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
