{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous functions for MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "> miscellaneous functions for MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mae.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import builtins\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.foundation import *\n",
    "from fastcore.basics import patch_to\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    " @patch_to(MetricLogger)\n",
    " def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    " # Test the update function\n",
    " \n",
    "def test_update():\n",
    "     logger = MetricLogger()\n",
    "     \n",
    "     # Test with float\n",
    "     logger.update(loss=0.5)\n",
    "     test_eq('loss' in logger.meters, True)\n",
    "     test_eq(logger.meters['loss'].value, 0.5)\n",
    "     \n",
    "     # Test with int\n",
    "     logger.update(epoch=1)\n",
    "     test_eq('epoch' in logger.meters, True)\n",
    "     test_eq(logger.meters['epoch'].value, 1)\n",
    "     \n",
    "     # Test with None (should be ignored)\n",
    "     logger.update(ignored=None)\n",
    "     test_eq('ignored' in logger.meters, False)\n",
    "     \n",
    "     # Test with tensor\n",
    "     tensor_value = torch.tensor(1.5)\n",
    "     logger.update(tensor_metric=tensor_value)\n",
    "     test_eq('tensor_metric' in logger.meters, True)\n",
    "     test_eq(logger.meters['tensor_metric'].value, 1.5)\n",
    "\n",
    " # Run the test\n",
    " test_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MetricLogger)\n",
    "def __getattr__(self, attr):\n",
    "    if attr in self.meters:\n",
    "        return self.meters[attr]\n",
    "    if attr in self.__dict__:\n",
    "        return self.__dict__[attr]\n",
    "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "        type(self).__name__, attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_getattr():\n",
    "    logger = MetricLogger()\n",
    "    logger.update(loss=0.5, epoch=1)\n",
    "    test_eq(logger.loss.value, 0.5)\n",
    "    test_eq(logger.epoch.value, 1)\n",
    "    test_eq(logger.meters['loss'].value, 0.5)\n",
    "    test_eq(logger.meters['epoch'].value, 1)\n",
    "    test_eq(logger.meters['loss'].avg, 0.5)\n",
    "    test_eq(logger.meters['epoch'].avg, 1)\n",
    "\n",
    "# Run the test\n",
    "test_getattr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MetricLogger)\n",
    "def __str__(self):\n",
    "    loss_str = []\n",
    "    for name, meter in self.meters.items():\n",
    "        loss_str.append(\n",
    "            \"{}: {}\".format(name, str(meter))\n",
    "        )\n",
    "    return self.delimiter.join(loss_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_str():\n",
    "\tlogger = MetricLogger()\n",
    "\tlogger.update(loss=0.5, epoch=1)\n",
    "\ttest_eq(str(logger), 'loss: 0.5000 (0.5000)\\tepoch: 1.0000 (1.0000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MetricLogger)\n",
    "def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_synchronize_between_processes():\n",
    "    logger = MetricLogger()\n",
    "    logger.update(loss=0.5, epoch=1)\n",
    "    logger.synchronize_between_processes()\n",
    "    test_eq(logger.loss.value, 0.5)\n",
    "    test_eq(logger.epoch.value, 1)\n",
    "    test_eq(logger.loss.global_avg, 0.5)\n",
    "    test_eq(logger.epoch.global_avg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MetricLogger)\n",
    "def add_meter(self, name, meter):\n",
    "    self.meters[name] = meter\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_add_meter():\n",
    "    logger = MetricLogger()\n",
    "    meter = SmoothedValue(window_size=10)\n",
    "    logger.add_meter('new_meter', meter)\n",
    "    test_eq(logger.meters['new_meter'], meter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(MetricLogger)\n",
    "def log_every(\n",
    "\tself, \n",
    "\titerable, \n",
    "\tprint_freq, \n",
    "\theader=None\n",
    "\t):\n",
    "    i = 0\n",
    "    if not header:\n",
    "        header = ''\n",
    "    start_time = time.time()\n",
    "    end = time.time()\n",
    "    iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "    data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "    space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "    log_msg = [\n",
    "        header,\n",
    "        '[{0' + space_fmt + '}/{1}]',\n",
    "        'eta: {eta}',\n",
    "        '{meters}',\n",
    "        'time: {time}',\n",
    "        'data: {data}'\n",
    "    ]\n",
    "    if torch.cuda.is_available():\n",
    "        log_msg.append('max mem: {memory:.0f}')\n",
    "    log_msg = self.delimiter.join(log_msg)\n",
    "    MB = 1024.0 * 1024.0\n",
    "    for obj in iterable:\n",
    "        data_time.update(time.time() - end)\n",
    "        yield obj\n",
    "        iter_time.update(time.time() - end)\n",
    "        if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "            eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "            if torch.cuda.is_available():\n",
    "                print(log_msg.format(\n",
    "                    i, len(iterable), eta=eta_string,\n",
    "                    meters=str(self),\n",
    "                    time=str(iter_time), data=str(data_time),\n",
    "                    memory=torch.cuda.max_memory_allocated() / MB))\n",
    "            else:\n",
    "                print(log_msg.format(\n",
    "                    i, len(iterable), eta=eta_string,\n",
    "                    meters=str(self),\n",
    "                    time=str(iter_time), data=str(data_time)))\n",
    "        i += 1\n",
    "        end = time.time()\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    if len(iterable) > 0:\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "    else:\n",
    "        print('{} Total time: {} (0.0000 s / it)'.format(\n",
    "\t\theader, total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total time: 0:00:00 (0.0000 s / it)\n",
      "\t[0/1]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      " Total time: 0:00:00 (0.0010 s / it)\n",
      "\t[0/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      "\t[2/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      " Total time: 0:00:00 (0.0001 s / it)\n",
      "\t[0/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      "\t[1/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      "\t[2/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      " Total time: 0:00:00 (0.0001 s / it)\n",
      "Custom Header\t[0/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      "Custom Header\t[1/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      "Custom Header\t[2/3]\teta: 0:00:00\t\ttime: 0.0000\tdata: 0.0000\n",
      "Custom Header Total time: 0:00:00 (0.0001 s / it)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def test_log_every():\n",
    "    logger = MetricLogger()\n",
    "    \n",
    "    # Test with empty iterable\n",
    "    empty_iterable = []\n",
    "    # Avoid division by zero for empty iterable\n",
    "    test_eq(list(logger.log_every(empty_iterable, 1)), [])\n",
    "    \n",
    "    # Test with single item iterable\n",
    "    single_item = [1]\n",
    "    test_eq(list(logger.log_every(single_item, 1)), [1])\n",
    "    \n",
    "    # Test with print_freq greater than iterable length\n",
    "    small_iterable = [1, 2, 3]\n",
    "    test_eq(list(logger.log_every(small_iterable, 10)), [1, 2, 3])\n",
    "    \n",
    "\t# Test with None header\n",
    "    list(logger.log_every(small_iterable, 1, header=None))\n",
    "    \n",
    "    # Test with custom header\n",
    "    list(logger.log_every(small_iterable, 1, header=\"Custom Header\"))\n",
    "test_log_every() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distibuted Training Setup printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This function sets up a custom printing mechanism for distributed processing environments. Here's what it does:\n",
    " \n",
    " 1. It takes a boolean parameter `is_master` to identify if the current process is the master process.\n",
    " 2. It stores the built-in `print` function in `builtin_print`.\n",
    " 3. It defines a new `print` function that:\n",
    "    - Checks if printing should be forced (either explicitly or when there are more than 8 processes)\n",
    "    - Only prints if it's the master process or if printing is forced\n",
    "    - Adds a timestamp to each print statement\n",
    " 4. It replaces the built-in `print` function with this new custom function.\n",
    " \n",
    " This setup ensures that in distributed environments, only the master process (or forced prints) will output to the console, reducing clutter and improving log readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Setup for distributed processing\n",
    "def setup_for_distributed(is_master):\n",
    "    \"Disable printing for non-master processes in distributed setups\"\n",
    "    builtin_print = builtins.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        force = force or (get_world_size() > 8)\n",
    "        if is_master or force:\n",
    "            now = datetime.datetime.now().time()\n",
    "            builtin_print('[{}] '.format(now), end='')  # print with time stamp\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    builtins.print = print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `is_dist_avail_and_initialized` is a nifty little function that does exactly what it says on the tin. It checks if distributed processing is both available and initialized. This is super handy when you're working with distributed systems and need to quickly verify if everything's set up and ready to go. It's a one-liner that combines two checks into a single, easy-to-read boolean. Remember, in Python, `and` short-circuits, so if `dist.is_available()` is False, it won't even bother checking `dist.is_initialized()`. Efficient and clear - just the way we like it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Test when distributed is not available or initialized\n",
    "def test_get_rank_not_initialized():\n",
    "    test_eq(get_rank(), 0)\n",
    "\n",
    "# Note: We can't easily test the distributed case in this context,\n",
    "# as it would require setting up a distributed environment.\n",
    "# In a real-world scenario, you might want to add more tests\n",
    "# for the distributed case in a separate test suite.\n",
    "\n",
    "test_get_rank_not_initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_distributed_mode(args):\n",
    "    if args.dist_on_itp:\n",
    "        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n",
    "        os.environ['LOCAL_RANK'] = str(args.gpu)\n",
    "        os.environ['RANK'] = str(args.rank)\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n",
    "    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        setup_for_distributed(is_master=True)  # hack\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}): {}, gpu {}'.format(\n",
    "        args.rank, args.dist_url, args.gpu), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Gradients and floating point precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_grad_norm_(\n",
    "\tparameters, \n",
    "\tnorm_type: float = 2.0\n",
    "\t) -> torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tCompute the norm of the gradients for a set of parameters.\n",
    "\t\"\"\"\n",
    "\tif isinstance(parameters, torch.Tensor):\n",
    "\t\tparameters = [parameters]\n",
    "\n",
    "\tparameters = [p for p in parameters if p.grad is not None]\n",
    "\tnorm_type = float(norm_type)\n",
    "\tif len(parameters) == 0:\n",
    "\t\treturn torch.tensor(0.)\n",
    "\tdevice = parameters[0].grad.device\n",
    "\tif norm_type == inf:\n",
    "\t\ttotal_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "\telse:\n",
    "\t\ttotal_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "\treturn total_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def test_get_grad_norm_():\n",
    "\t# Test case 1: Normal case with multiple parameters\n",
    "\tparameters = [torch.nn.Parameter(torch.randn(2, 2)) for _ in range(3)]\n",
    "\tfor p in parameters:\n",
    "\t\tp.grad = torch.randn(2, 2)\n",
    "\t\n",
    "\ttotal_norm = get_grad_norm_(parameters, norm_type=2)\n",
    "\texpected_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2) for p in parameters]), 2)\n",
    "\ttest_close(total_norm, expected_norm)\n",
    "\n",
    "\t# Test case 2: Single tensor parameter\n",
    "\tsingle_param = torch.nn.Parameter(torch.randn(2, 2))\n",
    "\tsingle_param.grad = torch.randn(2, 2)\n",
    "\tsingle_norm = get_grad_norm_(single_param, norm_type=2)\n",
    "\texpected_single_norm = torch.norm(single_param.grad.detach(), 2)\n",
    "\ttest_close(single_norm, expected_single_norm)\n",
    "\n",
    "\t# Test case 3: Empty parameter list\n",
    "\tempty_norm = get_grad_norm_([])\n",
    "\ttest_eq(empty_norm, torch.tensor(0.))\n",
    "\n",
    "\t# Test case 4: Infinity norm\n",
    "\tinf_norm = get_grad_norm_(parameters, norm_type=float('inf'))\n",
    "\texpected_inf_norm = max(p.grad.detach().abs().max() for p in parameters)\n",
    "\ttest_close(inf_norm, expected_inf_norm)\n",
    "\n",
    "\tprint(\"All tests passed!\")\n",
    "\n",
    "test_get_grad_norm_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MagicMock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 73\u001b[0m\n\u001b[1;32m     65\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_checkpoint\u001b[38;5;241m.\u001b[39massert_called_once_with(\n\u001b[1;32m     66\u001b[0m         save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_output\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     67\u001b[0m         tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-10\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m         client_state\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m}\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtest_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 40\u001b[0m, in \u001b[0;36mtest_save_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[1;32m     39\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 40\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMagicMock\u001b[49m()\n\u001b[1;32m     41\u001b[0m model_without_ddp \u001b[38;5;241m=\u001b[39m MagicMock()\n\u001b[1;32m     42\u001b[0m model_without_ddp\u001b[38;5;241m.\u001b[39mstate_dict\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights1\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MagicMock' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def save_model(\n",
    "    args, \n",
    "    epoch, \n",
    "    model, \n",
    "    model_without_ddp, \n",
    "    optimizer, \n",
    "    loss_scaler\n",
    "    ):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    if loss_scaler is not None:\n",
    "        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            to_save = {\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'scaler': loss_scaler.state_dict(),\n",
    "                'args': args,\n",
    "            }\n",
    "\n",
    "            save_on_master(to_save, checkpoint_path)\n",
    "    else:\n",
    "        client_state = {'epoch': epoch}\n",
    "        model.save_checkpoint(\n",
    "            save_dir=args.output_dir, \n",
    "            tag=\"checkpoint-%s\" % epoch_name, \n",
    "            client_state=client_state\n",
    "            )\n",
    "\n",
    "# Test function\n",
    "\n",
    "def test_save_model():\n",
    "    # Mock objects and arguments\n",
    "    class Args:\n",
    "        output_dir = 'test_output'\n",
    "    args = Args()\n",
    "    epoch = 10\n",
    "    model = MagicMock()\n",
    "    model_without_ddp = MagicMock()\n",
    "    # This line uses MagicMock from the unittest.mock module\n",
    "    # To use it, you need to import it at the beginning of the file:\n",
    "    # from unittest.mock import MagicMock\n",
    "    model_without_ddp.state_dict.return_value = {'layer1': 'weights1'}\n",
    "    optimizer = MagicMock()\n",
    "    optimizer.state_dict.return_value = {'param_groups': []}\n",
    "    loss_scaler = MagicMock()\n",
    "    loss_scaler.state_dict.return_value = {'scale': 1.0}\n",
    "\n",
    "    # Test case 1: With loss_scaler\n",
    "    with patch('builtins.open', mock_open()) as mock_file, \\\n",
    "         patch('torch.save') as mock_save, \\\n",
    "         patch('os.makedirs') as mock_makedirs:\n",
    "        save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler)\n",
    "        mock_makedirs.assert_called_once_with('test_output', exist_ok=True)\n",
    "        mock_save.assert_called_once()\n",
    "        saved_dict = mock_save.call_args[0][0]\n",
    "        test_eq(saved_dict['epoch'], 10)\n",
    "        test_eq(saved_dict['model'], {'layer1': 'weights1'})\n",
    "        test_eq(saved_dict['optimizer'], {'param_groups': []})\n",
    "        test_eq(saved_dict['scaler'], {'scale': 1.0})\n",
    "        test_eq(saved_dict['args'], args)\n",
    "\n",
    "    # Test case 2: Without loss_scaler\n",
    "    model.save_checkpoint = MagicMock()\n",
    "    save_model(args, epoch, model, model_without_ddp, optimizer, None)\n",
    "    model.save_checkpoint.assert_called_once_with(\n",
    "        save_dir='test_output',\n",
    "        tag=\"checkpoint-10\",\n",
    "        client_state={'epoch': 10}\n",
    "    )\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
