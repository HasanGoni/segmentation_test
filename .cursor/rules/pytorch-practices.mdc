---
description: 
globs: 
alwaysApply: false
---
# PyTorch Best Practices

## Model Development
- Use `nn.Module` as base class for all models
- Initialize weights properly using `nn.init` functions
- Use `model.train()` and `model.eval()` appropriately
- Implement forward hooks for feature visualization
- Use `torch.jit` for model optimization when possible

## Training Loop Essentials
```python
def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    for batch in dataloader:
        optimizer.zero_grad()
        loss = criterion(model(batch['input']), batch['target'])
        loss.backward()
        optimizer.step()
```

## Memory Management
- Use `torch.cuda.empty_cache()` after large operations
- Implement gradient checkpointing for large models
- Use `pin_memory=True` for GPU training
- Clear variables that are no longer needed

## Debugging Tips
- Use `torch.autograd.detect_anomaly()` for NaN debugging
- Monitor GPU memory with `torch.cuda.memory_summary()`
- Use `torch.nn.utils.clip_grad_norm_` for gradient clipping
- Implement proper learning rate scheduling
