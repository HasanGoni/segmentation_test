[
  {
    "objectID": "tf_model_creation.html",
    "href": "tf_model_creation.html",
    "title": "Creating different Unet models in tensorflow",
    "section": "",
    "text": "different type of unet model from tversky focal loss paper will be implemented here\n\n\nprint(os.environ.get('CUDA_PATH'))\n\n/usr/lib/cuda\n\n\n\nStart with normal unet\n\nsource\n\npooling\n\n pooling (inputs, max_pool_only=False, both=True, pool_size=2)\n\n\nsource\n\n\nconv_block\n\n conv_block (inputs, filter_no, kernel_size, batch_nm=True, dropout=True,\n             drp_rt=0.1, kernel_initializer='glorot_normal')\n\nCreate a conv block with batch norma and dropout\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninputs\n\n\n\n\n\nfilter_no\n\n\n\n\n\nkernel_size\n\n\n\n\n\nbatch_nm\nbool\nTrue\n\n\n\ndropout\nbool\nTrue\n\n\n\ndrp_rt\nfloat\n0.1\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\n\n\ninput_size = tf.random.normal((1, 1152, 1632, 1))\nout_ = conv_block(input_size, 32, 3, batch_nm=False, dropout=False)\ntest_eq([1,1152, 1632, 32], out_.shape)\n\n\nsource\n\n\ndouble_conv\n\n double_conv (inputs, filter_no, kernel_size, batch_nm=True, dropout=True,\n              drp_rt=0.1, kernel_initializer='glorot_normal')\n\nCreate double conv block with batch norma and dropout\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninputs\n\n\n\n\n\nfilter_no\n\n\n\n\n\nkernel_size\n\n\n\n\n\nbatch_nm\nbool\nTrue\n\n\n\ndropout\nbool\nTrue\n\n\n\ndrp_rt\nfloat\n0.1\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\n\n\nout_c = double_conv(inputs=input_size, filter_no=32, kernel_size=3, batch_nm=True, dropout=True, drp_rt=0.1)\nout_p = pooling(out_c, both=True)\n\n\ntest_eq([1,1152, 1632, 32], out_c.shape)\ntest_eq([1,576, 816, 64], out_p.shape)\n\n\nh, w, c = 1152, 1632, 1\n\n\nsource\n\n\nencoder_block\n\n encoder_block (input_size:tensorflow.python.framework.tensor.Tensor,\n                filter_size:List[int]=[16, 32, 64, 128, 256],\n                kernel_initializer:str='glorot_normal')\n\nCreate Encoder block for Unet\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\nTensor\n\n\n\n\nfilter_size\ntyping.List[int]\n[16, 32, 64, 128, 256]\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\n\n\n#h, w, c = 1052, 1632, 1\n#h, w, c = 1152, 1632, 1\ninput_img = tf.random.normal((1, h, w, c))\ninputs,enc_outputs, final_l = encoder_block(\n    input_size=(h, w, c),\n    filter_size=[16,32,64,128],\n    kernel_initializer='glorot_normal' #option he_normal\n    )\n\n\nenc_outputs\n\n[&lt;KerasTensor: shape=(None, 1152, 1632, 16) dtype=float32 (created by layer 'dropout_16')&gt;,\n &lt;KerasTensor: shape=(None, 576, 816, 32) dtype=float32 (created by layer 'dropout_18')&gt;,\n &lt;KerasTensor: shape=(None, 288, 408, 64) dtype=float32 (created by layer 'dropout_20')&gt;,\n &lt;KerasTensor: shape=(None, 144, 204, 128) dtype=float32 (created by layer 'dropout_22')&gt;]\n\n\n\nfinal_l\n\n&lt;KerasTensor: shape=(None, 72, 102, 256) dtype=float32 (created by layer 'dropout_24')&gt;\n\n\n\ntest_eq((None, 1152, 1632, 1), inputs.shape)\n\n\nexpected_shapes = [\n    (None, 1152, 1632, 16),\n    (None, 576, 816, 32),\n    (None, 288, 408, 64),\n    (None, 144, 204, 128),\n]\n\nfor (expected, actual) in zip(expected_shapes, enc_outputs):\n    test_eq(expected, actual.shape)\n\n\nlen(enc_outputs)\n\n4\n\n\n\nsource\n\n\ncrop_and_concat\n\n crop_and_concat (upsampled:tensorflow.python.framework.tensor.Tensor,\n                  skip_connection:tensorflow.python.framework.tensor.Tenso\n                  r)\n\nCrop or pad the skip connection and concatenate it with the upsampled tensor. Args: - upsampled (tf.Tensor): Tensor from the decoder path. - skip_connection (tf.Tensor): Corresponding tensor from the encoder path. Returns: - tf.Tensor: Tensor after cropping or padding and concatenation.\n\nskip_connection = tf.random.normal([1, 16, 16, 128])  # Simulating a feature map from the encoder\nupsampled = tf.random.normal([1, 14, 14, 128])\ncrop_concat_image = crop_and_concat(upsampled, skip_connection)\ntest_eq(crop_concat_image.shape, [1, 14, 14, 256])\n\n Cropping branch\n\n\n\nsource\n\n\ndecoder_block\n\n decoder_block (filter_list:List[int],\n                encoder_outputs:List[tensorflow.python.framework.tensor.Te\n                nsor],\n                bottleneck:tensorflow.python.framework.tensor.Tensor)\n\nCreate a decoder block for unet\n\ninputs,enc_outputs, final_l = encoder_block(\n    input_size=(h, w, c),\n    filter_size=[16,32,64,128],\n    kernel_initializer='glorot_normal' #option he_normal\n    )\nout_, dec_out_list = decoder_block(\n    filter_list=[16,32,64,128],\n    encoder_outputs=enc_outputs,\n    bottleneck=final_l,\n    )\n\n\nexpected_decoder_shapes = [\n    (None, 144, 204, 256),\n    (None, 288, 408, 128),\n    (None, 576, 816, 64),\n    (None, 1152, 1632, 32),\n]\n\nfor (expected, actual) in zip(expected_decoder_shapes, dec_out_list):\n    test_eq(expected, actual.shape)\ntest_eq((None, 1152, 1632, 16), out_.shape)\n\n\nsource\n\n\nunet_model\n\n unet_model (input_size:Tuple[int]=(1152, 1632, 1),\n             filter_list:List[int]=[16, 32, 64, 128],\n             kernel_initializer:str='glorot_normal', n_classes:int=1)\n\nCreate a unet model\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\ntyping.Tuple[int]\n(1152, 1632, 1)\n\n\n\nfilter_list\ntyping.List[int]\n[16, 32, 64, 128]\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\nn_classes\nint\n1\n\n\n\n\n\nmodel_ = unet_model(\n    input_size=(1152, 1632, 1),\n    filter_list=[16,32,64,128],\n    n_classes=1,\n    )\n\n\ntf.keras.utils.plot_model(model_, show_shapes=True, show_layer_activations=True)\n\n\n\n\n\n\n\nAttention Unet\n\nsource\n\nunet_gating_signal\n\n unet_gating_signal (input:tensorflow.python.framework.tensor.Tensor,\n                     out_size:int, is_batchnorm:bool, name:str,\n                     kinit:str='glorot_normal')\n\nthis is simply 1x1 convolution, bn, activation\n\nsource\n\n\nexpend_as\n\n expend_as (tensor, rep, name)\n\n\nsource\n\n\nattention_blocks\n\n attention_blocks (x, g, inter_shape, name)\n\ntake g which is the spatially smaller signal, do a conv to get the same number of feature channels as x (bigger spatially) do a conv on x to also get same feature channels (theta_x) then, upsample g to be same size as x add x and g (concat_xg) relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients\n\nsource\n\n\ndecoder_block_attention_gates\n\n decoder_block_attention_gates (filter_list:List[int],\n                                encoder_outputs:List[tensorflow.python.fra\n                                mework.tensor.Tensor], bottleneck:tensorfl\n                                ow.python.framework.tensor.Tensor)\n\nCreate a decoder block for unet with attention gates\n\ninputs, encoder_output, bottle_neck_enc = encoder_block(\n    input_size=(256, 256,1),\n    filter_size=[64,128,256,512],\n    )\ndec_final, decoder_outputs = decoder_block_attention_gates(\n    filter_list=[64,128,256,512],\n    encoder_outputs=encoder_output,\n    bottleneck=bottle_neck_enc,\n    )\n\ntesting decoder output shape\n\nexpected_attn_deocder_shapes = [\n    (None, 32, 32, 512),\n    (None, 64, 64, 256),\n    (None, 128, 128, 128),\n    (None, 256, 256, 64),\n]\nfor (expected, actual) in zip(expected_attn_deocder_shapes, decoder_outputs):\n    test_eq(expected, actual.shape)\n\n\nsource\n\n\nunet_model_attention_gates\n\n unet_model_attention_gates (input_size:Tuple[int]=(256, 256, 1),\n                             filter_list:List[int]=[64, 128, 256, 512],\n                             kernel_initializer:str='glorot_normal',\n                             n_classes:int=1)\n\nCreate a unet model\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\ntyping.Tuple[int]\n(256, 256, 1)\n\n\n\nfilter_list\ntyping.List[int]\n[64, 128, 256, 512]\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\nn_classes\nint\n1\n\n\n\n\n\nattn_unet_model_ =unet_model_attention_gates(\n    input_size=(256, 256, 1),\n    filter_list=[64,128,256,512],\n    n_classes=1,\n    )\n\n\ntf.keras.utils.plot_model(attn_unet_model_, show_shapes=True, show_layer_activations=True)\n\n\n\n\n\n\n\nResidual Attention Unet\n\nsource\n\nres_double_conv\n\n res_double_conv (inputs, filter_no, kernel_size, batch_nm=True,\n                  dropout=True, drp_rt=0.1,\n                  kernel_initializer='glorot_normal')\n\nCreate double conv block with batch norma and dropout\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninputs\n\n\n\n\n\nfilter_no\n\n\n\n\n\nkernel_size\n\n\n\n\n\nbatch_nm\nbool\nTrue\n\n\n\ndropout\nbool\nTrue\n\n\n\ndrp_rt\nfloat\n0.1\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\n\n\nsource\n\n\nresidual_encoder_block\n\n residual_encoder_block\n                         (input_size:tensorflow.python.framework.tensor.Te\n                         nsor, filter_size:List[int]=[16, 32, 64, 128,\n                         256], kernel_initializer:str='glorot_normal')\n\nCreate Encoder block for residual Unet\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\nTensor\n\n\n\n\nfilter_size\ntyping.List[int]\n[16, 32, 64, 128, 256]\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\n\n\nsource\n\n\nresidual_decoder_block\n\n residual_decoder_block (filter_list:List[int],\n                         encoder_outputs:List[tensorflow.python.framework.\n                         tensor.Tensor], bottleneck:tensorflow.python.fram\n                         ework.tensor.Tensor)\n\nCreate a decoder block for unet with attention gates\n\nsource\n\n\nresidual_attn_unet\n\n residual_attn_unet (input_size:Tuple[int]=(256, 256, 1),\n                     filter_list:List[int]=[64, 128, 256, 512],\n                     kernel_initializer:str='glorot_normal',\n                     n_classes:int=1)\n\nCreate a residual attention unet model\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_size\ntyping.Tuple[int]\n(256, 256, 1)\n\n\n\nfilter_list\ntyping.List[int]\n[64, 128, 256, 512]\n\n\n\nkernel_initializer\nstr\nglorot_normal\noption he_normal\n\n\nn_classes\nint\n1\n\n\n\n\n\nresdual_attn_unet = residual_attn_unet(\n    input_size=(256, 256, 1),\n    filter_list=[64,128,256,512],\n    n_classes=1,\n    )\n\n\ntf.keras.utils.plot_model(resdual_attn_unet, show_shapes=True, show_layer_activations=True)\n\n\n\n\n\n\n\nChecking purpose\n\nfrom tensorflow.keras import layers\n\n\ndef res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n    '''\n    Residual convolutional layer.\n    Two variants....\n    Either put activation function before the addition with shortcut\n    or after the addition (which would be as proposed in the original resNet).\n    \n    1. conv - BN - Activation - conv - BN - Activation \n                                          - shortcut  - BN - shortcut+BN\n                                          \n    2. conv - BN - Activation - conv - BN   \n                                     - shortcut  - BN - shortcut+BN - Activation                                     \n    \n    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n    '''\n\n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv)\n    conv = layers.Activation('relu')(conv)\n    \n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv)\n    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n    if dropout &gt; 0:\n        conv = layers.Dropout(dropout)(conv)\n\n    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n    if batch_norm is True:\n        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n\n    res_path = layers.add([shortcut, conv])\n    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n    return res_path\n\n\ndef gating_signal(input, out_size, batch_norm=False):\n    \"\"\"\n    resize the down layer feature map into the same dimension as the up layer feature map\n    using 1x1 conv\n    :return: the gating feature map with the same dimension of the up layer feature map\n    \"\"\"\n    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n    if batch_norm:\n        x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    return x\n\ndef attention_block(x, gating, inter_shape):\n    shape_x = K.int_shape(x)\n    shape_g = K.int_shape(gating)\n\n# Getting the x signal to the same shape as the gating signal\n    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n    shape_theta_x = K.int_shape(theta_x)\n\n# Getting the gating signal to the same number of filters as the inter_shape\n    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)\n    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n                                 padding='same')(phi_g)  # 16\n\n    concat_xg = layers.add([upsample_g, theta_x])\n    act_xg = layers.Activation('relu')(concat_xg)\n    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n    sigmoid_xg = layers.Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n\n    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n\n    y = layers.multiply([upsample_psi, x])\n\n    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n    result_bn = layers.BatchNormalization()(result)\n    return result_bn\n\n\ndef repeat_elem(tensor, rep):\n    # lambda function to repeat Repeats the elements of a tensor along an axis\n    #by a factor of rep.\n    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape \n    #(None, 256,256,6), if specified axis=3 and rep=2.\n\n     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n                          arguments={'repnum': rep})(tensor)\n\ndef Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n    '''\n    Rsidual UNet, with attention \n    \n    '''\n    # network structure\n    FILTER_NUM = 64 # number of basic filters for the first layer\n    FILTER_SIZE = 3 # size of the convolutional filter\n    UP_SAMP_SIZE = 2 # size of upsampling filters\n    # input data\n    # dimension of the image depth\n    inputs = layers.Input(input_shape, dtype=tf.float32)\n    axis = 3\n\n    # Downsampling layers\n    # DownRes 1, double residual convolution + pooling\n    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n    # DownRes 2\n    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n    # DownRes 3\n    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n    # DownRes 4\n    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n    # DownRes 5, convolution only\n    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n\n    # Upsampling layers\n    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n    # UpRes 7\n    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n    # UpRes 8\n    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n    # UpRes 9\n    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n\n    # 1*1 convolutional layers\n    \n    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n\n    # Model integration\n    model = tf.keras.models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n    return model\n\ninput_shape = (256, 256, 1)\nattn_res_unet_ = Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True)\ntf.keras.utils.plot_model(attn_res_unet_, show_shapes=True, show_layer_activations=True)"
  },
  {
    "objectID": "05_pytorch_ds_creation.html",
    "href": "05_pytorch_ds_creation.html",
    "title": "segmentation_test",
    "section": "",
    "text": "# Create pytorch dataset from hf dataset\n\n\n\nfirst download hugingface dataset and then convert it pytorch dataset with augmentation, to work further\n\n\nIMAGE_HEGHT = 256\nIMAGE_WIDTH = 256\nIN_CHANNELS = 1\nOUT_CHANNELS = 1\n\n\nsource\n\nshow_hf_dataset\n\n show_hf_dataset (dataset:datasets.arrow_dataset.Dataset,\n                  idx:Optional[int]=None, split:str='train')\n\nShow hugging face random index\nNow testing\n\nshow_hf_dataset(dataset)\n\n dataset index will be visualized: 106\n\n\n\n\n\n\n#msk_no = len(dataset['train']['label'])\n#mask_max =[np.max(np.array(dataset['train']['label'][i])) for i in range(msk_no)]\n\n\ndef visualize_map(image, segmentation_map):\n    color_seg = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 1), dtype=np.uint8) # height, width, 3\n    for label, color in id2color.items():\n        color_seg[segmentation_map == label, :] = color\n\n\nsource\n\n\nSegmentationDataset\n\n SegmentationDataset (dataset, transform)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nDetails\n\n\n\n\ndataset\n\n\n\ntransform\nTransformations\n\n\n\n\nMEAN =0\nSTD = 1\ntrain_transform = A.Compose([\n    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEGHT),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=MEAN, std=STD),\n    \n])\n\nval_transform = A.Compose([\n    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEGHT),\n    A.Normalize(mean=MEAN, std=STD),\n\n])\n\n\n#ds_test = load_dataset(\"EduardoPacheco/FoodSeg103\")\n\n\n#msk=ds_test['train']['label'][0]\n\n\n#np.array(msk).shape\n\n\n#msk\n\n\nid2label = {0: 'background', 1: 'Pin'}\n\n\npixel_values, target, original_image, original_segmentation_map = train_dataset[3]\nprint(pixel_values.shape)\nprint(target.shape)\n\ntorch.Size([256, 256, 1])\ntorch.Size([1, 256, 256])\ntorch.Size([256, 256])\n\n\n\ndef collate_fn(inputs):\n    batch = dict()\n    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n    batch[\"original_images\"] = [i[2] for i in inputs]\n    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n\n    return batch\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2)\nval_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2)\n\n\nbatch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  if isinstance(v,torch.Tensor):\n    print(k,v.shape)\n\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\ntorch.Size([256, 256, 1])\npixel_values torch.Size([2, 1, 256, 256])\nlabels torch.Size([2, 256, 256])\n\n\n\nbatch[\"pixel_values\"].dtype, batch[\"labels\"].dtype\n\n(torch.float32, torch.int64)\n\n\n\nunnormalized_image = batch[\"pixel_values\"][0].numpy() * np.array(STD)+ np.array(MEAN)\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nprint(unnormalized_image.shape)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nprint(unnormalized_image.shape)\nunnormalized_image = Image.fromarray(unnormalized_image[:,:,0])\nunnormalized_image\n\n(1, 256, 256)\n(256, 256, 1)\n\n\n\n\n\n\nsource\n\n\nUNetSmall\n\n UNetSmall (in_channels, out_channels)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTverskyFocalLoss\n\n TverskyFocalLoss (alpha=0.7, beta=0.3, gamma=2.0, epsilon=1e-05)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# creating dataset\ntrain_dataset = SegmentationDataset(dataset[\"train\"], transform=train_transform)\nval_dataset = SegmentationDataset(dataset[\"test\"], transform=val_transform)\n\n# now dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2)\nval_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2)\n\n\ndef save_model(model, save_path):\n    Path(save_path).parent.mkdir(exist_ok=True, parents=True)\n    torch.save(model.state_dict(), save_path)\n\n\nsource\n\n\ntrain\n\n train (train_dataloader:torch.utils.data.dataloader.DataLoader,\n        val_dataloader:torch.utils.data.dataloader.DataLoader,\n        optimizer:torch.optim.optimizer.Optimizer,\n        model:torch.nn.modules.module.Module,\n        criterion:torch.nn.modules.module.Module, device:torch.device,\n        epochs:int, save_path:str)\n\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f'Using device: {device}')\n\nUsing device: cuda\n\n\n\n# creating dataset\ntrain_dataset = SegmentationDataset(dataset[\"train\"], transform=train_transform)\nval_dataset = SegmentationDataset(dataset[\"test\"], transform=val_transform)\n\n# now dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2)\nval_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2)\n\n\nmodel = UNetSmall(IN_CHANNELS, OUT_CHANNELS).to(device)\ncriterion = TverskyFocalLoss(alpha=0.7, beta=0.3, gamma=2.0)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n\n\nfor name, param in model.named_parameters():\n    print(name)\n\nencoder.0.weight\nencoder.0.bias\nencoder.2.weight\nencoder.2.bias\nmiddle.0.weight\nmiddle.0.bias\ndecoder.0.weight\ndecoder.0.bias\ndecoder.2.weight\ndecoder.2.bias\ndecoder.4.weight\ndecoder.4.bias\n\n\n\nbatch = next(iter(train_dataloader))\n\n\nbatch['labels'].shape\n\ntorch.Size([2, 1, 256, 256])\n\n\n\nlogits=model(batch['pixel_values'].to(device))\nlogits.shape\ncriterion(logits, batch['labels'].to(device))\n\ntensor(0.9611, device='cuda:0', grad_fn=&lt;MeanBackward0&gt;)\n\n\n\ntrain(\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    optimizer=optimizer,\n    criterion=criterion,\n    model=model,\n    device=device,\n    epochs=1,\n    save_path='model.pth'\n\n\n)\n\nepoch: 1/1\nEpoch [1/1], Loss: 0.0003, Learning Rate: 0.00001000\nEpoch [1/1], Loss: 0.0003, Val Loss: 0.0003\n\n\n\n\n\n\n\n\n\ndef load_model(model, checkpoint_path):\n    model.load_state_dict(torch.load(checkpoint_path))\n    model.eval()  # Set the model to evaluation mode\n    return model\n\n\ncheckpoint_path = 'model.pth'\n\n\nloaded_model = load_model(UNetSmall(IN_CHANNELS, OUT_CHANNELS), checkpoint_path)\nloaded_model = loaded_model.to(device)\n\n\ninf_img = val_transform(image=np.array(dataset['test']['image'][0]))\ninf_img['image'].shape\n\n(256, 256)\n\n\n\nnew_img=(inf_img['image'])\n\n\ninp_img_infr = np.expand_dims(np.expand_dims(new_img,axis=-1),0).shape\n\n\nloaded_model(torch.tensor(np.expand_dims(np.expand_dims(new_img,axis=-1),0)).to(device))\n\nRuntimeError: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 256, 256, 1] to have 1 channels, but got 256 channels instead\n\n\n\ninp_img_infr=inp_img_infr.to(device)\n\nAttributeError: 'tuple' object has no attribute 'to'"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "path = Path(r'/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224')\npath.ls()\n\n(#3) [Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/models'),Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/X'),Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/y')]\n\n\n\nsource\n\nPreprocess\n\n Preprocess (image_path:Union[pathlib.Path,str],\n             label_path:Union[pathlib.Path,str], im_height:int=224,\n             im_width:int=224, bf_size:int=30, bs:int=8,\n             one_channel:bool=False, test_size:float=0.2)\n\n\npreprocess_obj = Preprocess(\n                           image_path=path/'X',\n                           label_path=path/'y',\n                           im_height=IMAGE_HEIGHT,\n                           im_width=IMAGE_WIDTH,\n                           bs=BATCH_SIZE,\n                           one_channel=False,\n                           test_size=0.2\n                           )\n\n\nAugmentation needs to be done before resizing the image - - - - - - - - - - - - - - - - -\n\n\nsource\n\n\nPreprocess.show_image\n\n Preprocess.show_image (im_file)\n\n\nsource\n\n\nPreprocess.read_image\n\n Preprocess.read_image (im_file, one_channel=False)\n\n\nfn =str((path/'X').ls()[0])\nfn_lbl = str((path/'y').ls()[0])\nimg_ = preprocess_obj.read_image(fn, one_channel=True)\ntest_eq(img_.numpy().shape, (1024, 1224, 1))\nimg_ = preprocess_obj.read_image(fn, one_channel=False)\nlbl_ = preprocess_obj.read_image(fn_lbl, one_channel=False)\ntest_eq(img_.numpy().shape, (1024, 1224, 3))\n\n2023-04-18 10:54:58.979576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:54:58.996838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:54:58.997556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:54:58.999003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-18 10:54:59.003367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:54:59.004910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:54:59.005818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:55:00.242731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:55:00.243227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:55:00.243526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n2023-04-18 10:55:00.243628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2023-04-18 10:55:00.243890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1189 MB memory:  -&gt; device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:01:00.0, compute capability: 7.5\n\n\n\nsource\n\n\nPreprocess.augmentation_\n\n Preprocess.augmentation_ (im_height:int, im_width:int,\n                           image:tensorflow.python.framework.tensor.Tensor\n                           ,\n                           mask:tensorflow.python.framework.tensor.Tensor)\n\n\nsource\n\n\nPreprocess.show_aug\n\n Preprocess.show_aug (image, mask, original_image=None,\n                      original_mask=None)\n\n\nsource\n\n\nPreprocess.read_aug\n\n Preprocess.read_aug (im_file:str, lbl_file:str, one_channel:bool=False,\n                      aug:bool=False)\n\n\nim, msk = preprocess_obj.read_aug(im_file=fn, lbl_file=fn_lbl, one_channel=False, aug=True)\nim.shape, msk.shape\ntest_eq_type(type(im), np.ndarray), test_eq_type(type(msk), np.ndarray);\n\n\n#preprocess_obj.show_aug(image=im, mask=msk, original_image=img_.numpy(), original_mask=lbl_.numpy())\n\n\nsource\n\n\nPreprocess.normalize\n\n Preprocess.normalize\n                       (image:Union[numpy.ndarray,tensorflow.python.framew\n                       ork.tensor.Tensor], min=0)\n\n\nim.shape\n\n(1224, 1024, 3)\n\n\n\nnorm_img = preprocess_obj.normalize(im)\nnp_img_ = norm_img.numpy()\nnp_img_.min(), im.min(), np_img_.max(), im.max()\n#test_eq(np_img_.min(),0.0), test_eq(np_img_.max(),1.0);\n\n(0.0, 0, 0.87058824, 222)\n\n\n\nsource\n\n\nPreprocess.process_image_and_mask\n\n Preprocess.process_image_and_mask (im_file:str, lbl_file:str,\n                                    norm:bool=True,\n                                    one_channel:bool=False,\n                                    aug_data:bool=True)\n\n\nim, msk = preprocess_obj.process_image_and_mask(im_file=fn, lbl_file=fn_lbl, norm=True, one_channel=False)\n\n\ntype(im),  type(msk)\n\n(tensorflow.python.framework.ops.EagerTensor,\n tensorflow.python.framework.ops.EagerTensor)\n\n\n\ntest_eq(im.numpy().shape, (IMAGE_HEIGHT,IMAGE_WIDTH,3))\ntest_eq(msk.numpy().shape, (IMAGE_HEIGHT,IMAGE_WIDTH,3))\n\n\n\nCreate dataset\n\nsource\n\nPreprocess.process_data\n\n Preprocess.process_data (image, label, norm:bool=True,\n                          one_channel:bool=False, aug_data:bool=True)\n\n\nsource\n\n\nPreprocess.set_shapes\n\n Preprocess.set_shapes (img, label, img_shape)\n\n\nsource\n\n\nPreprocess.create_dataset\n\n Preprocess.create_dataset (images, labels, train:bool=True,\n                            norm:bool=True, aug:bool=True)\n\n\nsource\n\n\nPreprocess.create_train_test_dataset\n\n Preprocess.create_train_test_dataset ()\n\n\none_batch = next(iter(train_ds))\n\n2023-04-18 10:55:42.416394: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\nimages_ds, masks_ds = one_batch\n\n\nsource\n\n\nconvert_np_and_uint8\n\n convert_np_and_uint8 (img:tensorflow.python.framework.tensor.Tensor)\n\nConvert img to np.array and uint8\n\nsource\n\n\nconvert_one_channel\n\n convert_one_channel (img:tensorflow.python.framework.tensor.Tensor)\n\nConvert image to one channel\n\nsource\n\n\nPreprocess.create_color_mask\n\n Preprocess.create_color_mask (mask:Union[&lt;built-\n                               infunctionarray&gt;,tensorflow.python.framewor\n                               k.tensor.Tensor], img:Union[&lt;built-infuncti\n                               onarray&gt;,tensorflow.python.framework.tensor\n                               .Tensor], threshold:float=0.5)\n\nCreating color mask for segmentation\n\nsource\n\n\nPreprocess.display_np_batch\n\n Preprocess.display_np_batch (images:numpy.ndarray, masks:numpy.ndarray,\n                              threshold:float=0.5)\n\nDisplaying batch of images and masks\n\nsource\n\n\nPreprocess.display_ds\n\n Preprocess.display_ds\n                        (ds:tensorflow.python.data.ops.dataset_ops.Dataset\n                        V2)\n\n\n#preprocess_obj.display_ds(train_ds)\n\n\nsource\n\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Segmentation related functions will be developed here",
    "section": "",
    "text": "git clone &lt;repo&gt;\npip install -e ."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Segmentation related functions will be developed here",
    "section": "",
    "text": "git clone &lt;repo&gt;\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Segmentation related functions will be developed here",
    "section": "How to use",
    "text": "How to use\n\nfrom segmentation_test.tf_model_creation import *\n\n2023-11-26 20:19:15.671190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-11-26 20:19:15.671221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-11-26 20:19:15.671942: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-26 20:19:15.677295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-26 20:19:16.258260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\nh,w,c = 1152, 1632,1\nn_classes = 1\nfilter_list=[64, 128, 256, 512]"
  },
  {
    "objectID": "index.html#create-a-normal-unet",
    "href": "index.html#create-a-normal-unet",
    "title": "Segmentation related functions will be developed here",
    "section": "Create a Normal UNet",
    "text": "Create a Normal UNet\n\nnormal_unet_model = unet_model(\n    input_size=(h, w, c),\n    filter_list=filter_list,\n    n_classes=n_classes\n)\n\n\nprint(normal_unet_model.summary())\n\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_4 (InputLayer)        [(None, 1152, 1632, 1)]      0         []                            \n                                                                                                  \n conv2d_21 (Conv2D)          (None, 1152, 1632, 64)       640       ['input_4[0][0]']             \n                                                                                                  \n batch_normalization_18 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_21[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_18 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_18[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_18 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_18[0][0]']       \n                                                                                                  \n conv2d_22 (Conv2D)          (None, 1152, 1632, 64)       36928     ['dropout_18[0][0]']          \n                                                                                                  \n batch_normalization_19 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_22[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_19 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_19[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_19 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_19[0][0]']       \n                                                                                                  \n max_pooling2d_4 (MaxPoolin  (None, 576, 816, 64)         0         ['dropout_19[0][0]']          \n g2D)                                                                                             \n                                                                                                  \n average_pooling2d_4 (Avera  (None, 576, 816, 64)         0         ['dropout_19[0][0]']          \n gePooling2D)                                                                                     \n                                                                                                  \n concatenate_8 (Concatenate  (None, 576, 816, 128)        0         ['max_pooling2d_4[0][0]',     \n )                                                                   'average_pooling2d_4[0][0]'] \n                                                                                                  \n conv2d_23 (Conv2D)          (None, 576, 816, 128)        147584    ['concatenate_8[0][0]']       \n                                                                                                  \n batch_normalization_20 (Ba  (None, 576, 816, 128)        512       ['conv2d_23[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_20 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_20[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_20 (Dropout)        (None, 576, 816, 128)        0         ['activation_20[0][0]']       \n                                                                                                  \n conv2d_24 (Conv2D)          (None, 576, 816, 128)        147584    ['dropout_20[0][0]']          \n                                                                                                  \n batch_normalization_21 (Ba  (None, 576, 816, 128)        512       ['conv2d_24[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_21 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_21[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_21 (Dropout)        (None, 576, 816, 128)        0         ['activation_21[0][0]']       \n                                                                                                  \n max_pooling2d_5 (MaxPoolin  (None, 288, 408, 128)        0         ['dropout_21[0][0]']          \n g2D)                                                                                             \n                                                                                                  \n average_pooling2d_5 (Avera  (None, 288, 408, 128)        0         ['dropout_21[0][0]']          \n gePooling2D)                                                                                     \n                                                                                                  \n concatenate_9 (Concatenate  (None, 288, 408, 256)        0         ['max_pooling2d_5[0][0]',     \n )                                                                   'average_pooling2d_5[0][0]'] \n                                                                                                  \n conv2d_25 (Conv2D)          (None, 288, 408, 256)        590080    ['concatenate_9[0][0]']       \n                                                                                                  \n batch_normalization_22 (Ba  (None, 288, 408, 256)        1024      ['conv2d_25[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_22 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_22[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_22 (Dropout)        (None, 288, 408, 256)        0         ['activation_22[0][0]']       \n                                                                                                  \n conv2d_26 (Conv2D)          (None, 288, 408, 256)        590080    ['dropout_22[0][0]']          \n                                                                                                  \n batch_normalization_23 (Ba  (None, 288, 408, 256)        1024      ['conv2d_26[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_23 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_23[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_23 (Dropout)        (None, 288, 408, 256)        0         ['activation_23[0][0]']       \n                                                                                                  \n max_pooling2d_6 (MaxPoolin  (None, 144, 204, 256)        0         ['dropout_23[0][0]']          \n g2D)                                                                                             \n                                                                                                  \n average_pooling2d_6 (Avera  (None, 144, 204, 256)        0         ['dropout_23[0][0]']          \n gePooling2D)                                                                                     \n                                                                                                  \n concatenate_10 (Concatenat  (None, 144, 204, 512)        0         ['max_pooling2d_6[0][0]',     \n e)                                                                  'average_pooling2d_6[0][0]'] \n                                                                                                  \n conv2d_27 (Conv2D)          (None, 144, 204, 512)        2359808   ['concatenate_10[0][0]']      \n                                                                                                  \n batch_normalization_24 (Ba  (None, 144, 204, 512)        2048      ['conv2d_27[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_24 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_24[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_24 (Dropout)        (None, 144, 204, 512)        0         ['activation_24[0][0]']       \n                                                                                                  \n conv2d_28 (Conv2D)          (None, 144, 204, 512)        2359808   ['dropout_24[0][0]']          \n                                                                                                  \n batch_normalization_25 (Ba  (None, 144, 204, 512)        2048      ['conv2d_28[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_25 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_25[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_25 (Dropout)        (None, 144, 204, 512)        0         ['activation_25[0][0]']       \n                                                                                                  \n max_pooling2d_7 (MaxPoolin  (None, 72, 102, 512)         0         ['dropout_25[0][0]']          \n g2D)                                                                                             \n                                                                                                  \n average_pooling2d_7 (Avera  (None, 72, 102, 512)         0         ['dropout_25[0][0]']          \n gePooling2D)                                                                                     \n                                                                                                  \n concatenate_11 (Concatenat  (None, 72, 102, 1024)        0         ['max_pooling2d_7[0][0]',     \n e)                                                                  'average_pooling2d_7[0][0]'] \n                                                                                                  \n conv2d_29 (Conv2D)          (None, 72, 102, 1024)        9438208   ['concatenate_11[0][0]']      \n                                                                                                  \n batch_normalization_26 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_29[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_26 (Activation)  (None, 72, 102, 1024)        0         ['batch_normalization_26[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_26 (Dropout)        (None, 72, 102, 1024)        0         ['activation_26[0][0]']       \n                                                                                                  \n conv2d_30 (Conv2D)          (None, 72, 102, 1024)        9438208   ['dropout_26[0][0]']          \n                                                                                                  \n batch_normalization_27 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_30[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_27 (Activation)  (None, 72, 102, 1024)        0         ['batch_normalization_27[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_27 (Dropout)        (None, 72, 102, 1024)        0         ['activation_27[0][0]']       \n                                                                                                  \n conv2d_transpose_4 (Conv2D  (None, 144, 204, 512)        2097664   ['dropout_27[0][0]']          \n Transpose)                                                                                       \n                                                                                                  \n concatenate_12 (Concatenat  (None, 144, 204, 1024)       0         ['conv2d_transpose_4[0][0]',  \n e)                                                                  'dropout_25[0][0]']          \n                                                                                                  \n conv2d_31 (Conv2D)          (None, 144, 204, 512)        4719104   ['concatenate_12[0][0]']      \n                                                                                                  \n batch_normalization_28 (Ba  (None, 144, 204, 512)        2048      ['conv2d_31[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_28 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_28[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_28 (Dropout)        (None, 144, 204, 512)        0         ['activation_28[0][0]']       \n                                                                                                  \n conv2d_32 (Conv2D)          (None, 144, 204, 512)        2359808   ['dropout_28[0][0]']          \n                                                                                                  \n batch_normalization_29 (Ba  (None, 144, 204, 512)        2048      ['conv2d_32[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_29 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_29[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_29 (Dropout)        (None, 144, 204, 512)        0         ['activation_29[0][0]']       \n                                                                                                  \n conv2d_transpose_5 (Conv2D  (None, 288, 408, 256)        524544    ['dropout_29[0][0]']          \n Transpose)                                                                                       \n                                                                                                  \n concatenate_13 (Concatenat  (None, 288, 408, 512)        0         ['conv2d_transpose_5[0][0]',  \n e)                                                                  'dropout_23[0][0]']          \n                                                                                                  \n conv2d_33 (Conv2D)          (None, 288, 408, 256)        1179904   ['concatenate_13[0][0]']      \n                                                                                                  \n batch_normalization_30 (Ba  (None, 288, 408, 256)        1024      ['conv2d_33[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_30 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_30[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_30 (Dropout)        (None, 288, 408, 256)        0         ['activation_30[0][0]']       \n                                                                                                  \n conv2d_34 (Conv2D)          (None, 288, 408, 256)        590080    ['dropout_30[0][0]']          \n                                                                                                  \n batch_normalization_31 (Ba  (None, 288, 408, 256)        1024      ['conv2d_34[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_31 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_31[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_31 (Dropout)        (None, 288, 408, 256)        0         ['activation_31[0][0]']       \n                                                                                                  \n conv2d_transpose_6 (Conv2D  (None, 576, 816, 128)        131200    ['dropout_31[0][0]']          \n Transpose)                                                                                       \n                                                                                                  \n concatenate_14 (Concatenat  (None, 576, 816, 256)        0         ['conv2d_transpose_6[0][0]',  \n e)                                                                  'dropout_21[0][0]']          \n                                                                                                  \n conv2d_35 (Conv2D)          (None, 576, 816, 128)        295040    ['concatenate_14[0][0]']      \n                                                                                                  \n batch_normalization_32 (Ba  (None, 576, 816, 128)        512       ['conv2d_35[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_32 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_32[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_32 (Dropout)        (None, 576, 816, 128)        0         ['activation_32[0][0]']       \n                                                                                                  \n conv2d_36 (Conv2D)          (None, 576, 816, 128)        147584    ['dropout_32[0][0]']          \n                                                                                                  \n batch_normalization_33 (Ba  (None, 576, 816, 128)        512       ['conv2d_36[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_33 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_33[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_33 (Dropout)        (None, 576, 816, 128)        0         ['activation_33[0][0]']       \n                                                                                                  \n conv2d_transpose_7 (Conv2D  (None, 1152, 1632, 64)       32832     ['dropout_33[0][0]']          \n Transpose)                                                                                       \n                                                                                                  \n concatenate_15 (Concatenat  (None, 1152, 1632, 128)      0         ['conv2d_transpose_7[0][0]',  \n e)                                                                  'dropout_19[0][0]']          \n                                                                                                  \n conv2d_37 (Conv2D)          (None, 1152, 1632, 64)       73792     ['concatenate_15[0][0]']      \n                                                                                                  \n batch_normalization_34 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_37[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_34 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_34[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_34 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_34[0][0]']       \n                                                                                                  \n conv2d_38 (Conv2D)          (None, 1152, 1632, 64)       36928     ['dropout_34[0][0]']          \n                                                                                                  \n batch_normalization_35 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_38[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_35 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_35[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_35 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_35[0][0]']       \n                                                                                                  \n conv2d_39 (Conv2D)          (None, 1152, 1632, 1)        65        ['dropout_35[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 37321025 (142.37 MB)\nTrainable params: 37309249 (142.32 MB)\nNon-trainable params: 11776 (46.00 KB)\n__________________________________________________________________________________________________\nNone"
  },
  {
    "objectID": "index.html#create-a-u-net-with-attention",
    "href": "index.html#create-a-u-net-with-attention",
    "title": "Segmentation related functions will be developed here",
    "section": "Create a U-Net with Attention",
    "text": "Create a U-Net with Attention\n\nattn_unet_model = unet_model_attention_gates(\n    input_size=(h, w, c),\n    filter_list=filter_list,\n    n_classes=n_classes\n)\n\n\nprint(attn_unet_model.summary())\n\nModel: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_5 (InputLayer)        [(None, 1152, 1632, 1)]      0         []                            \n                                                                                                  \n conv2d_40 (Conv2D)          (None, 1152, 1632, 64)       640       ['input_5[0][0]']             \n                                                                                                  \n batch_normalization_36 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_40[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_36 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_36[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_36 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_36[0][0]']       \n                                                                                                  \n conv2d_41 (Conv2D)          (None, 1152, 1632, 64)       36928     ['dropout_36[0][0]']          \n                                                                                                  \n batch_normalization_37 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_41[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_37 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_37[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_37 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_37[0][0]']       \n                                                                                                  \n max_pooling2d_8 (MaxPoolin  (None, 576, 816, 64)         0         ['dropout_37[0][0]']          \n g2D)                                                                                             \n                                                                                                  \n average_pooling2d_8 (Avera  (None, 576, 816, 64)         0         ['dropout_37[0][0]']          \n gePooling2D)                                                                                     \n                                                                                                  \n concatenate_16 (Concatenat  (None, 576, 816, 128)        0         ['max_pooling2d_8[0][0]',     \n e)                                                                  'average_pooling2d_8[0][0]'] \n                                                                                                  \n conv2d_42 (Conv2D)          (None, 576, 816, 128)        147584    ['concatenate_16[0][0]']      \n                                                                                                  \n batch_normalization_38 (Ba  (None, 576, 816, 128)        512       ['conv2d_42[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_38 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_38[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_38 (Dropout)        (None, 576, 816, 128)        0         ['activation_38[0][0]']       \n                                                                                                  \n conv2d_43 (Conv2D)          (None, 576, 816, 128)        147584    ['dropout_38[0][0]']          \n                                                                                                  \n batch_normalization_39 (Ba  (None, 576, 816, 128)        512       ['conv2d_43[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_39 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_39[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_39 (Dropout)        (None, 576, 816, 128)        0         ['activation_39[0][0]']       \n                                                                                                  \n max_pooling2d_9 (MaxPoolin  (None, 288, 408, 128)        0         ['dropout_39[0][0]']          \n g2D)                                                                                             \n                                                                                                  \n average_pooling2d_9 (Avera  (None, 288, 408, 128)        0         ['dropout_39[0][0]']          \n gePooling2D)                                                                                     \n                                                                                                  \n concatenate_17 (Concatenat  (None, 288, 408, 256)        0         ['max_pooling2d_9[0][0]',     \n e)                                                                  'average_pooling2d_9[0][0]'] \n                                                                                                  \n conv2d_44 (Conv2D)          (None, 288, 408, 256)        590080    ['concatenate_17[0][0]']      \n                                                                                                  \n batch_normalization_40 (Ba  (None, 288, 408, 256)        1024      ['conv2d_44[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_40 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_40[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_40 (Dropout)        (None, 288, 408, 256)        0         ['activation_40[0][0]']       \n                                                                                                  \n conv2d_45 (Conv2D)          (None, 288, 408, 256)        590080    ['dropout_40[0][0]']          \n                                                                                                  \n batch_normalization_41 (Ba  (None, 288, 408, 256)        1024      ['conv2d_45[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_41 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_41[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_41 (Dropout)        (None, 288, 408, 256)        0         ['activation_41[0][0]']       \n                                                                                                  \n max_pooling2d_10 (MaxPooli  (None, 144, 204, 256)        0         ['dropout_41[0][0]']          \n ng2D)                                                                                            \n                                                                                                  \n average_pooling2d_10 (Aver  (None, 144, 204, 256)        0         ['dropout_41[0][0]']          \n agePooling2D)                                                                                    \n                                                                                                  \n concatenate_18 (Concatenat  (None, 144, 204, 512)        0         ['max_pooling2d_10[0][0]',    \n e)                                                                  'average_pooling2d_10[0][0]']\n                                                                                                  \n conv2d_46 (Conv2D)          (None, 144, 204, 512)        2359808   ['concatenate_18[0][0]']      \n                                                                                                  \n batch_normalization_42 (Ba  (None, 144, 204, 512)        2048      ['conv2d_46[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_42 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_42[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_42 (Dropout)        (None, 144, 204, 512)        0         ['activation_42[0][0]']       \n                                                                                                  \n conv2d_47 (Conv2D)          (None, 144, 204, 512)        2359808   ['dropout_42[0][0]']          \n                                                                                                  \n batch_normalization_43 (Ba  (None, 144, 204, 512)        2048      ['conv2d_47[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_43 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_43[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_43 (Dropout)        (None, 144, 204, 512)        0         ['activation_43[0][0]']       \n                                                                                                  \n max_pooling2d_11 (MaxPooli  (None, 72, 102, 512)         0         ['dropout_43[0][0]']          \n ng2D)                                                                                            \n                                                                                                  \n average_pooling2d_11 (Aver  (None, 72, 102, 512)         0         ['dropout_43[0][0]']          \n agePooling2D)                                                                                    \n                                                                                                  \n concatenate_19 (Concatenat  (None, 72, 102, 1024)        0         ['max_pooling2d_11[0][0]',    \n e)                                                                  'average_pooling2d_11[0][0]']\n                                                                                                  \n conv2d_48 (Conv2D)          (None, 72, 102, 1024)        9438208   ['concatenate_19[0][0]']      \n                                                                                                  \n batch_normalization_44 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_48[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_44 (Activation)  (None, 72, 102, 1024)        0         ['batch_normalization_44[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_44 (Dropout)        (None, 72, 102, 1024)        0         ['activation_44[0][0]']       \n                                                                                                  \n conv2d_49 (Conv2D)          (None, 72, 102, 1024)        9438208   ['dropout_44[0][0]']          \n                                                                                                  \n batch_normalization_45 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_49[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_45 (Activation)  (None, 72, 102, 1024)        0         ['batch_normalization_45[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_45 (Dropout)        (None, 72, 102, 1024)        0         ['activation_45[0][0]']       \n                                                                                                  \n gating_signal0_conv (Conv2  (None, 72, 102, 512)         524800    ['dropout_45[0][0]']          \n D)                                                                                               \n                                                                                                  \n gating_signal0_bn (BatchNo  (None, 72, 102, 512)         2048      ['gating_signal0_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal0_act (Activa  (None, 72, 102, 512)         0         ['gating_signal0_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_50 (Conv2D)          (None, 72, 102, 512)         262656    ['gating_signal0_act[0][0]']  \n                                                                                                  \n g_upattention_block0 (Conv  (None, 72, 102, 512)         2359808   ['conv2d_50[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block0 (Conv2D  (None, 72, 102, 512)         1049088   ['dropout_43[0][0]']          \n )                                                                                                \n                                                                                                  \n add (Add)                   (None, 72, 102, 512)         0         ['g_upattention_block0[0][0]',\n                                                                     'xlattention_block0[0][0]']  \n                                                                                                  \n activation_46 (Activation)  (None, 72, 102, 512)         0         ['add[0][0]']                 \n                                                                                                  \n psiattention_block0 (Conv2  (None, 72, 102, 1)           513       ['activation_46[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_47 (Activation)  (None, 72, 102, 1)           0         ['psiattention_block0[0][0]'] \n                                                                                                  \n up_sampling2d (UpSampling2  (None, 144, 204, 1)          0         ['activation_47[0][0]']       \n D)                                                                                               \n                                                                                                  \n psi_upattention_block0 (La  (None, 144, 204, 512)        0         ['up_sampling2d[0][0]']       \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block0 (Mu  (None, 144, 204, 512)        0         ['psi_upattention_block0[0][0]\n ltiply)                                                            ',                            \n                                                                     'dropout_43[0][0]']          \n                                                                                                  \n q_attn_convattention_block  (None, 144, 204, 512)        262656    ['q_attnattention_block0[0][0]\n 0 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_0 (UpSampling2D  (None, 144, 204, 1024)       0         ['dropout_45[0][0]']          \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block0   (None, 144, 204, 512)        2048      ['q_attn_convattention_block0[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_20 (Concatenat  (None, 144, 204, 1536)       0         ['upsampling_0[0][0]',        \n e)                                                                  'q_attn_bnattention_block0[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_51 (Conv2D)          (None, 144, 204, 512)        7078400   ['concatenate_20[0][0]']      \n                                                                                                  \n batch_normalization_46 (Ba  (None, 144, 204, 512)        2048      ['conv2d_51[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_48 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_46[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_46 (Dropout)        (None, 144, 204, 512)        0         ['activation_48[0][0]']       \n                                                                                                  \n conv2d_52 (Conv2D)          (None, 144, 204, 512)        2359808   ['dropout_46[0][0]']          \n                                                                                                  \n batch_normalization_47 (Ba  (None, 144, 204, 512)        2048      ['conv2d_52[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_49 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_47[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_47 (Dropout)        (None, 144, 204, 512)        0         ['activation_49[0][0]']       \n                                                                                                  \n gating_signal1_conv (Conv2  (None, 144, 204, 256)        131328    ['dropout_47[0][0]']          \n D)                                                                                               \n                                                                                                  \n gating_signal1_bn (BatchNo  (None, 144, 204, 256)        1024      ['gating_signal1_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal1_act (Activa  (None, 144, 204, 256)        0         ['gating_signal1_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_53 (Conv2D)          (None, 144, 204, 256)        65792     ['gating_signal1_act[0][0]']  \n                                                                                                  \n g_upattention_block1 (Conv  (None, 144, 204, 256)        590080    ['conv2d_53[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block1 (Conv2D  (None, 144, 204, 256)        262400    ['dropout_41[0][0]']          \n )                                                                                                \n                                                                                                  \n add_1 (Add)                 (None, 144, 204, 256)        0         ['g_upattention_block1[0][0]',\n                                                                     'xlattention_block1[0][0]']  \n                                                                                                  \n activation_50 (Activation)  (None, 144, 204, 256)        0         ['add_1[0][0]']               \n                                                                                                  \n psiattention_block1 (Conv2  (None, 144, 204, 1)          257       ['activation_50[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_51 (Activation)  (None, 144, 204, 1)          0         ['psiattention_block1[0][0]'] \n                                                                                                  \n up_sampling2d_1 (UpSamplin  (None, 288, 408, 1)          0         ['activation_51[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block1 (La  (None, 288, 408, 256)        0         ['up_sampling2d_1[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block1 (Mu  (None, 288, 408, 256)        0         ['psi_upattention_block1[0][0]\n ltiply)                                                            ',                            \n                                                                     'dropout_41[0][0]']          \n                                                                                                  \n q_attn_convattention_block  (None, 288, 408, 256)        65792     ['q_attnattention_block1[0][0]\n 1 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_1 (UpSampling2D  (None, 288, 408, 512)        0         ['dropout_47[0][0]']          \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block1   (None, 288, 408, 256)        1024      ['q_attn_convattention_block1[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_21 (Concatenat  (None, 288, 408, 768)        0         ['upsampling_1[0][0]',        \n e)                                                                  'q_attn_bnattention_block1[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_54 (Conv2D)          (None, 288, 408, 256)        1769728   ['concatenate_21[0][0]']      \n                                                                                                  \n batch_normalization_48 (Ba  (None, 288, 408, 256)        1024      ['conv2d_54[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_52 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_48[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_48 (Dropout)        (None, 288, 408, 256)        0         ['activation_52[0][0]']       \n                                                                                                  \n conv2d_55 (Conv2D)          (None, 288, 408, 256)        590080    ['dropout_48[0][0]']          \n                                                                                                  \n batch_normalization_49 (Ba  (None, 288, 408, 256)        1024      ['conv2d_55[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_53 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_49[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_49 (Dropout)        (None, 288, 408, 256)        0         ['activation_53[0][0]']       \n                                                                                                  \n gating_signal2_conv (Conv2  (None, 288, 408, 128)        32896     ['dropout_49[0][0]']          \n D)                                                                                               \n                                                                                                  \n gating_signal2_bn (BatchNo  (None, 288, 408, 128)        512       ['gating_signal2_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal2_act (Activa  (None, 288, 408, 128)        0         ['gating_signal2_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_56 (Conv2D)          (None, 288, 408, 128)        16512     ['gating_signal2_act[0][0]']  \n                                                                                                  \n g_upattention_block2 (Conv  (None, 288, 408, 128)        147584    ['conv2d_56[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block2 (Conv2D  (None, 288, 408, 128)        65664     ['dropout_39[0][0]']          \n )                                                                                                \n                                                                                                  \n add_2 (Add)                 (None, 288, 408, 128)        0         ['g_upattention_block2[0][0]',\n                                                                     'xlattention_block2[0][0]']  \n                                                                                                  \n activation_54 (Activation)  (None, 288, 408, 128)        0         ['add_2[0][0]']               \n                                                                                                  \n psiattention_block2 (Conv2  (None, 288, 408, 1)          129       ['activation_54[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_55 (Activation)  (None, 288, 408, 1)          0         ['psiattention_block2[0][0]'] \n                                                                                                  \n up_sampling2d_2 (UpSamplin  (None, 576, 816, 1)          0         ['activation_55[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block2 (La  (None, 576, 816, 128)        0         ['up_sampling2d_2[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block2 (Mu  (None, 576, 816, 128)        0         ['psi_upattention_block2[0][0]\n ltiply)                                                            ',                            \n                                                                     'dropout_39[0][0]']          \n                                                                                                  \n q_attn_convattention_block  (None, 576, 816, 128)        16512     ['q_attnattention_block2[0][0]\n 2 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_2 (UpSampling2D  (None, 576, 816, 256)        0         ['dropout_49[0][0]']          \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block2   (None, 576, 816, 128)        512       ['q_attn_convattention_block2[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_22 (Concatenat  (None, 576, 816, 384)        0         ['upsampling_2[0][0]',        \n e)                                                                  'q_attn_bnattention_block2[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_57 (Conv2D)          (None, 576, 816, 128)        442496    ['concatenate_22[0][0]']      \n                                                                                                  \n batch_normalization_50 (Ba  (None, 576, 816, 128)        512       ['conv2d_57[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_56 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_50[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_50 (Dropout)        (None, 576, 816, 128)        0         ['activation_56[0][0]']       \n                                                                                                  \n conv2d_58 (Conv2D)          (None, 576, 816, 128)        147584    ['dropout_50[0][0]']          \n                                                                                                  \n batch_normalization_51 (Ba  (None, 576, 816, 128)        512       ['conv2d_58[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_57 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_51[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_51 (Dropout)        (None, 576, 816, 128)        0         ['activation_57[0][0]']       \n                                                                                                  \n gating_signal3_conv (Conv2  (None, 576, 816, 64)         8256      ['dropout_51[0][0]']          \n D)                                                                                               \n                                                                                                  \n gating_signal3_bn (BatchNo  (None, 576, 816, 64)         256       ['gating_signal3_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal3_act (Activa  (None, 576, 816, 64)         0         ['gating_signal3_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_59 (Conv2D)          (None, 576, 816, 64)         4160      ['gating_signal3_act[0][0]']  \n                                                                                                  \n g_upattention_block3 (Conv  (None, 576, 816, 64)         36928     ['conv2d_59[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block3 (Conv2D  (None, 576, 816, 64)         16448     ['dropout_37[0][0]']          \n )                                                                                                \n                                                                                                  \n add_3 (Add)                 (None, 576, 816, 64)         0         ['g_upattention_block3[0][0]',\n                                                                     'xlattention_block3[0][0]']  \n                                                                                                  \n activation_58 (Activation)  (None, 576, 816, 64)         0         ['add_3[0][0]']               \n                                                                                                  \n psiattention_block3 (Conv2  (None, 576, 816, 1)          65        ['activation_58[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_59 (Activation)  (None, 576, 816, 1)          0         ['psiattention_block3[0][0]'] \n                                                                                                  \n up_sampling2d_3 (UpSamplin  (None, 1152, 1632, 1)        0         ['activation_59[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block3 (La  (None, 1152, 1632, 64)       0         ['up_sampling2d_3[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block3 (Mu  (None, 1152, 1632, 64)       0         ['psi_upattention_block3[0][0]\n ltiply)                                                            ',                            \n                                                                     'dropout_37[0][0]']          \n                                                                                                  \n q_attn_convattention_block  (None, 1152, 1632, 64)       4160      ['q_attnattention_block3[0][0]\n 3 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_3 (UpSampling2D  (None, 1152, 1632, 128)      0         ['dropout_51[0][0]']          \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block3   (None, 1152, 1632, 64)       256       ['q_attn_convattention_block3[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_23 (Concatenat  (None, 1152, 1632, 192)      0         ['upsampling_3[0][0]',        \n e)                                                                  'q_attn_bnattention_block3[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_60 (Conv2D)          (None, 1152, 1632, 64)       110656    ['concatenate_23[0][0]']      \n                                                                                                  \n batch_normalization_52 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_60[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_60 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_52[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_52 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_60[0][0]']       \n                                                                                                  \n conv2d_61 (Conv2D)          (None, 1152, 1632, 64)       36928     ['dropout_52[0][0]']          \n                                                                                                  \n batch_normalization_53 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_61[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_61 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_53[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_53 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_61[0][0]']       \n                                                                                                  \n conv2d_62 (Conv2D)          (None, 1152, 1632, 1)        65        ['dropout_53[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 43600389 (166.32 MB)\nTrainable params: 43584773 (166.26 MB)\nNon-trainable params: 15616 (61.00 KB)\n__________________________________________________________________________________________________\nNone"
  },
  {
    "objectID": "index.html#create-a-u-net-model-with-attention-gates-and-residual-connections",
    "href": "index.html#create-a-u-net-model-with-attention-gates-and-residual-connections",
    "title": "Segmentation related functions will be developed here",
    "section": "Create a U-Net model with attention gates and residual connections",
    "text": "Create a U-Net model with attention gates and residual connections\n\nres_attn_unet_model = residual_attn_unet(\n    input_size=(h, w, c),\n    filter_list=filter_list,\n    n_classes=n_classes\n)\nprint(res_attn_unet_model.summary())\n\nModel: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_6 (InputLayer)        [(None, 1152, 1632, 1)]      0         []                            \n                                                                                                  \n conv2d_63 (Conv2D)          (None, 1152, 1632, 64)       640       ['input_6[0][0]']             \n                                                                                                  \n batch_normalization_54 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_63[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_62 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_54[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_54 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_62[0][0]']       \n                                                                                                  \n conv2d_64 (Conv2D)          (None, 1152, 1632, 64)       36928     ['dropout_54[0][0]']          \n                                                                                                  \n conv2d_65 (Conv2D)          (None, 1152, 1632, 64)       128       ['input_6[0][0]']             \n                                                                                                  \n batch_normalization_55 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_64[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_56 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_65[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_55 (Dropout)        (None, 1152, 1632, 64)       0         ['batch_normalization_55[0][0]\n                                                                    ']                            \n                                                                                                  \n add_4 (Add)                 (None, 1152, 1632, 64)       0         ['batch_normalization_56[0][0]\n                                                                    ',                            \n                                                                     'dropout_55[0][0]']          \n                                                                                                  \n activation_63 (Activation)  (None, 1152, 1632, 64)       0         ['add_4[0][0]']               \n                                                                                                  \n max_pooling2d_12 (MaxPooli  (None, 576, 816, 64)         0         ['activation_63[0][0]']       \n ng2D)                                                                                            \n                                                                                                  \n average_pooling2d_12 (Aver  (None, 576, 816, 64)         0         ['activation_63[0][0]']       \n agePooling2D)                                                                                    \n                                                                                                  \n concatenate_24 (Concatenat  (None, 576, 816, 128)        0         ['max_pooling2d_12[0][0]',    \n e)                                                                  'average_pooling2d_12[0][0]']\n                                                                                                  \n conv2d_66 (Conv2D)          (None, 576, 816, 128)        147584    ['concatenate_24[0][0]']      \n                                                                                                  \n batch_normalization_57 (Ba  (None, 576, 816, 128)        512       ['conv2d_66[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_64 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_57[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_56 (Dropout)        (None, 576, 816, 128)        0         ['activation_64[0][0]']       \n                                                                                                  \n conv2d_67 (Conv2D)          (None, 576, 816, 128)        147584    ['dropout_56[0][0]']          \n                                                                                                  \n conv2d_68 (Conv2D)          (None, 576, 816, 128)        16512     ['concatenate_24[0][0]']      \n                                                                                                  \n batch_normalization_58 (Ba  (None, 576, 816, 128)        512       ['conv2d_67[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_59 (Ba  (None, 576, 816, 128)        512       ['conv2d_68[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_57 (Dropout)        (None, 576, 816, 128)        0         ['batch_normalization_58[0][0]\n                                                                    ']                            \n                                                                                                  \n add_5 (Add)                 (None, 576, 816, 128)        0         ['batch_normalization_59[0][0]\n                                                                    ',                            \n                                                                     'dropout_57[0][0]']          \n                                                                                                  \n activation_65 (Activation)  (None, 576, 816, 128)        0         ['add_5[0][0]']               \n                                                                                                  \n max_pooling2d_13 (MaxPooli  (None, 288, 408, 128)        0         ['activation_65[0][0]']       \n ng2D)                                                                                            \n                                                                                                  \n average_pooling2d_13 (Aver  (None, 288, 408, 128)        0         ['activation_65[0][0]']       \n agePooling2D)                                                                                    \n                                                                                                  \n concatenate_25 (Concatenat  (None, 288, 408, 256)        0         ['max_pooling2d_13[0][0]',    \n e)                                                                  'average_pooling2d_13[0][0]']\n                                                                                                  \n conv2d_69 (Conv2D)          (None, 288, 408, 256)        590080    ['concatenate_25[0][0]']      \n                                                                                                  \n batch_normalization_60 (Ba  (None, 288, 408, 256)        1024      ['conv2d_69[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_66 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_60[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_58 (Dropout)        (None, 288, 408, 256)        0         ['activation_66[0][0]']       \n                                                                                                  \n conv2d_70 (Conv2D)          (None, 288, 408, 256)        590080    ['dropout_58[0][0]']          \n                                                                                                  \n conv2d_71 (Conv2D)          (None, 288, 408, 256)        65792     ['concatenate_25[0][0]']      \n                                                                                                  \n batch_normalization_61 (Ba  (None, 288, 408, 256)        1024      ['conv2d_70[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_62 (Ba  (None, 288, 408, 256)        1024      ['conv2d_71[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_59 (Dropout)        (None, 288, 408, 256)        0         ['batch_normalization_61[0][0]\n                                                                    ']                            \n                                                                                                  \n add_6 (Add)                 (None, 288, 408, 256)        0         ['batch_normalization_62[0][0]\n                                                                    ',                            \n                                                                     'dropout_59[0][0]']          \n                                                                                                  \n activation_67 (Activation)  (None, 288, 408, 256)        0         ['add_6[0][0]']               \n                                                                                                  \n max_pooling2d_14 (MaxPooli  (None, 144, 204, 256)        0         ['activation_67[0][0]']       \n ng2D)                                                                                            \n                                                                                                  \n average_pooling2d_14 (Aver  (None, 144, 204, 256)        0         ['activation_67[0][0]']       \n agePooling2D)                                                                                    \n                                                                                                  \n concatenate_26 (Concatenat  (None, 144, 204, 512)        0         ['max_pooling2d_14[0][0]',    \n e)                                                                  'average_pooling2d_14[0][0]']\n                                                                                                  \n conv2d_72 (Conv2D)          (None, 144, 204, 512)        2359808   ['concatenate_26[0][0]']      \n                                                                                                  \n batch_normalization_63 (Ba  (None, 144, 204, 512)        2048      ['conv2d_72[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_68 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_63[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_60 (Dropout)        (None, 144, 204, 512)        0         ['activation_68[0][0]']       \n                                                                                                  \n conv2d_73 (Conv2D)          (None, 144, 204, 512)        2359808   ['dropout_60[0][0]']          \n                                                                                                  \n conv2d_74 (Conv2D)          (None, 144, 204, 512)        262656    ['concatenate_26[0][0]']      \n                                                                                                  \n batch_normalization_64 (Ba  (None, 144, 204, 512)        2048      ['conv2d_73[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_65 (Ba  (None, 144, 204, 512)        2048      ['conv2d_74[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_61 (Dropout)        (None, 144, 204, 512)        0         ['batch_normalization_64[0][0]\n                                                                    ']                            \n                                                                                                  \n add_7 (Add)                 (None, 144, 204, 512)        0         ['batch_normalization_65[0][0]\n                                                                    ',                            \n                                                                     'dropout_61[0][0]']          \n                                                                                                  \n activation_69 (Activation)  (None, 144, 204, 512)        0         ['add_7[0][0]']               \n                                                                                                  \n max_pooling2d_15 (MaxPooli  (None, 72, 102, 512)         0         ['activation_69[0][0]']       \n ng2D)                                                                                            \n                                                                                                  \n average_pooling2d_15 (Aver  (None, 72, 102, 512)         0         ['activation_69[0][0]']       \n agePooling2D)                                                                                    \n                                                                                                  \n concatenate_27 (Concatenat  (None, 72, 102, 1024)        0         ['max_pooling2d_15[0][0]',    \n e)                                                                  'average_pooling2d_15[0][0]']\n                                                                                                  \n conv2d_75 (Conv2D)          (None, 72, 102, 1024)        9438208   ['concatenate_27[0][0]']      \n                                                                                                  \n batch_normalization_66 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_75[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_70 (Activation)  (None, 72, 102, 1024)        0         ['batch_normalization_66[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_62 (Dropout)        (None, 72, 102, 1024)        0         ['activation_70[0][0]']       \n                                                                                                  \n conv2d_76 (Conv2D)          (None, 72, 102, 1024)        9438208   ['dropout_62[0][0]']          \n                                                                                                  \n conv2d_77 (Conv2D)          (None, 72, 102, 1024)        1049600   ['concatenate_27[0][0]']      \n                                                                                                  \n batch_normalization_67 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_76[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_68 (Ba  (None, 72, 102, 1024)        4096      ['conv2d_77[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_63 (Dropout)        (None, 72, 102, 1024)        0         ['batch_normalization_67[0][0]\n                                                                    ']                            \n                                                                                                  \n add_8 (Add)                 (None, 72, 102, 1024)        0         ['batch_normalization_68[0][0]\n                                                                    ',                            \n                                                                     'dropout_63[0][0]']          \n                                                                                                  \n activation_71 (Activation)  (None, 72, 102, 1024)        0         ['add_8[0][0]']               \n                                                                                                  \n gating_signal0_conv (Conv2  (None, 72, 102, 512)         524800    ['activation_71[0][0]']       \n D)                                                                                               \n                                                                                                  \n gating_signal0_bn (BatchNo  (None, 72, 102, 512)         2048      ['gating_signal0_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal0_act (Activa  (None, 72, 102, 512)         0         ['gating_signal0_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_78 (Conv2D)          (None, 72, 102, 512)         262656    ['gating_signal0_act[0][0]']  \n                                                                                                  \n g_upattention_block0 (Conv  (None, 72, 102, 512)         2359808   ['conv2d_78[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block0 (Conv2D  (None, 72, 102, 512)         1049088   ['activation_69[0][0]']       \n )                                                                                                \n                                                                                                  \n add_9 (Add)                 (None, 72, 102, 512)         0         ['g_upattention_block0[0][0]',\n                                                                     'xlattention_block0[0][0]']  \n                                                                                                  \n activation_72 (Activation)  (None, 72, 102, 512)         0         ['add_9[0][0]']               \n                                                                                                  \n psiattention_block0 (Conv2  (None, 72, 102, 1)           513       ['activation_72[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_73 (Activation)  (None, 72, 102, 1)           0         ['psiattention_block0[0][0]'] \n                                                                                                  \n up_sampling2d_4 (UpSamplin  (None, 144, 204, 1)          0         ['activation_73[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block0 (La  (None, 144, 204, 512)        0         ['up_sampling2d_4[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block0 (Mu  (None, 144, 204, 512)        0         ['psi_upattention_block0[0][0]\n ltiply)                                                            ',                            \n                                                                     'activation_69[0][0]']       \n                                                                                                  \n q_attn_convattention_block  (None, 144, 204, 512)        262656    ['q_attnattention_block0[0][0]\n 0 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_0 (UpSampling2D  (None, 144, 204, 1024)       0         ['activation_71[0][0]']       \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block0   (None, 144, 204, 512)        2048      ['q_attn_convattention_block0[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_28 (Concatenat  (None, 144, 204, 1536)       0         ['upsampling_0[0][0]',        \n e)                                                                  'q_attn_bnattention_block0[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_79 (Conv2D)          (None, 144, 204, 512)        7078400   ['concatenate_28[0][0]']      \n                                                                                                  \n batch_normalization_69 (Ba  (None, 144, 204, 512)        2048      ['conv2d_79[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_74 (Activation)  (None, 144, 204, 512)        0         ['batch_normalization_69[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_64 (Dropout)        (None, 144, 204, 512)        0         ['activation_74[0][0]']       \n                                                                                                  \n conv2d_80 (Conv2D)          (None, 144, 204, 512)        2359808   ['dropout_64[0][0]']          \n                                                                                                  \n conv2d_81 (Conv2D)          (None, 144, 204, 512)        786944    ['concatenate_28[0][0]']      \n                                                                                                  \n batch_normalization_70 (Ba  (None, 144, 204, 512)        2048      ['conv2d_80[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_71 (Ba  (None, 144, 204, 512)        2048      ['conv2d_81[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_65 (Dropout)        (None, 144, 204, 512)        0         ['batch_normalization_70[0][0]\n                                                                    ']                            \n                                                                                                  \n add_10 (Add)                (None, 144, 204, 512)        0         ['batch_normalization_71[0][0]\n                                                                    ',                            \n                                                                     'dropout_65[0][0]']          \n                                                                                                  \n activation_75 (Activation)  (None, 144, 204, 512)        0         ['add_10[0][0]']              \n                                                                                                  \n gating_signal1_conv (Conv2  (None, 144, 204, 256)        131328    ['activation_75[0][0]']       \n D)                                                                                               \n                                                                                                  \n gating_signal1_bn (BatchNo  (None, 144, 204, 256)        1024      ['gating_signal1_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal1_act (Activa  (None, 144, 204, 256)        0         ['gating_signal1_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_82 (Conv2D)          (None, 144, 204, 256)        65792     ['gating_signal1_act[0][0]']  \n                                                                                                  \n g_upattention_block1 (Conv  (None, 144, 204, 256)        590080    ['conv2d_82[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block1 (Conv2D  (None, 144, 204, 256)        262400    ['activation_67[0][0]']       \n )                                                                                                \n                                                                                                  \n add_11 (Add)                (None, 144, 204, 256)        0         ['g_upattention_block1[0][0]',\n                                                                     'xlattention_block1[0][0]']  \n                                                                                                  \n activation_76 (Activation)  (None, 144, 204, 256)        0         ['add_11[0][0]']              \n                                                                                                  \n psiattention_block1 (Conv2  (None, 144, 204, 1)          257       ['activation_76[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_77 (Activation)  (None, 144, 204, 1)          0         ['psiattention_block1[0][0]'] \n                                                                                                  \n up_sampling2d_5 (UpSamplin  (None, 288, 408, 1)          0         ['activation_77[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block1 (La  (None, 288, 408, 256)        0         ['up_sampling2d_5[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block1 (Mu  (None, 288, 408, 256)        0         ['psi_upattention_block1[0][0]\n ltiply)                                                            ',                            \n                                                                     'activation_67[0][0]']       \n                                                                                                  \n q_attn_convattention_block  (None, 288, 408, 256)        65792     ['q_attnattention_block1[0][0]\n 1 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_1 (UpSampling2D  (None, 288, 408, 512)        0         ['activation_75[0][0]']       \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block1   (None, 288, 408, 256)        1024      ['q_attn_convattention_block1[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_29 (Concatenat  (None, 288, 408, 768)        0         ['upsampling_1[0][0]',        \n e)                                                                  'q_attn_bnattention_block1[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_83 (Conv2D)          (None, 288, 408, 256)        1769728   ['concatenate_29[0][0]']      \n                                                                                                  \n batch_normalization_72 (Ba  (None, 288, 408, 256)        1024      ['conv2d_83[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_78 (Activation)  (None, 288, 408, 256)        0         ['batch_normalization_72[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_66 (Dropout)        (None, 288, 408, 256)        0         ['activation_78[0][0]']       \n                                                                                                  \n conv2d_84 (Conv2D)          (None, 288, 408, 256)        590080    ['dropout_66[0][0]']          \n                                                                                                  \n conv2d_85 (Conv2D)          (None, 288, 408, 256)        196864    ['concatenate_29[0][0]']      \n                                                                                                  \n batch_normalization_73 (Ba  (None, 288, 408, 256)        1024      ['conv2d_84[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_74 (Ba  (None, 288, 408, 256)        1024      ['conv2d_85[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_67 (Dropout)        (None, 288, 408, 256)        0         ['batch_normalization_73[0][0]\n                                                                    ']                            \n                                                                                                  \n add_12 (Add)                (None, 288, 408, 256)        0         ['batch_normalization_74[0][0]\n                                                                    ',                            \n                                                                     'dropout_67[0][0]']          \n                                                                                                  \n activation_79 (Activation)  (None, 288, 408, 256)        0         ['add_12[0][0]']              \n                                                                                                  \n gating_signal2_conv (Conv2  (None, 288, 408, 128)        32896     ['activation_79[0][0]']       \n D)                                                                                               \n                                                                                                  \n gating_signal2_bn (BatchNo  (None, 288, 408, 128)        512       ['gating_signal2_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal2_act (Activa  (None, 288, 408, 128)        0         ['gating_signal2_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_86 (Conv2D)          (None, 288, 408, 128)        16512     ['gating_signal2_act[0][0]']  \n                                                                                                  \n g_upattention_block2 (Conv  (None, 288, 408, 128)        147584    ['conv2d_86[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block2 (Conv2D  (None, 288, 408, 128)        65664     ['activation_65[0][0]']       \n )                                                                                                \n                                                                                                  \n add_13 (Add)                (None, 288, 408, 128)        0         ['g_upattention_block2[0][0]',\n                                                                     'xlattention_block2[0][0]']  \n                                                                                                  \n activation_80 (Activation)  (None, 288, 408, 128)        0         ['add_13[0][0]']              \n                                                                                                  \n psiattention_block2 (Conv2  (None, 288, 408, 1)          129       ['activation_80[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_81 (Activation)  (None, 288, 408, 1)          0         ['psiattention_block2[0][0]'] \n                                                                                                  \n up_sampling2d_6 (UpSamplin  (None, 576, 816, 1)          0         ['activation_81[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block2 (La  (None, 576, 816, 128)        0         ['up_sampling2d_6[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block2 (Mu  (None, 576, 816, 128)        0         ['psi_upattention_block2[0][0]\n ltiply)                                                            ',                            \n                                                                     'activation_65[0][0]']       \n                                                                                                  \n q_attn_convattention_block  (None, 576, 816, 128)        16512     ['q_attnattention_block2[0][0]\n 2 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_2 (UpSampling2D  (None, 576, 816, 256)        0         ['activation_79[0][0]']       \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block2   (None, 576, 816, 128)        512       ['q_attn_convattention_block2[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_30 (Concatenat  (None, 576, 816, 384)        0         ['upsampling_2[0][0]',        \n e)                                                                  'q_attn_bnattention_block2[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_87 (Conv2D)          (None, 576, 816, 128)        442496    ['concatenate_30[0][0]']      \n                                                                                                  \n batch_normalization_75 (Ba  (None, 576, 816, 128)        512       ['conv2d_87[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_82 (Activation)  (None, 576, 816, 128)        0         ['batch_normalization_75[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_68 (Dropout)        (None, 576, 816, 128)        0         ['activation_82[0][0]']       \n                                                                                                  \n conv2d_88 (Conv2D)          (None, 576, 816, 128)        147584    ['dropout_68[0][0]']          \n                                                                                                  \n conv2d_89 (Conv2D)          (None, 576, 816, 128)        49280     ['concatenate_30[0][0]']      \n                                                                                                  \n batch_normalization_76 (Ba  (None, 576, 816, 128)        512       ['conv2d_88[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_77 (Ba  (None, 576, 816, 128)        512       ['conv2d_89[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_69 (Dropout)        (None, 576, 816, 128)        0         ['batch_normalization_76[0][0]\n                                                                    ']                            \n                                                                                                  \n add_14 (Add)                (None, 576, 816, 128)        0         ['batch_normalization_77[0][0]\n                                                                    ',                            \n                                                                     'dropout_69[0][0]']          \n                                                                                                  \n activation_83 (Activation)  (None, 576, 816, 128)        0         ['add_14[0][0]']              \n                                                                                                  \n gating_signal3_conv (Conv2  (None, 576, 816, 64)         8256      ['activation_83[0][0]']       \n D)                                                                                               \n                                                                                                  \n gating_signal3_bn (BatchNo  (None, 576, 816, 64)         256       ['gating_signal3_conv[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n gating_signal3_act (Activa  (None, 576, 816, 64)         0         ['gating_signal3_bn[0][0]']   \n tion)                                                                                            \n                                                                                                  \n conv2d_90 (Conv2D)          (None, 576, 816, 64)         4160      ['gating_signal3_act[0][0]']  \n                                                                                                  \n g_upattention_block3 (Conv  (None, 576, 816, 64)         36928     ['conv2d_90[0][0]']           \n 2DTranspose)                                                                                     \n                                                                                                  \n xlattention_block3 (Conv2D  (None, 576, 816, 64)         16448     ['activation_63[0][0]']       \n )                                                                                                \n                                                                                                  \n add_15 (Add)                (None, 576, 816, 64)         0         ['g_upattention_block3[0][0]',\n                                                                     'xlattention_block3[0][0]']  \n                                                                                                  \n activation_84 (Activation)  (None, 576, 816, 64)         0         ['add_15[0][0]']              \n                                                                                                  \n psiattention_block3 (Conv2  (None, 576, 816, 1)          65        ['activation_84[0][0]']       \n D)                                                                                               \n                                                                                                  \n activation_85 (Activation)  (None, 576, 816, 1)          0         ['psiattention_block3[0][0]'] \n                                                                                                  \n up_sampling2d_7 (UpSamplin  (None, 1152, 1632, 1)        0         ['activation_85[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n psi_upattention_block3 (La  (None, 1152, 1632, 64)       0         ['up_sampling2d_7[0][0]']     \n mbda)                                                                                            \n                                                                                                  \n q_attnattention_block3 (Mu  (None, 1152, 1632, 64)       0         ['psi_upattention_block3[0][0]\n ltiply)                                                            ',                            \n                                                                     'activation_63[0][0]']       \n                                                                                                  \n q_attn_convattention_block  (None, 1152, 1632, 64)       4160      ['q_attnattention_block3[0][0]\n 3 (Conv2D)                                                         ']                            \n                                                                                                  \n upsampling_3 (UpSampling2D  (None, 1152, 1632, 128)      0         ['activation_83[0][0]']       \n )                                                                                                \n                                                                                                  \n q_attn_bnattention_block3   (None, 1152, 1632, 64)       256       ['q_attn_convattention_block3[\n (BatchNormalization)                                               0][0]']                       \n                                                                                                  \n concatenate_31 (Concatenat  (None, 1152, 1632, 192)      0         ['upsampling_3[0][0]',        \n e)                                                                  'q_attn_bnattention_block3[0]\n                                                                    [0]']                         \n                                                                                                  \n conv2d_91 (Conv2D)          (None, 1152, 1632, 64)       110656    ['concatenate_31[0][0]']      \n                                                                                                  \n batch_normalization_78 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_91[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_86 (Activation)  (None, 1152, 1632, 64)       0         ['batch_normalization_78[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_70 (Dropout)        (None, 1152, 1632, 64)       0         ['activation_86[0][0]']       \n                                                                                                  \n conv2d_92 (Conv2D)          (None, 1152, 1632, 64)       36928     ['dropout_70[0][0]']          \n                                                                                                  \n conv2d_93 (Conv2D)          (None, 1152, 1632, 64)       12352     ['concatenate_31[0][0]']      \n                                                                                                  \n batch_normalization_79 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_92[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n batch_normalization_80 (Ba  (None, 1152, 1632, 64)       256       ['conv2d_93[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n dropout_71 (Dropout)        (None, 1152, 1632, 64)       0         ['batch_normalization_79[0][0]\n                                                                    ']                            \n                                                                                                  \n add_16 (Add)                (None, 1152, 1632, 64)       0         ['batch_normalization_80[0][0]\n                                                                    ',                            \n                                                                     'dropout_71[0][0]']          \n                                                                                                  \n activation_87 (Activation)  (None, 1152, 1632, 64)       0         ['add_16[0][0]']              \n                                                                                                  \n conv2d_94 (Conv2D)          (None, 1152, 1632, 1)        65        ['activation_87[0][0]']       \n                                                                                                  \n==================================================================================================\nTotal params: 46052293 (175.68 MB)\nTrainable params: 46030789 (175.59 MB)\nNon-trainable params: 21504 (84.00 KB)\n__________________________________________________________________________________________________\nNone"
  },
  {
    "objectID": "model_developement.html",
    "href": "model_developement.html",
    "title": "Model Developement",
    "section": "",
    "text": "Different models will be developed here and tested here\nmpl.rcParams['image.cmap'] = 'gray'"
  },
  {
    "objectID": "model_developement.html#fully-convolutional-network",
    "href": "model_developement.html#fully-convolutional-network",
    "title": "Model Developement",
    "section": "Fully Convolutional Network",
    "text": "Fully Convolutional Network\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input\n\n# Load the VGG16 model\nvgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the layers in the VGG16 model\nfor layer in vgg16.layers:\n    layer.trainable = False\n\n# Define the input layer\ninput_layer = Input(shape=(224, 224, 3))\n\n# Get the output of the VGG16 model\nvgg_output = vgg16(input_layer)\n\n# Add convolutional layers on top of the VGG16 model\nconv1 = Conv2D(4096, (7, 7), activation='relu', padding='same')(vgg_output)\nconv2 = Conv2D(4096, (1, 1), activation='relu', padding='same')(conv1)\nconv3 = Conv2D(2, (1, 1), activation='softmax', padding='same')(conv2)\n\n# Define the decoder\nupsample1 = Conv2DTranspose(2, (4, 4), strides=(2, 2), padding='same')(conv3)\nconv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(upsample1)\nconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\nupsample2 = Conv2DTranspose(2, (4, 4), strides=(2, 2), padding='same')(conv5)\nconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(upsample2)\nconv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\nupsample3 = Conv2DTranspose(2, (16, 16), strides=(8, 8), padding='same')(conv7)\n\n# Create a new model with the VGG16 encoder and the decoder\nfcn = Model(inputs=input_layer, outputs=upsample3)\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n    8192/58889256 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  376832/58889256 [..............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  704512/58889256 [..............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3031040/58889256 [&gt;.............................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 3227648/58889256 [&gt;.............................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5971968/58889256 [==&gt;...........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6594560/58889256 [==&gt;...........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6955008/58889256 [==&gt;...........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7151616/58889256 [==&gt;...........................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9183232/58889256 [===&gt;..........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11845632/58889256 [=====&gt;........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12304384/58889256 [=====&gt;........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14598144/58889256 [======&gt;.......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15155200/58889256 [======&gt;.......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17170432/58889256 [=======&gt;......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19898368/58889256 [=========&gt;....................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22626304/58889256 [==========&gt;...................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23019520/58889256 [==========&gt;...................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25812992/58889256 [============&gt;.................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27000832/58889256 [============&gt;.................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29016064/58889256 [=============&gt;................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31162368/58889256 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31899648/58889256 [===============&gt;..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34013184/58889256 [================&gt;.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36052992/58889256 [=================&gt;............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38019072/58889256 [==================&gt;...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40648704/58889256 [===================&gt;..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41140224/58889256 [===================&gt;..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43884544/58889256 [=====================&gt;........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46014464/58889256 [======================&gt;.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b48726016/58889256 [=======================&gt;......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b49053696/58889256 [=======================&gt;......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51519488/58889256 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54116352/58889256 [==========================&gt;...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54706176/58889256 [==========================&gt;...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57319424/58889256 [============================&gt;.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58889256/58889256 [==============================] - 2s 0us/step\n\n\n\nsource\n\nblocks\n\n blocks (x:tensorflow.python.framework.tensor.Tensor, n_convs:int,\n         filters:int, kernel_size:int=3, activation:str='relu',\n         pool_size:int=2, pool_stride:int=2, name:str='block')\n\n\nsource\n\n\nVGG_16\n\n VGG_16 (input_image:tensorflow.python.framework.tensor.Tensor)\n\n\ndata_path = Path.cwd().parent.parent.parent/'data_first/Trainingsdata_1024_1224'\nIMAGE_HEIGHT = 1024\nIMAGE_WIDTH = 1224\npreprocess_obj = Preprocess(\n                           image_path=data_path/'X',\n                           label_path=data_path/'y',\n                           im_height=IMAGE_HEIGHT,\n                           im_width=IMAGE_WIDTH,\n                           bs=BATCH_SIZE\n                           )\ntrain_ds, test_ds = preprocess_obj.create_train_test_dataset()\n\nWARNING:tensorflow:From /home/goni/mambaforge/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n\n\n2023-04-13 14:17:58.202878: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2023-04-13 14:17:58.202950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ISCN5CG2091P37): /proc/driver/nvidia/version does not exist\n2023-04-13 14:17:58.203354: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nfor img,lbl in train_ds.take(2):\n    print(img.shape)\n    print(np.max(lbl.numpy()))\n    print(np.max(img.numpy()))\n\n(10, 1024, 1224, 1)\n1.0\n1.0\n(10, 1024, 1224, 1)\n1.0\n1.0\n\n\n2023-04-13 14:18:00.397396: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\nsource\n\n\nfcn_8_decoder\n\n fcn_8_decoder (convs, n_classes)\n\nDefines the FCN 8 decoder. Args: convs (tuple of tensors) - output of the encoder network n_classes (int) - number of classes Returns: tensor with shape (height, width, n_classes) containing class probabilities\n\nsource\n\n\nsegmentation_model\n\n segmentation_model ()\n\nModel will be created from encoder and decoder\n\ninputs = tf.keras.layers.Input(\n        shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1,))\nconvs = VGG_16(\n        input_image=inputs\n        )\n\n\nconvs\n\n(&lt;KerasTensor: shape=(None, 512, 612, 64) dtype=float32 (created by layer 'block1_pool1')&gt;,\n &lt;KerasTensor: shape=(None, 256, 306, 128) dtype=float32 (created by layer 'block2_pool1')&gt;,\n &lt;KerasTensor: shape=(None, 128, 153, 256) dtype=float32 (created by layer 'block3_pool2')&gt;,\n &lt;KerasTensor: shape=(None, 64, 76, 512) dtype=float32 (created by layer 'block4_pool2')&gt;,\n &lt;KerasTensor: shape=(None, 32, 38, 4096) dtype=float32 (created by layer 'conv7')&gt;)\n\n\n\nmodel = segmentation_model()\n\n\nshow_model(model)\n\n\n\n\n\nsource\n\n\nCosineDecay\n\n CosineDecay (initial_learning_rate, total_steps, warmup_steps=0,\n              alpha=0.0, name=None)\n\nThe learning rate schedule base class.\nYou can use a learning rate schedule to modulate how the learning rate of your optimizer changes over time.\nSeveral built-in learning rate schedules are available, such as tf.keras.optimizers.schedules.ExponentialDecay or tf.keras.optimizers.schedules.PiecewiseConstantDecay:\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-2,\n    decay_steps=10000,\n    decay_rate=0.9)\noptimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\nA LearningRateSchedule instance can be passed in as the learning_rate argument of any optimizer.\nTo implement your own schedule object, you should implement the __call__ method, which takes a step argument (scalar integer tensor, the current training step count). Like for any other Keras object, you can also optionally make your object serializable by implementing the get_config and from_config methods.\nExample:\nclass MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n  def __init__(self, initial_learning_rate):\n    self.initial_learning_rate = initial_learning_rate\n\n  def __call__(self, step):\n     return self.initial_learning_rate / (step + 1)\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n\npreprocess_obj.bs\n\n10\n\n\n\nepochs = 5\n#from easy_pin_detection.config import *\n#total_steps = train_count//BATCH_SIZE * epochs\n#warmup_steps = int(0.25 * total_steps)\n#total_steps, warmup_steps\n\n(50, 12)\n\n\n\n#initial_learning_rate = 0.0001\n#alpha = 0.1\n#opt = tf.keras.optimizers.Adam(CosineDecay(\n                                        #initial_learning_rate=initial_learning_rate,\n                                        #total_steps=total_steps,\n                                        #warmup_steps=warmup_steps,\n                                        #alpha=alpha\n                                        #)\n                                #)\n\n\n#from easy_pin_detection.saki_tools import tversky_focal_loss\n\n\n#model.compile(\n    #loss=tversky_focal_loss(alpha=0.7, beta=0.3,gamma=1.5, name='TFL'),\n    #optimizer=opt,\n    #metrics=[\n            #'accuracy',\n            #tf.keras.metrics.BinaryIoU(\n                                            #name='iou_pin',\n                                            #target_class_ids=[1],\n                                            #threshold=0.5),\n\n            #])\n\n\n#_date_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n#MODEL_SAVE_PATH     = data_path/'models'/_date_str\n#MODEL_SAVE_FILE     = f\"{_date_str}_saved_model\"\n#Path(MODEL_SAVE_PATH).mkdir(parents=True, exist_ok=True);\n#MODEL_CPK_FILE      = \"training_001/checkpoint.cpkt\"\n#TF_BOARD_LOG        = \"training_001/.logs\"\n#MODEL_SAVE_PATH, MODEL_SAVE_FILE, MODEL_CPK_FILE, TF_BOARD_LOG\n\n(Path('/home/goni/workspace/projects/easy_pin_detection/data_first/Trainingsdata_1024_1224/models/20230413-032741'),\n '20230413-032741_saved_model',\n 'training_001/checkpoint.cpkt',\n 'training_001/.logs')\n\n\n\n#tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        #log_dir=TF_BOARD_LOG, \n        #histogram_freq=0,\n        #write_images=True, \n        #write_steps_per_second=True)\n    \n## Setup a training checkpoint\n#model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        #filepath=MODEL_CPK_FILE,\n        #save_weights_only=True,\n        #save_best_only=True,\n        #monitor='loss',\n        #verbose=1) \n\n#early_stopping = tf.keras.callbacks.EarlyStopping(\n        ## Stop training when `val_loss` is no longer improving\n        #monitor=\"val_loss\",\n        ## \"no longer improving\" being defined as \"no better than 1e-2 less\"\n        #min_delta=1e-2,\n        ## \"no longer improving\" being further defined as \"for at least 2 epochs\"\n        #patience=5,\n        #verbose=1,\n    #)\n\n\n#def display(display_list):\n  #plt.figure(figsize=(15, 15))\n\n  #title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  #for i in range(len(display_list)):\n    #plt.subplot(1, len(display_list), i+1)\n    #plt.title(title[i])\n    #plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    #plt.axis('off')\n  #plt.show()\n\n\n#for images, masks in train_ds.take(2):\n  #sample_image, sample_mask = images[0], masks[0]\n  #display([sample_image, sample_mask])\n\n\ndef create_mask(pred_mask):\n  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]\n\n\ndef show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n\n\n#class DisplayCallback(tf.keras.callbacks.Callback):\n  #def on_epoch_end(self, epoch, logs=None):\n    #clear_output(wait=True)\n    #show_predictions()\n    #print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n\n\n#epochs = 1000\n#initial_learning_rate = 0.001\n#alpha = 0.1\n#opt = tf.keras.optimizers.Adam(CosineDecay(\n                                        #initial_learning_rate=initial_learning_rate,\n                                        #total_steps=total_steps,\n                                        #warmup_steps=warmup_steps,\n                                        #alpha=alpha\n                                        #)\n                                #)\n#model.compile(\n    #loss=tversky_focal_loss(alpha=0.7, beta=0.3,gamma=1.5, name='TFL'),\n    #optimizer=opt,\n    #metrics=[\n            #'accuracy',\n            #tf.keras.metrics.BinaryIoU(\n                                            #name='iou_pin',\n                                            #target_class_ids=[1],\n                                            #threshold=0.5),\n\n            #])\n\n#history = model.fit(train_ds,\n                    #steps_per_epoch=steps_per_epoch, \n                    #validation_data=test_ds,\n                    #validation_steps=validation_steps, \n                    #callbacks=[tensorboard_callback, model_checkpoint, DisplayCallback()],\n                    #epochs=epochs)\n\n#tf.saved_model.save(model, f'{MODEL_SAVE_PATH}/{MODEL_SAVE_FILE}')"
  },
  {
    "objectID": "confg.html",
    "href": "confg.html",
    "title": "Config",
    "section": "",
    "text": "Configuration for this module will be here"
  },
  {
    "objectID": "metrics_and_loss.html",
    "href": "metrics_and_loss.html",
    "title": "Metrics and loss",
    "section": "",
    "text": "These functions are taken mostly from sakitools, which is developed by Fabian hickert\n\n\nsource\n\ndice_coef\n\n dice_coef (y_true, y_pred, smooth=0)\n\n\nsource\n\n\ntversky_focal_loss\n\n tversky_focal_loss (alpha=0.5, beta=0.5, gamma=1.5, name=None)\n\n\nsource\n\n\ndice_loss\n\n dice_loss (y_true, y_pred)\n\n\nsource\n\n\ndice_coef_loss\n\n dice_coef_loss (y_true, y_pred)\n\n\nsource\n\n\nFDR\n\n FDR (thresholds=0.5, name='fdr_overreject', **kwargs)\n\n! FDR - False discovery rate metric FP / (FP + TP)\n\nsource\n\n\nFNR\n\n FNR (thresholds=0.5, name='fnr_escapee', **kwargs)\n\n! FNR - False negative rate metric FN / (FN + TP)\n\nsource\n\n\ntversky_index\n\n tversky_index (alpha=0.5, beta=0.5, gamma=1.5, name=None)"
  },
  {
    "objectID": "viz_utils.html",
    "href": "viz_utils.html",
    "title": "Visulization utility",
    "section": "",
    "text": "Utility to visualize everything will be implmented here\n\n\nsource\n\nshow_model\n\n show_model (model, show_shapes=True, show_layer_names=True,\n             to_file='model.png')\n\nPlot a Keras model as a graph."
  }
]